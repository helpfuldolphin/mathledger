\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{xcolor}

% Import Macros and Variables
\input{macros}

\title{\textbf{\projectname}: Reflexive Formal Learning and Dual-Attestation Substrate for Verifiable Machine Cognition}
\author{The MathLedger Research Fleet}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}

    We present \projectname, a novel substrate for verifiable machine cognition integrating Reflexive Formal Learning (\RFL) and Dual-Attestation.

    Phase I experiments were conducted to establish baseline operational characteristics and validate the infrastructure through a negative control.

    The system's integrity was validated through a \valMirrorCoverage\% coverage mirror audit.

    \textbf{No uplift claims are made as of Evidence Pack v1.} Phase I serves as infrastructure validation; Phase II uplift experiments (U2) are preregistered but not yet run.

\end{abstract}



\input{sections/02_methodology}

\input{sections/03_results}



\section{Discussion}

\label{sec:discussion}

Phase I experiments aimed to characterize the behavior of the \projectname{} substrate under various configurations. The \Ht{} dynamics (Figure \ref{fig:rfl_ht_convergence}) were observed to understand system states.



\section{Future Work}

\label{sec:future_work}

\subsection{Phase II Uplift Experiments}

Phase II introduces four uplift slices designed for environments where policy-based candidate ordering should produce measurable improvements:

\begin{enumerate}
    \item \textbf{slice\_uplift\_goal} --- Goal-Conditioned Target: Success requires hitting specific target formulas.
    \item \textbf{slice\_uplift\_sparse} --- Sparse Reward: Many candidates, few provable; RFL learns to avoid dead zones.
    \item \textbf{slice\_uplift\_tree} --- Chain Depth: Target requires chain of k intermediate lemmas.
    \item \textbf{slice\_uplift\_dependency} --- Multiple Subgoals: Success requires all k sub-goals proved in same cycle.
\end{enumerate}

Phase II U2 experiments are preregistered but \textbf{not yet run}. No uplift claims are made as of Evidence Pack v1.

\subsection{RLVF Positioning}

Industry is moving from RLHF (Reinforcement Learning from Human Feedback) to RLPF (Process Feedback) to \textbf{RLVF (RL with Verifiable Feedback)}. The \projectname{} RFL mechanism implements RLVF: all feedback is kernel-verifiable (e.g., Lean proofs, truth tables), with no human labels, preferences, or proxy rewards.

Phase I demonstrated that the RFL infrastructure behaves correctly on a symmetric negative control. Phase II will test whether policies can actually lower epistemic risk on non-degenerate slices, under a preregistered, statistically sound protocol.



\section{Conclusion}

\label{sec:conclusion}

Phase I successfully established baseline operational parameters for the \projectname{} system and validated the RFL infrastructure through a negative control experiment. The system correctly shows no uplift when the environment is path-invariant, which is the expected behavior for a well-designed negative control.

\textbf{No uplift claims are made as of Evidence Pack v1.} Phase II U2 experiments on four uplift slices are preregistered but not yet executed.



\appendix

\section{Evidence Pack}

\label{app:evidence}



The following manifests provide cryptographic verification of the experimental runs.



\begin{table}[h]

    \centering

    \begin{tabular}{l l}

        \toprule

        \textbf{Artifact} & \textbf{SHA-256 (Short)} \\

        \midrule

        $H_t$ Snapshot & \valHtShortHash \\

        Mirror Audit Report & \texttt{artifacts/mirror/mirror_report.json} \\

        Drift Table & \texttt{drift_table.json} \\

        \bottomrule

    \end{tabular}

    \caption{Cryptographic manifest of key experimental artifacts.}

    \label{tab:manifest}

\end{table}



\bibliographystyle{plain}

\bibliography{references}



\end{document}
