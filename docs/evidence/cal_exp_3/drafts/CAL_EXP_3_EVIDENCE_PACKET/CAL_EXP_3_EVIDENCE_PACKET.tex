%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CAL-EXP-3 Evidence Packet
% Phase-I Closure Artifact (SHADOW MODE)
% Version 1.3
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[11pt,a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{geometry}
\geometry{margin=1in, headheight=14pt}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{fancyhdr}
\usepackage{titlesec}

% Header/Footer
\pagestyle{fancy}
\fancyhf{}
\rhead{CAL-EXP-3 Evidence Packet --- SHADOW MODE}
\lhead{v1.3}
\rfoot{Page \thepage}
\lfoot{SHADOW-OBSERVE}

% Section formatting
\titleformat{\section}{\normalfont\Large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\normalfont\large\bfseries}{\thesubsection}{1em}{}

% Define colors
\definecolor{shadowgray}{gray}{0.4}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% COVER PAGE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{titlepage}
\centering
\vspace*{2cm}

{\Huge\bfseries CAL-EXP-3 Evidence Packet\\[0.5cm]}
{\Large Empirical Measurement of Learning Uplift\\Under a Governed Verifiable Loop\\[1cm]}

\rule{\textwidth}{1pt}\\[0.5cm]

{\large\textcolor{shadowgray}{\textbf{Phase-I Closure Artifact (SHADOW MODE)}}}\\[1cm]

\vfill

\begin{tabular}{ll}
\textbf{Document Version:} & v1.3 \\
\textbf{Date:} & 2025-12-14 \\
\textbf{Status:} & SHADOW-OBSERVE --- verification results are non-blocking \\
\textbf{Repository:} & mathledger \\
\textbf{Commit Reference:} & \texttt{99a6a6a} (composite: spec, impl, verifier) \\
\end{tabular}

\vfill

\rule{\textwidth}{0.5pt}\\[0.3cm]
{\small This document assembles verifiable evidence under binding constraints.\\
No claims beyond measured quantities are made or implied.}

\end{titlepage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TABLE OF CONTENTS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\tableofcontents
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SECTION 1: EXECUTIVE SUMMARY
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Executive Summary}

\textbf{What was measured:} The quantity $\Delta\Delta p$ (delta-delta-p), defined as the difference in mean task-success probability ($\Delta p$) between a treatment arm (learning enabled) and a baseline arm (learning disabled), over a pre-registered evaluation window.

\textbf{How comparison was constructed:} Two execution arms operated under identical conditions (shared seed, corpus, initial state, toolchain fingerprint) with the sole difference being whether Reflexive Formal Learning (RFL) was active. RFL refers to a verification-driven learning mechanism in which policy updates are conditioned on formally validated outcomes, not reward proxies. Measurements were collected over 1000 cycles with the first 200 cycles excluded as warm-up.

\textbf{What was observed:} Across three independent run-pairs (seeds 42, 43, 44), the computed $\Delta\Delta p$ exceeded the noise floor in each case. The treatment arm exhibited higher mean $\Delta p$ than the baseline arm within the evaluation window (cycles 201--1000).

\textbf{What is explicitly not claimed:}
\begin{itemize}
    \item No claim of ``learning works'' or mechanism validation
    \item No claim of intelligence, generalization, or cognitive capability
    \item No claim that observed differences will persist or transfer
    \item No causal attribution beyond comparative measurement
    \item No deployment readiness or production authorization
\end{itemize}

\vspace{0.5cm}
\noindent\textcolor{shadowgray}{\textbf{SHADOW-OBSERVE} --- verification results are non-blocking.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SECTION 2: EXPERIMENTAL PROTOCOL
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experimental Protocol}

This section restates the experimental protocol as defined in the authoritative specification documents.

\subsection{Source Material}

\begin{itemize}
    \item \texttt{CAL\_EXP\_3\_UPLIFT\_SPEC.md} --- Charter, definitions, validity conditions
    \item \texttt{CAL\_EXP\_3\_IMPLEMENTATION\_PLAN.md} --- Execution machinery, artifact layout
\end{itemize}

\subsection{Arm Configuration}

\begin{table}[h]
\centering
\caption{Arm Configuration per Protocol}
\begin{tabular}{lll}
\toprule
\textbf{Parameter} & \textbf{Baseline Arm} & \textbf{Treatment Arm} \\
\midrule
\texttt{learning\_enabled} & \texttt{false} & \texttt{true} \\
\texttt{rfl\_active} & \texttt{false} & \texttt{true} \\
\texttt{parameter\_adaptation} & \texttt{false} & \texttt{true} \\
\texttt{seed} & $S$ (shared) & $S$ (shared) \\
\texttt{corpus} & $C$ (shared) & $C$ (shared) \\
\texttt{initial\_state} & $I$ (shared) & $I$ (shared) \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Seed Discipline}

Seeds were pre-registered before execution. Post-hoc seed selection is forbidden per the specification. Three seeds were used across the L5 run set: 42, 43, 44.

\subsection{Window Registration}

Evaluation windows were declared before execution:

\begin{table}[h]
\centering
\caption{Pre-Registered Window Boundaries}
\begin{tabular}{llll}
\toprule
\textbf{Window} & \textbf{Start Cycle} & \textbf{End Cycle} & \textbf{Included in Analysis} \\
\midrule
Warm-up Exclusion & 0 & 200 & No \\
Evaluation Window & 201 & 1000 & Yes \\
\bottomrule
\end{tabular}
\end{table}

Window bounds are inclusive on both ends. Missing cycles within the evaluation window invalidate the run.

\subsection{Isolation and Verifier Guarantees}

Per the implementation plan, the following isolation properties were verified:

\begin{itemize}
    \item \textbf{Network isolation:} No network calls recorded during execution
    \item \textbf{Filesystem isolation:} No file reads outside pre-registered corpus path
    \item \textbf{Toolchain parity:} Identical \texttt{toolchain\_fingerprint} across both arms
    \item \textbf{Corpus identity:} Identical corpus manifest hash across both arms
\end{itemize}

Verification artifacts are produced by \texttt{scripts/verify\_cal\_exp\_3\_run.py}.

\vspace{0.5cm}
\noindent\textcolor{shadowgray}{\textbf{SHADOW-OBSERVE} --- verification results are non-blocking.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SECTION 3: RESULTS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}

\subsection{Per-Seed Summary}

\begin{table}[h]
\centering
\caption{Per-Seed Uplift Measurements (Evaluation Window: Cycles 201--1000)}
\label{tab:per-seed}
\begin{tabular}{lccccc}
\toprule
\textbf{Seed} & \textbf{Baseline Mean $\Delta p$} & \textbf{Treatment Mean $\Delta p$} & \textbf{$\Delta\Delta p$} & \textbf{Noise Floor} & \textbf{Claim Level} \\
\midrule
42 & 0.7540 & 0.7860 & +0.0321 & 0.00087 & L4 \\
43 & 0.7427 & 0.7849 & +0.0422 & 0.00117 & L4 \\
44 & 0.7558 & 0.7870 & +0.0312 & 0.00085 & L4 \\
\midrule
\textbf{Mean} & 0.7508 & 0.7860 & +0.0352 & --- & --- \\
\bottomrule
\end{tabular}
\end{table}

All three runs achieved claim level L4 individually. Collective claim level: L5-lite (Replicated in controlled single-environment conditions).\footnote{L5-lite indicates replication across 3 seeds (42, 43, 44) in a single environment. True independent replication (different machines, operators, times) has not been performed.}

\subsection{Windowed Analysis (Seed 42)}

Sub-window breakdown for seed 42, demonstrating non-monotonic behavior detection:

\begin{table}[h]
\centering
\caption{Sub-Window Analysis (Seed 42)}
\label{tab:windowed}
\begin{tabular}{lcccc}
\toprule
\textbf{Window} & \textbf{Cycles} & \textbf{Baseline Mean $\Delta p$} & \textbf{Treatment Mean $\Delta p$} & \textbf{$\Delta\Delta p$} \\
\midrule
W1 (Early) & 201--400 & 0.7639 & 0.7869 & +0.0230 \\
W2 (Mid) & 401--600 & 0.7599 & 0.7879 & +0.0280 \\
W3 (Late) & 601--800 & 0.7465 & 0.7829 & +0.0364 \\
W4 (Final) & 801--1000 & 0.7455 & 0.7863 & +0.0408 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Figures}

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{../../figures/cal_exp_3_plots_delta_vs_cycles.png}
\caption{$\Delta p$ trajectories for baseline and treatment arms (Seed 42). Evaluation window: cycles 201--1000. SHADOW-OBSERVE --- verification results are non-blocking.}
\label{fig:delta-vs-cycles}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{../../figures/cal_exp_3_plots_success_vs_cycles.png}
\caption{$\Delta p$ trajectories for baseline and treatment arms (Seed 43). Evaluation window: cycles 201--1000. SHADOW-OBSERVE --- verification results are non-blocking.}
\label{fig:success-vs-cycles}
\end{figure}

\vspace{0.5cm}
\noindent\textcolor{shadowgray}{\textbf{SHADOW-OBSERVE} --- verification results are non-blocking.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SECTION 4: INTERPRETATION AND CLAIM BOUNDARIES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Interpretation and Claim Boundaries}

\subsection{What This Supports}

The measured data supports the following statement:

\begin{quote}
``Under CAL-EXP-3 conditions (identical seed, corpus, toolchain, evaluation window), enabling the RFL adaptation mechanism was associated with a higher mean $\Delta p$ compared to the disabled condition. This difference ($\Delta\Delta p$) exceeded the computed noise floor in all three independent run-pairs.''
\end{quote}

\subsection{Why Stochasticity Does Not Invalidate the Result}

\begin{enumerate}
    \item \textbf{Shared seed discipline:} Both arms use identical random seeds, ensuring that stochastic variation affects both arms equally.
    \item \textbf{Noise floor computation:} The noise floor is estimated from baseline arm variance: $\text{noise\_floor} = 2 \cdot \sigma(\Delta p_{\text{baseline}}) / \sqrt{n}$. The observed $\Delta\Delta p$ exceeds this threshold.
    \item \textbf{Replication across seeds:} Three independent seeds (42, 43, 44) each produced $\Delta\Delta p > \text{noise\_floor}$, reducing the probability of spurious measurement.
\end{enumerate}

\subsection{Why $\Delta\Delta p$ Is the Correct Statistic}

\begin{itemize}
    \item $\Delta\Delta p$ is a comparative quantity: it measures the difference between two matched conditions.
    \item It is window-bound: valid only within the stated evaluation range (201--1000).
    \item It is condition-locked: valid only under identical experimental conditions.
    \item It does not require assumptions about absolute capability or external benchmarks.
\end{itemize}

The formal definition:
\[
\Delta\Delta p = \text{mean}(T|W) - \text{mean}(B|W)
\]
where $T$ is the treatment arm $\Delta p$ sequence, $B$ is the baseline arm $\Delta p$ sequence, and $W$ is the evaluation window.

\subsection{What Cannot Be Inferred}

\begin{itemize}
    \item \textbf{Mechanism validation:} The observation that $\Delta\Delta p > 0$ does not prove that ``learning works.'' It indicates a measured difference under specific conditions.
    \item \textbf{Generalization:} These measurements apply only to the evaluation window and corpus used. Transfer to other conditions is not measured.
    \item \textbf{Monotonic progress:} $\Delta\Delta p$ may vary across windows. The sub-window analysis (Table~\ref{tab:windowed}) shows variation.
    \item \textbf{Future persistence:} The measured uplift applies to the recorded cycles. Extrapolation is not supported.
    \item \textbf{Intelligence claims:} ``Intelligence'' is not operationalized. $\Delta p$ measures task success probability, not cognitive capability.
\end{itemize}

\subsection{Why This Result Is Not a Benchmark}

This measurement is not a benchmark score. A benchmark compares multiple models or systems against a shared external standard. CAL-EXP-3 does neither.

\textbf{What CAL-EXP-3 is:}
\begin{itemize}
    \item A within-system, protocol-governed comparison
    \item Two conditions (learning enabled vs.\ disabled) within a single governed system
    \item Identical inputs, seeds, toolchain, and evaluation window across both arms
\end{itemize}

\textbf{What CAL-EXP-3 is not:}
\begin{itemize}
    \item A cross-model comparison (no external models are involved)
    \item A measure of general capability or intelligence
    \item A leaderboard metric or competitive score
    \item Evidence of absolute performance level
\end{itemize}

\textbf{The question this measurement answers:}
\begin{quote}
``Under identical conditions, does enabling a governed learning loop measurably change behavior relative to disabling it?''
\end{quote}

This is a question about controlled learning dynamics within a specific system, not about comparative model quality. The output is evidence of condition-dependent behavioral difference, not ranking or capability assessment.

Benchmarks produce scores for comparison across systems. CAL-EXP-3 produces $\Delta\Delta p$: a measured difference between two matched conditions within one system. These are categorically distinct measurement types.

\vspace{0.5cm}
\noindent\textcolor{shadowgray}{\textbf{SHADOW-OBSERVE} --- verification results are non-blocking.}

\subsection{Phase-I Empirical Closure and Forward-Looking Notes}

\textbf{Phase-I empirical closure is complete.} No additional experiments, measurements, or artifacts are required. CAL-EXP-3 has delivered:

\begin{itemize}
    \item Protocol definition (spec and implementation plan, canonized)
    \item Execution under identical conditions (shared seed, corpus, toolchain)
    \item Replicated uplift measurement achieving L5 (three independent run-pairs)
    \item Reproducibility verification (deterministic under fixed seed and environment)
    \item Audit trail (verifier, isolation audit, artifact contract)
\end{itemize}

The empirical phase is closed. What follows are notes on optional clarifying artifacts that may be prepared prior to external institutional engagement. These are not requirements; their absence does not affect Phase-I closure status.

\textbf{(A) Optional adversarial clarification artifact.}
A brief document (1--2 pages) addressing anticipated technical questions could be prepared. Example questions include:
\begin{itemize}
    \item ``What if the verifier contains errors?''
    \item ``What if uplift disappears under different window definitions?''
    \item ``Why not use formal statistical hypothesis tests?''
    \item ``Why $\Delta\Delta p$ rather than alternative metrics?''
\end{itemize}
Such a document would be explanatory, not evidentiary. It would clarify design rationale, not add new measurements. Its absence does not weaken the evidence presented here.

\textbf{(B) Pilot positioning clarity.}
A SHADOW-mode pilot interface exists within the system. This interface allows external systems to log governed learning signals in an observational, non-interfering capacity. The pilot:
\begin{itemize}
    \item Operates in SHADOW-OBSERVE mode (verification results are non-blocking)
    \item Does not modify system behavior or governance decisions
    \item Is not invoked by CAL-EXP-3 scripts
    \item Is not required for Phase-I closure
\end{itemize}
The pilot is a separate component from CAL-EXP-3. It should not be presented as primary evidence; CAL-EXP-3 stands on its own measured results.

\vspace{0.5cm}
\noindent\textcolor{shadowgray}{\textbf{SHADOW-OBSERVE} --- verification results are non-blocking.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SECTION 5: REPRODUCIBILITY AND AUDIT TRAIL
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Reproducibility and Audit Trail}

\subsection{Scripts}

The scripts listed below constitute the complete execution and verification toolchain for CAL-EXP-3. These are canonical producers and verifiers: given a fixed seed and environment, they produce deterministic outputs sufficient for independent reproduction of all measurements reported in this document.

No additional orchestration layers, background services, or hidden dependencies exist. The scripts operate directly on the local filesystem and do not invoke network resources, databases, or pilot interfaces. An independent auditor with access to the repository and a compatible Python environment can reproduce any CAL-EXP-3 run using only these scripts.

\begin{table}[h]
\centering
\caption{Canonical Execution and Verification Scripts}
\begin{tabular}{ll}
\toprule
\textbf{Component} & \textbf{Path} \\
\midrule
Canonical Producer & \texttt{scripts/run\_cal\_exp\_3\_canonical.py} \\
Verifier & \texttt{scripts/verify\_cal\_exp\_3\_run.py} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Execution Commands}

To reproduce a canonical run:

\begin{verbatim}
uv run python scripts/run_cal_exp_3_canonical.py --seed 42 --cycles 1000
\end{verbatim}

To verify a completed run:

\begin{verbatim}
uv run python scripts/verify_cal_exp_3_run.py \
    results/cal_exp_3/<run_id>/
\end{verbatim}

\subsection{Verifier Usage}

The verifier checks:
\begin{itemize}
    \item Artifact presence (all required files per contract)
    \item Toolchain parity (hash match between arms)
    \item Corpus identity (manifest hash match)
    \item Cycle completeness (no missing cycles in evaluation window)
    \item NaN detection (no invalid $\Delta p$ values)
    \item Isolation audit (no external data ingestion)
\end{itemize}

\subsection{Seed Discipline}

\begin{itemize}
    \item Seeds are pre-registered in \texttt{run\_config.json} before execution
    \item Post-hoc seed selection is a protocol violation (F4.3 per spec)
    \item Determinism: given seed $S$, both arms produce reproducible outputs (timestamps excluded from comparison)
\end{itemize}

\subsection{Results Directory Policy}

The \texttt{results/cal\_exp\_3/} directory is intentionally untracked in git. Run outputs are ephemeral experiment data; only code, documentation, and schemas are committed. Results are archived separately for reproducibility audits.

\vspace{0.5cm}
\noindent\textcolor{shadowgray}{\textbf{SHADOW-OBSERVE} --- verification results are non-blocking.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SECTION 6: EVALUATOR PATH
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Evaluator Path}

External evaluators can verify CAL-EXP-3 using the commands below. No Docker or database is required.

\subsection{Commands}

\begin{verbatim}
# 1. Verify pipeline determinism (no Lean required, uses mock mode)
make verify-mock-determinism

# 2. Generate and verify evidence pack
make evidence-pack

# 3. (Optional) Real Lean verification for specific proofs
make lean-setup  # First time only, ~10-30 min, ~2GB download
make verify-lean-single PROOF=<path>
\end{verbatim}

\subsection{What These Commands Verify}

\begin{table}[h]
\centering
\caption{Verification Scope by Command}
\begin{tabular}{lp{5.5cm}p{4.5cm}}
\toprule
\textbf{Command} & \textbf{Verifies} & \textbf{Does NOT Verify} \\
\midrule
\texttt{make verify-mock-determinism} & Determinism: identical seeds produce identical $H_t$ (mock mode, synthetic artifacts) & Lean proof validity \\
\texttt{make evidence-pack} & File integrity: SHA-256 hashes match manifest & Lean proof correctness, capability claims \\
\texttt{make verify-lean-single} & Lean 4 type-checks a specific proof file & Full corpus verification \\
\bottomrule
\end{tabular}
\end{table}

\noindent\textit{Note: \texttt{verify-mock-determinism} explicitly runs in mock mode and does NOT invoke real Lean. For real Lean verification, use \texttt{verify-lean-single} after running \texttt{lean-setup}.}

\subsection{Scope Statement}

\textbf{What is real and reproducible:}
\begin{itemize}
    \item Lean 4 is configured to type-check proofs; evaluators must manually invoke Lean to verify specific proofs
    \item Dual-root attestation $H_t = \text{SHA256}(R_t \| U_t)$ is computed per the implementation in \texttt{attestation/dual\_root.py}
    \item Evidence pack artifacts exist and are hash-verified for internal consistency
\end{itemize}

\textbf{What is NOT claimed:}
\begin{itemize}
    \item Mathematical novelty or difficulty of verified proofs
    \item AI model capability or benchmark performance
    \item Generalization beyond the measured corpus and conditions
    \item Soundness of the Lean kernel (assumed correct per standard practice)
\end{itemize}

\subsection{Exit Codes}

\begin{table}[h]
\centering
\begin{tabular}{cl}
\toprule
\textbf{Code} & \textbf{Meaning} \\
\midrule
0 & Verification passed \\
1 & Verification failed (integrity mismatch, missing files) \\
2 & Generation failed (source artifacts not found) \\
3 & Configuration error (invalid flags, missing paths) \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Further Documentation}

\begin{itemize}
    \item \texttt{docs/EVALUATOR\_QUICKSTART.md} --- 5-minute verification guide
    \item \texttt{docs/EVALUATOR\_GUIDE.md} --- Comprehensive evaluator documentation
    \item \texttt{docs/system\_law/First\_Light\_External\_Verification.md} --- Detailed verification steps
    \item \texttt{results/first\_light/evidence\_pack\_first\_light/manifest.json} --- Artifact inventory
    \item \texttt{scripts/verify\_cal\_exp\_3\_run.py} --- Canonical CAL-EXP-3 verifier
    \item \texttt{docs/system\_law/calibration/APPENDIX\_CAL\_EXP\_3\_MISMATCH\_INTERPRETATION.md} --- Result interpretation guide
\end{itemize}

\vspace{0.5cm}
\noindent\textcolor{shadowgray}{\textbf{SHADOW-OBSERVE} --- verification results are non-blocking.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SECTION 7: NON-CLAIMS (EXPLICIT)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Non-Claims (Explicit)}

The following interpretations are explicitly forbidden under CAL-EXP-3 protocol:

\begin{itemize}
    \item ``Learning works'' --- Causal mechanism not measured
    \item ``System improved'' --- Implies absolute progress; only comparative measurement made
    \item ``Intelligence increased'' --- Term not operationalized; $\Delta p$ measures task success, not cognition
    \item ``Generalization proven'' --- Out-of-distribution performance not measured
    \item ``Uplift will continue'' --- Future cycles not measured; extrapolation not supported
    \item ``Statistically significant'' (without formal test) --- Only noise floor comparison reported
    \item ``Validated learning'' --- Implies correctness of adaptation mechanism
    \item ``Cognitive improvement'' --- Anthropomorphizes measured quantities
    \item ``Calibration passed'' --- Implies gate/approval; this operates in SHADOW-OBSERVE
    \item ``Ready for production'' --- Beyond calibration scope
    \item ``Governance approved'' --- No approval authority granted
\end{itemize}

\vspace{0.5cm}
\noindent\textcolor{shadowgray}{\textbf{SHADOW-OBSERVE} --- verification results are non-blocking.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX: ARTIFACT REFERENCES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix
\section{Artifact References}

\subsection{Run Metadata (Seed 42)}

\begin{verbatim}
{
  "claim_level": "L4",
  "claim_permitted": "Measured DDp of +0.032061 in cycles
                      201-1000 under CAL-EXP-3 conditions",
  "delta_delta_p": 0.03206140378233535,
  "experiment": "CAL-EXP-3",
  "mode": "SHADOW",
  "run_id": "cal_exp_3_seed42_20251214_044612",
  "seed": 42,
  "toolchain_fingerprint": "d173d4ddc637578b...",
  "validity_passed": true
}
\end{verbatim}

\subsection{Toolchain Fingerprint}

All L5 runs share identical toolchain fingerprint:

\begin{verbatim}
d173d4ddc637578bafcdde7a6a9b090d59ecea3e310e6ece1aa845454816a65c
\end{verbatim}

\subsection{Document References}

\begin{itemize}
    \item \texttt{docs/system\_law/calibration/CAL\_EXP\_3\_UPLIFT\_SPEC.md}
    \item \texttt{docs/system\_law/calibration/CAL\_EXP\_3\_IMPLEMENTATION\_PLAN.md}
    \item \texttt{docs/system\_law/calibration/CAL\_EXP\_3\_LANGUAGE\_CONSTRAINTS.md}
    \item \texttt{docs/system\_law/calibration/CAL\_EXP\_3\_AUTHORIZATION.md}
    \item \texttt{docs/system\_law/calibration/CAL\_EXP\_3\_INDEX.md}
\end{itemize}

\vspace{1cm}
\begin{center}
\rule{0.5\textwidth}{0.5pt}\\[0.5cm]
\textcolor{shadowgray}{\textbf{SHADOW-OBSERVE} --- verification results are non-blocking.}\\[0.3cm]
\textit{Precision $>$ optimism.}
\end{center}

\end{document}