\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{enumitem}

% Import Macros and Variables
\input{macros}

% Theorem environments
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}

\title{\textbf{\projectname}: A Verifiable Learning Substrate with Ledger-Attested Feedback}
\author{The MathLedger Research Fleet}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
Contemporary AI systems achieve extraordinary performance yet remain opaque and non-verifiable, creating a crisis of trust for safety-critical deployment. We introduce \projectname{}, a substrate for \emph{verifiable machine cognition} that integrates formal verification, cryptographic attestation, and learning dynamics into a single epistemic loop. The system implements \emph{Reflexive Formal Learning} (RFL), a symbolic analogue of gradient descent where updates are driven by verified proof events rather than statistical loss.

Phase I experiments validate the measurement and governance substrate under controlled conditions. All runs produced the expected fail-closed behavior; no convergence or capability claims are made. The contribution is infrastructural: a working prototype of ledger-attested learning that enables auditability at scale.

\textbf{Keywords:} verifiable learning, formal verification, cryptographic attestation, reflexive feedback, fail-closed governance
\end{abstract}

%=============================================================================
\section{Introduction: The Verifiability Gap}
\label{sec:intro}

Modern large language models are universal approximators of text, not of truth. Hallucination is structurally baked into density-estimation objectives; conventional evaluations penalize abstention and reward confident output regardless of correctness \cite{marcus2020}. In safety-critical domains---finance, law, infrastructure, policy---this creates an untenable gap between capability and trust.

The AI industry is discovering a structural constraint:
\begin{quote}
\emph{Performance without verifiability is not deployable at scale.}
\end{quote}

Mathematics offers a way out: verifiable reasoning with machine-checkable proofs. \projectname{} converts mathematics into a \emph{living protocol} for learning under formal law.

\subsection{What Problem Does This Address?}

Existing approaches to improving AI reliability fall into three categories, each with limitations:

\begin{enumerate}[leftmargin=1.5em]
\item \textbf{Reward shaping (RLHF, DPO):} Human preferences guide learning, but preferences are noisy, inconsistent, and gameable. The feedback signal is statistical, not verifiable.

\item \textbf{Verifier-guided generation:} Proof assistants check outputs post-hoc, but rejected outputs provide no structured learning signal. The verifier is a filter, not a teacher.

\item \textbf{Benchmark scaling:} Larger test sets reduce variance but do not establish correctness. Passing benchmarks does not imply understanding.
\end{enumerate}

\projectname{} takes a different approach: \emph{the verifier's binary decision becomes the learning signal itself}. Every update is justified by a formally verified proof or explicit abstention, recorded in an immutable ledger. This creates a closed epistemic loop where learning is constrained to verified truth.

\subsection{The Chain of Verifiable Cognition}

The system implements an end-to-end pipeline:
\[
\text{Input} \to \text{Proof-or-Abstain} \to \text{Ledger Attestation} \to \text{Dual Commitment} \to \text{Policy Update}
\]

Each component is cryptographically bound:
\begin{itemize}[leftmargin=1.5em]
\item \textbf{Proof-or-Abstain:} The Lean kernel verifies reasoning or the system explicitly abstains. No middle ground.
\item \textbf{Ledger Attestation:} Verified events are sealed into a monotone, append-only ledger with Merkle roots.
\item \textbf{Dual Commitment:} Both reasoning artifacts ($r_t$) and interface state ($u_t$) are committed: $H_t = \mathrm{Hash}(\texttt{EPOCH:} \| r_t \| u_t)$.
\item \textbf{Policy Update:} Reflexive Formal Learning (RFL) adjusts the policy based on verification outcomes.
\end{itemize}

This architecture enables a new primitive: \emph{learning from verified truth rather than statistical loss}.

\subsection{What Is Genuinely New}

\projectname{} combines three elements not previously integrated:

\begin{enumerate}[leftmargin=1.5em]
\item \textbf{Ledger-attested learning signals:} Unlike reward models or human feedback, the learning signal is a cryptographically committed verification outcome.

\item \textbf{Fail-closed governance:} The system cannot silently degrade. Either verification succeeds and learning proceeds, or the system abstains and logs the failure.

\item \textbf{Auditability as infrastructure:} Every update has a replayable provenance chain. Post-hoc analysis can reconstruct exactly what was learned and why.
\end{enumerate}

This paper reports Phase I experiments that validate the substrate. No capability or convergence claims are made.

%=============================================================================
\section{System Architecture}
\label{sec:architecture}

\subsection{Pipeline Overview}

\input{sections/02_methodology}

\subsection{The Monotone Ledger}

\begin{definition}[Monotone Ledger]
A ledger $\mathcal{L}$ is a sequence of blocks $(B_1, B_2, \ldots)$ where each $B_t$ contains verified proofs with: (i) canonical statement hashes, (ii) Lean verification status, and (iii) a Merkle root $R_t$ over sorted proof IDs. The ledger is \emph{monotone} if $\bigcup_{i \le t} B_i \subseteq \bigcup_{i \le t+1} B_i$.
\end{definition}

Monotonicity ensures that verified knowledge only grows. Statements cannot be retracted; only new proofs can be added.

\subsection{Dual Attestation}

At each epoch $t$, the system commits to two roots:
\begin{itemize}[leftmargin=1.5em]
\item $r_t$: Reasoning root over canonicalized proof artifacts
\item $u_t$: UI root over interface state (DOM, logs, user confirmations)
\end{itemize}

These are bound by $H_t = \mathrm{Hash}(\texttt{EPOCH:} \| r_t \| u_t)$ with prefix-free domain separation. The tuple $(r_t, u_t, H_t)$ is the \emph{epistemic fingerprint} of the epoch---the only scalar permitted as a summary of what occurred.

%=============================================================================
\section{Reflexive Formal Learning: Formal Anchor}
\label{sec:rfl}

Reflexive Formal Learning (RFL) is a symbolic analogue of gradient descent operating on verification outcomes rather than numerical errors.

\subsection{Core Definitions}

Let $\Pi$ be the space of symbolic reasoning policies and $P_\pi$ the event distribution induced by policy $\pi$.

\begin{definition}[Verification Outcome]
For reasoning event $e_t$, the verifier produces:
\[
\mathcal{V}(e_t) \in \{1, 0, \bot\}
\]
where $1$ = verified proof, $0$ = failed proof, $\bot$ = abstention.
\end{definition}

\begin{definition}[Epistemic Risk]
\label{def:epistemic-risk}
The epistemic risk of policy $\pi$ is:
\[
\mathcal{J}(\pi) = \mathbb{E}_{e \sim P_\pi}[\mathbf{1}\{\mathcal{V}(e) \neq 1\}] = \Pr_{e \sim P_\pi}[\mathcal{V}(e) \neq 1]
\]
This measures the probability mass on non-verified events (failures and abstentions).
\end{definition}

\subsection{The RFL Update Rule}

At each step $t$:
\begin{equation}
\label{eq:rfl-update}
\pi_{t+1} = \pi_t \oplus \eta \cdot \Phi(\mathcal{V}(e_t), \pi_t)
\end{equation}
where $\oplus$ is algebraic composition on policy space and $\Phi: \{1, 0, \bot\} \times \Pi \to \Delta\Pi$ maps verification outcomes to policy adjustments.

The intuition is:
\begin{quote}
\emph{Policies that cause fewer failures and abstentions become more likely; policies that cause them become less likely.}
\end{quote}

\begin{proposition}[RFL as Stochastic Approximation]
\label{prop:sa}
Under standard regularity conditions (bounded updates, filtration adaptivity, summable step sizes), the RFL dynamics constitute a stochastic approximation process on $\mathcal{J}(\pi)$. The learning signal arises from verified truth rather than statistical loss.
\end{proposition}

\begin{remark}
Proposition~\ref{prop:sa} establishes that RFL has the mathematical structure of a learning algorithm. It does not claim convergence in finite time or under Phase I conditions. The RFL recursion has stochastic approximation form; convergence to a minimum requires additional stability assumptions that are not claimed in Phase I. Full proofs appear in Appendix~\ref{app:formal-proofs}.
\end{remark}

\subsection{Abstention as First-Class Outcome}

Unlike reward-based systems that penalize abstention, RFL treats it as informative:
\begin{itemize}[leftmargin=1.5em]
\item Abstention prevents false positives (hallucinations committed to ledger)
\item Abstention rates provide signal about policy quality
\item High abstention with stable $\mathcal{J}(\pi)$ indicates the policy is appropriately cautious
\end{itemize}

This inverts the standard incentive structure: the system is rewarded for knowing what it does not know.

%=============================================================================
\section{Formal Properties of the Substrate}
\label{sec:formal-properties}

To strengthen the theoretical rigor of Phase I, we state formal results for three key properties: (1) the RFL update rule as a stochastic approximation process, (2) the monotonicity and tamper-evidence of the ledger, and (3) the binding property of the dual attestation hash. Each result is stated under clear assumptions; full proofs appear in Appendix~\ref{app:formal-proofs}.

\subsection{RFL Update as Stochastic Approximation}

\begin{proposition}[RFL as Stochastic Approximation]
\label{prop:sa-formal}
Consider the RFL policy update $\pi_{t+1} = \pi_t \oplus \eta_t \Phi(\mathcal{V}(e_t), \pi_t)$, where $\Phi(\mathcal{V}(e_t), \pi_t)$ is the adjustment induced by verification outcome $\mathcal{V}(e_t) \in \{1, 0, \bot\}$ at time $t$, and $\eta_t > 0$ is the learning step size. Assume:
\begin{enumerate}[leftmargin=1.5em]
\item \textbf{(Bounded updates)} There exists $L < \infty$ such that $\|\Phi(\mathcal{V}(e), \pi)\| \le L$ for all events and policies.
\item \textbf{(Martingale noise)} The update deviations $M_{t+1} := \Phi(\mathcal{V}(e_t), \pi_t) - h(\pi_t)$ satisfy $\mathbb{E}[M_{t+1} \mid \mathcal{F}_t] = 0$ with bounded variance, where $h(\pi) := \mathbb{E}[\Phi(\mathcal{V}(e), \pi) \mid \pi]$.
\item \textbf{(Robbins--Monro stepsizes)} $\sum_{t=0}^\infty \eta_t = \infty$ and $\sum_{t=0}^\infty \eta_t^2 < \infty$.
\end{enumerate}
Under these conditions, the RFL recursion can be written in canonical stochastic approximation form:
\[
\pi_{t+1} = \pi_t + \eta_t \big( h(\pi_t) + M_{t+1} \big)
\]
where $M_{t+1}$ is a martingale-difference noise term. This establishes that RFL has the mathematical structure of a learning algorithm. Convergence to an equilibrium requires additional stability assumptions (e.g., contraction of $h$) that are not claimed in Phase I.
\end{proposition}

\subsection{Monotone Ledger and Tamper-Evidence}

\begin{proposition}[Monotonicity and Tamper-Evidence]
\label{prop:ledger-formal}
Let $\mathcal{L} = (B_1, B_2, \ldots, B_T)$ be a ledger of sequential blocks, where each block $B_t$ contains verified proofs. Define the knowledge state $K_t := \bigcup_{i=1}^t B_i$. Let $L_t$ denote the ledger head hash after block $t$, computed as $L_t = \mathrm{Hash}(L_{t-1} \| R_t)$ where $R_t$ is the Merkle root of $B_t$. Assume:
\begin{enumerate}[leftmargin=1.5em]
\item Blocks are append-only (no modification after appending).
\item The hash function is collision-resistant.
\end{enumerate}
Then:
\begin{enumerate}[leftmargin=1.5em]
\item \textbf{(Monotonicity)} $K_t \subseteq K_{t+1}$ for all $t$. Verified knowledge only grows.
\item \textbf{(Tamper-Evidence)} For any altered ledger $\tilde{\mathcal{L}} \neq \mathcal{L}$, the head hash $\tilde{L}_T \neq L_T$ except with negligible probability.
\end{enumerate}
\end{proposition}

\subsection{Dual Attestation Binding}

\begin{lemma}[Binding Property of Dual Attestation]
\label{lem:binding}
At each epoch $t$, the system commits to reasoning root $r_t$ (32-byte digest) and UI root $u_t$ (32-byte digest), then publishes $H_t = \mathrm{Hash}(\texttt{EPOCH:} \| r_t \| u_t)$. Under the assumption that the hash function is collision-resistant and the encoding uses fixed-width (32-byte) digests with prefix-free domain separation, the hash $H_t$ binds the pair $(r_t, u_t)$: it is computationally infeasible for any $(r'_t, u'_t) \neq (r_t, u_t)$ to produce the same $H_t$.
\end{lemma}

\begin{remark}
The fixed-width encoding (32-byte digests) eliminates concatenation ambiguity. The prefix \texttt{EPOCH:} provides domain separation from other hash uses in the system. Together with Proposition~\ref{prop:ledger-formal}, this ensures every aspect of the system's state is tamper-evident and auditably linked to verified proofs.
\end{remark}

%=============================================================================
\section{Phase I Experimental Results}
\label{sec:results}

\input{sections/03_results}

\subsection{Interpreting Phase I Outcomes}

The Phase I results establish three facts:

\begin{enumerate}[leftmargin=1.5em]
\item \textbf{The measurement substrate works.} $\Delta p$ (success rate proxy) is computable per cycle. Variance between arms is measurable.

\item \textbf{Fail-closed governance triggers correctly.} F5.2 (variance ratio out of bounds) and F5.3 (windowed drift excessive) fired in all runs, capping claims at L0.

\item \textbf{Non-convergence is informative, not a failure.} Phase I was designed to validate infrastructure, not demonstrate capability. The fact that fail-close triggers fired correctly \emph{is} the success condition.
\end{enumerate}

%=============================================================================
\section{Discussion: Why This Matters}
\label{sec:discussion}

Phase I experiments aimed to characterize the behavior of the \projectname{} substrate under various configurations. The \Ht{} dynamics (Figure~\ref{fig:rfl_ht_convergence}) were observed to understand system states.

\subsection{Comparison to Adjacent Work}

\projectname{} occupies a distinct position in the landscape of verifiable AI:

\begin{table}[htbp]
\centering
\begin{tabular}{l|c|c|c}
\textbf{Approach} & \textbf{Learning Signal} & \textbf{Auditability} & \textbf{Fail-Closed} \\
\hline
RLHF & Human preference & Low & No \\
Verifier-guided & Post-hoc filter & Medium & No \\
Proof-carrying code & None (static) & High & Yes \\
\textbf{\projectname{} (RFL)} & \textbf{Verified truth} & \textbf{High} & \textbf{Yes} \\
\end{tabular}
\caption{Comparison of approaches to reliable AI. \projectname{} uniquely combines verified learning signals with fail-closed governance.}
\label{tab:comparison}
\end{table}

\subsection{Layer-3 Infrastructure}

\projectname{} is not a proof generator or a user-facing application. It is \emph{Layer-3 infrastructure}: the flight data recorder for AI reasoning.

\begin{itemize}[leftmargin=1.5em]
\item \textbf{Layer 1 (Human):} Users pose queries, interpret results, make decisions
\item \textbf{Layer 2 (Engine):} AI models generate formal artifacts
\item \textbf{Layer 3 (Ledger):} \projectname{} provides immutable provenance and attestation
\end{itemize}

The system does not compete with proof generators; it makes their outputs trustworthy at scale.

%=============================================================================
\section{Explicit Non-Claims and Scope Boundaries}
\label{sec:non-claims}

To maintain epistemic discipline, we explicitly state what Phase I does \emph{not} establish:

\subsection{What Phase I Does NOT Establish}

\begin{itemize}[leftmargin=1.5em]
\item \textbf{Capability claims:} No claim that the system ``understands'' or ``reasons'' in any general sense.
\item \textbf{Convergence:} No claim that RFL converges under Phase I conditions. All runs failed the variance gate.
\item \textbf{Threshold validity:} Thresholds are frozen parameters, not validated optima.
\item \textbf{Generalization:} No out-of-distribution testing was performed.
\item \textbf{Real-world applicability:} Only synthetic harness data was used.
\end{itemize}

\subsection{SHADOW Mode Semantics}

All Phase I experiments operate in SHADOW mode: verification results are \emph{observational and non-blocking}. The system records what happened but does not gate production decisions.

\subsection{Phase Quarantine}

Phase I and Phase II are strictly separated:
\begin{itemize}[leftmargin=1.5em]
\item \textbf{Phase I:} Assumes ideal verifier, hermetic environment, synthetic data
\item \textbf{Phase II:} Tests governance stability under auxiliary perturbation (frozen but not executed)
\end{itemize}

No Phase II claims are made in this work. Phase II specification is frozen pending execution authorization.

%=============================================================================
\section{Future Work}
\label{sec:future_work}

Future work will focus on integrating RFL to observe if reflexive feedback can dampen oscillatory states in the decision boundary and achieve measurable reductions in abstention rates and improvements in convergence latency.

\subsection{Phase II Calibration}

Phase II of the calibration program addresses governance stability: specifically, whether the governance verdict (failure codes, claim level) is invariant under perturbation of auxiliary parameters not part of the frozen predicate set. The Phase II specification is frozen, but execution has not yet occurred. No claims regarding governance invariance or sensitivity are made in this work. Phase II results, when available, will be reported separately and will not retroactively modify the Phase I conclusions presented here.

%=============================================================================
\section{Conclusion}
\label{sec:conclusion}

\projectname{} demonstrates that ledger-attested learning is technically feasible. Phase I successfully established:

\begin{enumerate}[leftmargin=1.5em]
\item A working pipeline from proof generation through dual attestation to policy feedback
\item Measurement infrastructure for $\Delta p$ and variance metrics
\item Fail-closed governance that correctly triggers under out-of-bounds conditions
\item Explicit non-claims and scope boundaries that enable honest assessment
\end{enumerate}

The contribution is infrastructural, not empirical. We have built the substrate; demonstrating capability on that substrate is future work.

The system stands as proof-of-concept for a new paradigm: \emph{learning from verified truth}. Whether this paradigm scales to complex reasoning remains an open question. What Phase I establishes is that the question can now be asked with rigor.

%=============================================================================
\appendix

\section{Evidence Pack}
\label{app:evidence}

The following manifests provide cryptographic verification of the experimental runs.

\begin{table}[h]
    \centering
    \begin{tabular}{l l}
        \toprule
        \textbf{Artifact} & \textbf{SHA-256 (Short)} \\
        \midrule
        \Ht{} Snapshot & \valHtShortHash \\
        Mirror Audit Report & \texttt{artifacts/mirror/mirror\_report.json} \\
        Drift Table & \texttt{drift\_table.json} \\
        \bottomrule
    \end{tabular}
    \caption{Cryptographic manifest of key experimental artifacts.}
    \label{tab:manifest}
\end{table}

All artifacts are available in the evidence directory: \texttt{docs/evidence/cal\_exp\_3/}.

%=============================================================================
\section{Formal Proofs}
\label{app:formal-proofs}

This appendix provides complete proofs for the formal properties stated in Section~\ref{sec:formal-properties}.

\subsection{Proof of Proposition~\ref{prop:sa-formal} (RFL as Stochastic Approximation)}

\begin{proof}
The update $\pi_{t+1} = \pi_t \oplus \eta_t \Phi(\mathcal{V}(e_t), \pi_t)$ can be interpreted as an additive update in a suitable parameterization. Define $h(\pi) := \mathbb{E}[\Phi(\mathcal{V}(e), \pi) \mid \pi]$, the expected update given the current policy. Define the noise term:
\[
M_{t+1} := \Phi(\mathcal{V}(e_t), \pi_t) - h(\pi_t)
\]
By construction, $\mathbb{E}[M_{t+1} \mid \mathcal{F}_t] = h(\pi_t) - h(\pi_t) = 0$, so $M_{t+1}$ is a martingale difference adapted to $\mathcal{F}_t$. The update becomes:
\[
\pi_{t+1} = \pi_t + \eta_t \big( h(\pi_t) + M_{t+1} \big)
\]
This is the canonical Robbins--Monro stochastic approximation form. Under assumptions (bounded updates, martingale noise with bounded variance, Robbins--Monro stepsizes), standard SA theory applies. The function $h$ plays the role of the mean-field drift.

We emphasize: this establishes that RFL has SA \emph{structure}. Convergence to an equilibrium of $\dot{\pi} = h(\pi)$ requires that such an equilibrium exists and is attractive (e.g., $h$ is a contraction). These additional stability conditions are not claimed in Phase I.
\end{proof}

\subsection{Proof of Proposition~\ref{prop:ledger-formal} (Monotonicity and Tamper-Evidence)}

\begin{proof}
\textbf{(1) Monotonicity:} By definition, $K_t = \bigcup_{i=1}^t B_i$. When block $B_{t+1}$ is appended:
\[
K_{t+1} = \bigcup_{i=1}^{t+1} B_i = K_t \cup B_{t+1} \supseteq K_t
\]
Since blocks are append-only, no element of $K_t$ is removed. Thus $K_t \subseteq K_{t+1}$.

\textbf{(2) Tamper-Evidence:} Suppose an adversary produces $\tilde{\mathcal{L}} = (\tilde{B}_1, \ldots, \tilde{B}_T) \neq \mathcal{L}$ with the same head hash $\tilde{L}_T = L_T$. Let $j$ be the smallest index where $\tilde{B}_j \neq B_j$.

\textbf{Case A:} If $\tilde{B}_j$ differs from $B_j$ as a set, then Merkle root $\tilde{R}_j \neq R_j$ (deterministic construction). Given $L_j = \mathrm{Hash}(L_{j-1} \| R_j)$ and $\tilde{L}_j = \mathrm{Hash}(L_{j-1} \| \tilde{R}_j)$ (assuming prior blocks match), we have $\tilde{L}_j \neq L_j$ unless a hash collision occurs. By collision resistance, this happens with negligible probability.

\textbf{Case B:} If the sequence lengths differ (block omitted or inserted), the hash chain incorporates a different number of blocks, yielding $\tilde{L}_T \neq L_T$ by similar reasoning.

In both cases, $\tilde{L}_T = L_T$ implies a hash collision, which is computationally infeasible.
\end{proof}

\subsection{Proof of Lemma~\ref{lem:binding} (Dual Attestation Binding)}

\begin{proof}
The hash input is $m = \texttt{EPOCH:} \| r_t \| u_t$, where $r_t$ and $u_t$ are fixed-width 32-byte digests. This encoding is unambiguous: the prefix \texttt{EPOCH:} is a fixed string, and the 32-byte widths mean there is a one-to-one correspondence between pairs $(r_t, u_t)$ and input strings $m$.

Suppose $(r'_t, u'_t) \neq (r_t, u_t)$ yields the same hash:
\[
\mathrm{Hash}(\texttt{EPOCH:} \| r'_t \| u'_t) = \mathrm{Hash}(\texttt{EPOCH:} \| r_t \| u_t)
\]
Let $m' = \texttt{EPOCH:} \| r'_t \| u'_t$ and $m = \texttt{EPOCH:} \| r_t \| u_t$. Since the pairs differ and encoding is bijective, $m' \neq m$. Thus we have a hash collision, which is infeasible under collision resistance.

Therefore, $H_t$ uniquely commits to $(r_t, u_t)$. Once published, the agent cannot claim a different pair without finding a collision.
\end{proof}

%=============================================================================
\bibliographystyle{plain}
\bibliography{references}

\end{document}
