# Evidence Pack Specification — Phase X

**Document Version:** 1.0.0
**Status:** SPECIFICATION — Schema-Only (No Code)
**Classification:** INTERNAL — Pre-Launch Review Alignment
**Date:** 2025-12-10
**Dependencies:** Phase_X_Prelaunch_Review.md, Phase_X_P3_Spec.md, Phase_X_P4_Spec.md

---

## 1. Purpose

This specification defines the **Evidence Pack** structure for Phase X Pre-Launch Review artifacts. The Evidence Pack is the canonical bundle format for whitepaper submission, frontier lab outreach, and defense compliance demonstration.

**Scope:** Schema definitions only. No implementation code.

---

## 2. Evidence Pack Overview

### 2.1 Pack Structure

```
evidence_pack_{run_id}/
├── manifest.json                           # Bundle manifest with checksums
├── p3_synthetic/
│   ├── synthetic_raw.jsonl                 # Raw Δp observations (P3)
│   ├── stability_report.json               # Stability summary (P3)
│   ├── red_flag_matrix.json                # Red-flag event matrix (P3)
│   ├── metrics_windows.json                # Windowed metrics (P3)
│   └── tda_metrics.json                    # TDA metrics (SNS/PCS/DRS/HSS)
├── p4_shadow/
│   ├── divergence_log.jsonl                # Divergence records (P4)
│   ├── twin_trajectory.jsonl               # Twin predictions (P4)
│   ├── calibration_report.json             # Calibration summary (P4)
│   ├── divergence_distribution.json        # Divergence statistics (P4)
│   └── tda_metrics.json                    # TDA metrics for real+twin
├── visualizations/
│   ├── delta_p_trendline.svg               # Δp trajectory plot
│   ├── rsi_trajectory.svg                  # RSI trajectory plot
│   ├── omega_occupancy.svg                 # Ω-region occupancy plot
│   ├── twin_vs_reality.svg                 # P4 twin-vs-real overlay
│   ├── red_flag_heatmap.svg                # Red-flag severity heatmap
│   └── tda_dashboard.svg                   # 4-panel TDA metrics
├── compliance/
│   ├── compliance_narrative.md             # NDAA mapping narrative
│   └── audit_attestation.json              # Cryptographic attestation
└── schemas/
    └── bundle_schema_version.json          # Schema version manifest
```

### 2.2 Artifact Requirements Matrix

| Artifact | Source Phase | Required | Format | Validation Schema |
|----------|--------------|----------|--------|-------------------|
| `manifest.json` | Generator | YES | JSON | `evidence_bundle.schema.json` |
| `synthetic_raw.jsonl` | P3 | YES | JSONL | `first_light_synthetic_raw.schema.json` |
| `stability_report.json` | P3 | YES | JSON | `first_light_stability_report.schema.json` |
| `red_flag_matrix.json` | P3 | YES | JSON | `first_light_red_flag_matrix.schema.json` |
| `metrics_windows.json` | P3 | YES | JSON | `first_light_metrics_windows.schema.json` |
| `tda_metrics.json` (P3) | P3 | YES | JSON | `tda_metrics.schema.json` |
| `divergence_log.jsonl` | P4 | YES | JSONL | `p4_divergence_log.schema.json` |
| `twin_trajectory.jsonl` | P4 | YES | JSONL | `p4_twin_trajectory.schema.json` |
| `calibration_report.json` | P4 | YES | JSON | `p4_calibration_report.schema.json` |
| `divergence_distribution.json` | P4 | YES | JSON | `p4_divergence_distribution.schema.json` |
| `tda_metrics.json` (P4) | P4 | YES | JSON | `tda_metrics.schema.json` |
| `compliance_narrative.md` | Manual | YES | Markdown | N/A (prose) |
| `audit_attestation.json` | Generator | YES | JSON | `audit_attestation.schema.json` |
| Visualization SVGs | Generator | YES | SVG | N/A (visual) |

### 2.3 Optional: Proof Log Hash Snapshot

For provenance shadow-tracking, drop `compliance/proof_log_snapshot.json` in the pack root by running:

```
python -m scripts.first_light_proof_hash_snapshot \
    --proof-log <path/to/proof_log.jsonl> \
    --output <pack-root>/compliance/proof_log_snapshot.json
```

The snapshot records the canonical SHA-256 over sorted proof hashes, the source path, and entry count. When invoking the backend evidence builder, pass `--include-proof-snapshot` (or set `FIRST_LIGHT_INCLUDE_PROOF_SNAPSHOT=1` with `FIRST_LIGHT_PROOF_LOG=<path>`) to have the hook run automatically; failures remain SHADOW-only advisories and never block pack generation.

### 2.4 Status File Pairing (`first_light_status.json`)

The Evidence Pack is logically paired with `first_light_status.json`, a machine-readable summary generated by `scripts/generate_first_light_status.py`. When this file exists in the run directory, the Evidence Pack builder automatically embeds a **status reference** in the manifest.

**External Verifier Guidance:**

External verifiers should treat the Evidence Pack and `first_light_status.json` as a paired artifact set:

| Artifact | Purpose | Verification Role |
|----------|---------|-------------------|
| Evidence Pack (`manifest.json`) | Cryptographic bundle of P3/P4 artifacts with Merkle root | Provides artifact inventory, hashes, and completeness |
| `first_light_status.json` | Machine-readable status summary | Provides aggregate status flags, metric snapshots, and warnings |

**Cross-Link Structure:**

When `first_light_status.json` exists, the manifest includes:

```json
{
  "status_reference": {
    "path": "first_light_status.json",
    "sha256": "<64-char hex hash>",
    "schema_version": "1.0.0",
    "shadow_mode_ok": true
  }
}
```

**Verification Procedure:**

1. **Hash Verification:** Compute SHA-256 of `first_light_status.json` and compare to `status_reference.sha256`
2. **Status Consistency:** Verify `shadow_mode_ok: true` (required for SHADOW MODE compliance)
3. **Cross-Reference:** Status `metrics_snapshot` should align with artifact values in the pack

**Key Status Fields:**

| Field | Type | Description |
|-------|------|-------------|
| `shadow_mode_ok` | boolean | Overall SHADOW MODE compliance flag |
| `p3_harness_ok` | boolean | P3 run completed successfully |
| `p4_harness_ok` | boolean | P4 run completed successfully |
| `evidence_pack_ok` | boolean | Evidence pack integrity verified |
| `metrics_snapshot` | object | Key metrics (success rates, divergence, etc.) |
| `warnings` | array | Advisory warnings (observational only) |

**SHADOW MODE Contract:**

The status file is purely observational. Its values do not gate any decisions. External verifiers use it for:
- Quick status assessment before deep artifact inspection
- Automated CI/CD integration (JSON-parseable)
- Audit trail documentation

---

## 3. Bundle Manifest Schema

### 3.1 `evidence_bundle.schema.json`

The bundle manifest provides:
- Artifact inventory with checksums
- Generation metadata
- Schema version bindings
- Completeness verification

**Key Fields:**

| Field | Type | Description |
|-------|------|-------------|
| `bundle_id` | string | Unique identifier (UUID) |
| `bundle_version` | string | Semantic version of bundle format |
| `generated_at` | datetime | ISO8601 timestamp |
| `p3_run_id` | string | Source P3 run identifier |
| `p4_run_id` | string | Source P4 run identifier |
| `artifacts` | array | Artifact inventory with hashes |
| `completeness` | object | Required artifact checklist |
| `cryptographic_root` | string | Merkle root of all artifact hashes |

---

## 4. P3 Artifact Specifications

### 4.1 Synthetic Raw Observations

**File:** `p3_synthetic/synthetic_raw.jsonl`
**Schema:** `first_light_synthetic_raw.schema.json` (existing)

Per-cycle USLA state observations from synthetic generator.

### 4.2 Stability Report

**File:** `p3_synthetic/stability_report.json`
**Schema:** `first_light_stability_report.schema.json` (existing)

Aggregate stability metrics and success criteria evaluation.

### 4.3 Red Flag Matrix

**File:** `p3_synthetic/red_flag_matrix.json`
**Schema:** `first_light_red_flag_matrix.schema.json` (existing)

Complete red-flag event inventory with severity classification.

### 4.4 Metrics Windows

**File:** `p3_synthetic/metrics_windows.json`
**Schema:** `first_light_metrics_windows.schema.json` (existing)

Per-window aggregated metrics for time series analysis.

### 4.5 TDA Metrics (P3)

**File:** `p3_synthetic/tda_metrics.json`
**Schema:** `tda_metrics.schema.json` (NEW)

Topological Data Analysis metrics computed per window:

| Metric | Abbrev | Description | Range |
|--------|--------|-------------|-------|
| Structural Non-Triviality Score | SNS | Topological complexity of reasoning graph | [0, 1] |
| Persistence Coherence Score | PCS | Stability of persistent homology features | [0, 1] |
| Deviation-from-Reference Score | DRS | Drift from reference topology | [0, ∞) |
| Hallucination Stability Score | HSS | Composite hallucination risk indicator | [0, 1] |

---

## 5. P4 Artifact Specifications

### 5.1 Divergence Log

**File:** `p4_shadow/divergence_log.jsonl`
**Schema:** `p4_divergence_log.schema.json` (existing)

Per-cycle twin-vs-real divergence records.

### 5.2 Twin Trajectory

**File:** `p4_shadow/twin_trajectory.jsonl`
**Schema:** `p4_twin_trajectory.schema.json` (existing)

Shadow twin prediction records.

### 5.3 Calibration Report

**File:** `p4_shadow/calibration_report.json`
**Schema:** `p4_calibration_report.schema.json` (NEW)

Twin model calibration summary:

| Field | Description |
|-------|-------------|
| `twin_accuracy.success` | Prediction accuracy for success outcome |
| `twin_accuracy.blocked` | Prediction accuracy for block decision |
| `twin_accuracy.omega` | Prediction accuracy for Ω membership |
| `calibration_curve` | Predicted vs observed probability bins |
| `brier_score` | Probabilistic calibration metric |
| `recalibration_recommendations` | Suggested parameter adjustments |

### 5.4 Divergence Distribution

**File:** `p4_shadow/divergence_distribution.json`
**Schema:** `p4_divergence_distribution.schema.json` (NEW)

Statistical distribution of divergence magnitudes:

| Field | Description |
|-------|-------------|
| `distribution.mean` | Mean divergence |
| `distribution.std` | Standard deviation |
| `distribution.percentiles` | P50, P90, P95, P99 values |
| `severity_counts` | NONE/INFO/WARN/CRITICAL counts |
| `streak_analysis` | Consecutive divergence statistics |
| `correlation_matrix` | Divergence correlation with state variables |

### 5.5 TDA Metrics (P4)

**File:** `p4_shadow/tda_metrics.json`
**Schema:** `tda_metrics.schema.json` (shared with P3)

TDA metrics computed for both real and twin trajectories, with comparison.

### 5.6 P5 Real Telemetry Divergence Report (RESERVED)

**File:** `p4_shadow/p5_divergence_real.json`
**Schema:** TBD — `p5_divergence_real.schema.json`
**Status:** SLOT RESERVED — No schema or implementation yet

This slot is reserved for Phase X P5 real telemetry integration. When P5 is activated, this artifact will contain:

| Field (Provisional) | Description |
|---------------------|-------------|
| `telemetry_source` | Source identifier (`real` vs `mock`) |
| `divergence_baseline` | Comparison against P4 mock baseline |
| `twin_tracking_accuracy` | Real-world twin prediction accuracy |
| `manifold_validation` | Real telemetry manifold constraint checks |
| `warm_start_calibration` | Twin warm-start calibration metrics |

**Activation Criteria:**
- P5 specification finalized (`Real_Telemetry_Topology_Spec.md`)
- Real telemetry adapter validated
- Divergence baseline established from P4 mock runs

**SHADOW MODE:** This artifact, when implemented, will remain observational only.

---

## 6. Compliance Artifacts

### 6.1 Compliance Narrative

**File:** `compliance/compliance_narrative.md`
**Format:** Structured Markdown with NDAA mapping sections

Required Sections:

1. **Executive Summary** — One-paragraph compliance posture
2. **NDAA Requirement Mapping** — Table linking artifacts to requirements
3. **Risk-Informed Strategy Evidence** — Quantified stability metrics
4. **Technical Controls Evidence** — Threshold-based anomaly detection
5. **Human Override Capability** — Shadow-mode invariant documentation
6. **Auditability Evidence** — Append-only logs with cryptographic hashes

### 6.2 Audit Attestation

**File:** `compliance/audit_attestation.json`
**Schema:** `audit_attestation.schema.json` (NEW)

Cryptographic attestation of evidence pack integrity:

| Field | Description |
|-------|-------------|
| `attestation_id` | Unique attestation identifier |
| `timestamp` | Attestation generation time |
| `merkle_root` | Root hash of all artifacts |
| `artifact_hashes` | Individual artifact SHA-256 hashes |
| `signing_identity` | Attestation signer identity |
| `signature` | Ed25519 signature over merkle_root |

---

## 7. Visualization Specifications

### 7.1 Δp Trendline (`delta_p_trendline.svg`)

| Element | Specification |
|---------|---------------|
| X-axis | Cycle index (0–999) |
| Y-axis | Δp value (probability delta) |
| P3 Series | Single synthetic trajectory (blue) |
| P4 Series | Twin (blue dashed) + Real (orange solid) |
| Annotations | Red-flag events as vertical lines with severity color |
| Format | SVG vector, min 300 DPI equivalent |

### 7.2 RSI Trajectory (`rsi_trajectory.svg`)

| Element | Specification |
|---------|---------------|
| X-axis | Cycle index |
| Y-axis | RSI value [0.0, 1.0] |
| Threshold Lines | WARN (0.3), CRITICAL (0.2) horizontal |
| Shading | Green above WARN, Yellow WARN-CRITICAL, Red below CRITICAL |
| Caption | "Reflexive Stability Index trajectory" |

### 7.3 Ω Region Occupancy (`omega_occupancy.svg`)

| Element | Specification |
|---------|---------------|
| X-axis | Cycle index |
| Y-axis | Binary (0/1) or windowed percentage |
| Fill | Green=in Ω, Red=outside Ω |
| Summary Metric | Ω-occupancy rate annotation |

### 7.4 Twin vs Reality (`twin_vs_reality.svg`)

| Element | Specification |
|---------|---------------|
| Series 1 | Twin Δp (blue dashed) |
| Series 2 | Real Δp (orange solid) |
| Shaded | Divergence area between curves |
| Annotations | CRITICAL divergence events marked |

### 7.5 Red Flag Heatmap (`red_flag_heatmap.svg`)

| Element | Specification |
|---------|---------------|
| X-axis | Cycle index |
| Y-axis | Flag type (CDI-010, RSI_COLLAPSE, etc.) |
| Cell Color | Severity (white=none, yellow=INFO, orange=WARN, red=CRITICAL) |

### 7.6 TDA Dashboard (`tda_dashboard.svg`)

| Element | Specification |
|---------|---------------|
| Layout | 2x2 grid |
| Panel 1 | SNS time series |
| Panel 2 | PCS time series |
| Panel 3 | DRS time series |
| Panel 4 | HSS time series |
| Overlay | P3 envelope (gray band) + P4 real (orange) |

---

## 8. Schema Shell Definitions

The following schemas are defined as shell specifications. Full JSON Schema documents are provided in `docs/system_law/schemas/evidence_pack/`.

### 8.1 Evidence Bundle Schema (`evidence_bundle.schema.json`)

```yaml
$id: https://mathledger.org/schemas/evidence_pack/evidence_bundle.v1.0.0.json
title: Evidence Pack Bundle Manifest
version: 1.0.0
required:
  - bundle_id
  - bundle_version
  - generated_at
  - p3_run_id
  - p4_run_id
  - artifacts
  - completeness
  - cryptographic_root
properties:
  bundle_id: {type: string, format: uuid}
  bundle_version: {type: string, pattern: "^\\d+\\.\\d+\\.\\d+$"}
  generated_at: {type: string, format: date-time}
  p3_run_id: {type: string}
  p4_run_id: {type: string}
  artifacts: {type: array, items: {$ref: "#/$defs/artifact"}}
  completeness: {$ref: "#/$defs/completeness"}
  cryptographic_root: {type: string, pattern: "^sha256:[a-f0-9]{64}$"}
$defs:
  artifact:
    required: [path, sha256, size_bytes, schema_ref]
    properties:
      path: {type: string}
      sha256: {type: string, pattern: "^[a-f0-9]{64}$"}
      size_bytes: {type: integer, minimum: 0}
      schema_ref: {type: string}
  completeness:
    required: [p3_artifacts, p4_artifacts, visualizations, compliance]
    properties:
      p3_artifacts: {$ref: "#/$defs/artifact_checklist"}
      p4_artifacts: {$ref: "#/$defs/artifact_checklist"}
      visualizations: {$ref: "#/$defs/artifact_checklist"}
      compliance: {$ref: "#/$defs/artifact_checklist"}
  artifact_checklist:
    type: object
    additionalProperties: {type: boolean}
```

### 8.2 TDA Metrics Schema (`tda_metrics.schema.json`)

```yaml
$id: https://mathledger.org/schemas/evidence_pack/tda_metrics.v1.0.0.json
title: TDA Metrics (SNS/PCS/DRS/HSS)
version: 1.0.0
required:
  - schema_version
  - run_id
  - source_phase
  - window_size
  - windows
  - summary
properties:
  schema_version: {type: string, const: "1.0.0"}
  run_id: {type: string}
  source_phase: {type: string, enum: ["P3", "P4"]}
  window_size: {type: integer, minimum: 1}
  windows: {type: array, items: {$ref: "#/$defs/tda_window"}}
  summary: {$ref: "#/$defs/tda_summary"}
$defs:
  tda_window:
    required: [window_index, start_cycle, end_cycle, metrics]
    properties:
      window_index: {type: integer, minimum: 0}
      start_cycle: {type: integer, minimum: 1}
      end_cycle: {type: integer, minimum: 1}
      metrics:
        required: [sns, pcs, drs, hss]
        properties:
          sns: {type: number, minimum: 0, maximum: 1}
          pcs: {type: number, minimum: 0, maximum: 1}
          drs: {type: number, minimum: 0}
          hss: {type: number, minimum: 0, maximum: 1}
  tda_summary:
    properties:
      sns: {$ref: "#/$defs/metric_stats"}
      pcs: {$ref: "#/$defs/metric_stats"}
      drs: {$ref: "#/$defs/metric_stats"}
      hss: {$ref: "#/$defs/metric_stats"}
  metric_stats:
    properties:
      mean: {type: number}
      min: {type: number}
      max: {type: number}
      std: {type: number}
      trend_slope: {type: number}
```

### 8.3 P4 Calibration Report Schema (`p4_calibration_report.schema.json`)

```yaml
$id: https://mathledger.org/schemas/phase_x_p4/calibration_report.v1.0.0.json
title: P4 Twin Calibration Report
version: 1.0.0
required:
  - schema_version
  - run_id
  - twin_accuracy
  - calibration
  - recommendations
properties:
  schema_version: {type: string, const: "1.0.0"}
  run_id: {type: string}
  twin_accuracy:
    required: [success, blocked, omega, hard_ok]
    properties:
      success: {type: number, minimum: 0, maximum: 1}
      blocked: {type: number, minimum: 0, maximum: 1}
      omega: {type: number, minimum: 0, maximum: 1}
      hard_ok: {type: number, minimum: 0, maximum: 1}
  calibration:
    properties:
      brier_score: {type: number, minimum: 0, maximum: 1}
      calibration_curve: {type: array, items: {$ref: "#/$defs/calibration_bin"}}
      reliability_diagram_data: {type: array}
  recommendations:
    type: array
    items:
      properties:
        parameter: {type: string}
        current_value: {type: number}
        suggested_value: {type: number}
        rationale: {type: string}
$defs:
  calibration_bin:
    properties:
      predicted_prob_low: {type: number}
      predicted_prob_high: {type: number}
      observed_frequency: {type: number}
      sample_count: {type: integer}
```

### 8.4 P4 Divergence Distribution Schema (`p4_divergence_distribution.schema.json`)

```yaml
$id: https://mathledger.org/schemas/phase_x_p4/divergence_distribution.v1.0.0.json
title: P4 Divergence Distribution
version: 1.0.0
required:
  - schema_version
  - run_id
  - total_cycles
  - distribution
  - severity_counts
  - streak_analysis
properties:
  schema_version: {type: string, const: "1.0.0"}
  run_id: {type: string}
  total_cycles: {type: integer, minimum: 1}
  distribution:
    required: [mean, std, min, max, percentiles]
    properties:
      mean: {type: number}
      std: {type: number}
      min: {type: number}
      max: {type: number}
      percentiles:
        properties:
          p50: {type: number}
          p90: {type: number}
          p95: {type: number}
          p99: {type: number}
      histogram_bins: {type: array, items: {$ref: "#/$defs/histogram_bin"}}
  severity_counts:
    required: [NONE, INFO, WARN, CRITICAL]
    properties:
      NONE: {type: integer, minimum: 0}
      INFO: {type: integer, minimum: 0}
      WARN: {type: integer, minimum: 0}
      CRITICAL: {type: integer, minimum: 0}
  severity_rates:
    properties:
      info_rate: {type: number, minimum: 0, maximum: 1}
      warn_rate: {type: number, minimum: 0, maximum: 1}
      critical_rate: {type: number, minimum: 0, maximum: 1}
  streak_analysis:
    properties:
      max_streak: {type: integer, minimum: 0}
      mean_streak: {type: number}
      streak_distribution: {type: array}
  correlation_matrix:
    type: object
    additionalProperties: {type: number}
$defs:
  histogram_bin:
    properties:
      bin_low: {type: number}
      bin_high: {type: number}
      count: {type: integer}
      density: {type: number}
```

### 8.5 Audit Attestation Schema (`audit_attestation.schema.json`)

```yaml
$id: https://mathledger.org/schemas/evidence_pack/audit_attestation.v1.0.0.json
title: Evidence Pack Audit Attestation
version: 1.0.0
required:
  - attestation_id
  - timestamp
  - bundle_id
  - merkle_root
  - artifact_hashes
properties:
  attestation_id: {type: string, format: uuid}
  timestamp: {type: string, format: date-time}
  bundle_id: {type: string, format: uuid}
  merkle_root: {type: string, pattern: "^sha256:[a-f0-9]{64}$"}
  artifact_hashes:
    type: array
    items:
      required: [path, sha256]
      properties:
        path: {type: string}
        sha256: {type: string, pattern: "^[a-f0-9]{64}$"}
  signing_identity:
    properties:
      name: {type: string}
      public_key: {type: string}
      key_type: {type: string, enum: ["ed25519", "secp256k1"]}
  signature:
    properties:
      algorithm: {type: string}
      value: {type: string}
      signed_data_hash: {type: string}
```

### 8.6 Compliance Narrative Schema (`compliance_narrative.schema.json`)

```yaml
$id: https://mathledger.org/schemas/evidence_pack/compliance_narrative.v1.0.0.json
title: Compliance Narrative Metadata
description: Structural metadata for compliance narrative (prose in Markdown)
version: 1.0.0
required:
  - schema_version
  - document_id
  - sections
  - ndaa_mapping
properties:
  schema_version: {type: string, const: "1.0.0"}
  document_id: {type: string}
  generated_at: {type: string, format: date-time}
  sections:
    type: array
    items:
      required: [id, title, required]
      properties:
        id: {type: string}
        title: {type: string}
        required: {type: boolean}
        word_count_min: {type: integer}
        artifact_refs: {type: array, items: {type: string}}
  ndaa_mapping:
    type: array
    items:
      required: [requirement_id, requirement_text, artifact_refs, narrative_section]
      properties:
        requirement_id: {type: string}
        requirement_text: {type: string}
        artifact_refs: {type: array, items: {type: string}}
        narrative_section: {type: string}
        compliance_status: {type: string, enum: ["COMPLIANT", "PARTIAL", "PENDING"]}
```

---

## 9. NDAA Requirement Mapping

### 9.1 Artifact-to-Requirement Matrix

| NDAA Requirement | Artifact Evidence | Compliance Mechanism |
|------------------|-------------------|----------------------|
| **Risk-informed strategy** | `stability_report.json`, `divergence_distribution.json` | Quantified stability envelopes enable risk-aware decisions |
| **Technical controls** | `red_flag_matrix.json`, Shadow-mode invariant | Threshold-based anomaly detection, architectural separation |
| **Human override capability** | Shadow-mode contract, P4 no-control-authority | Architecture forbids autonomous intervention |
| **Auditability** | `divergence_log.jsonl`, `audit_attestation.json` | Append-only logs with cryptographic hashes |
| **Verification independence** | `tda_metrics.json`, Lean proofs | External verification infrastructure |

### 9.2 Compliance Checklist

| Checkpoint | Evidence Artifact | Verification Method |
|------------|------------------|---------------------|
| CP-01 | `audit_attestation.json` | Signature verification |
| CP-02 | `merkle_root` in manifest | Recompute from artifacts |
| CP-03 | Shadow-mode logs | Check `mode: SHADOW` in all records |
| CP-04 | Red-flag actions | Verify `action: LOGGED_ONLY` |
| CP-05 | TDA metrics present | Schema validation |

---

## 10. Evidence Pack Generation Protocol

### 10.1 Prerequisites

| Prerequisite | Verification |
|--------------|--------------|
| P3 run completed | `stability_report.json` exists with `cycles_completed ≥ 1000` |
| P4 run completed | `calibration_report.json` exists with `cycles_completed ≥ 1000` |
| TDA metrics computed | Both `tda_metrics.json` files present |
| Visualizations rendered | All 6 SVG files in `visualizations/` |
| Compliance narrative drafted | `compliance_narrative.md` exists |

### 10.2 Generation Steps

1. **Collect Artifacts**
   - Copy P3 outputs to `p3_synthetic/`
   - Copy P4 outputs to `p4_shadow/`
   - Copy visualizations to `visualizations/`
   - Copy compliance docs to `compliance/`

2. **Compute Artifact Hashes**
   - SHA-256 for each artifact file
   - Store in manifest `artifacts[]`

3. **Build Merkle Tree**
   - Sort artifact hashes lexicographically
   - Compute binary Merkle tree
   - Store root as `cryptographic_root`

4. **Verify Completeness**
   - Check all required artifacts present
   - Populate `completeness` checklist

5. **Generate Attestation**
   - Create `audit_attestation.json`
   - Sign merkle_root with attestation key

6. **Write Manifest**
   - Generate `manifest.json` with all metadata

7. **Archive Bundle**
   - Create `evidence_pack_{bundle_id}.tar.gz`
   - Include checksum file

---

## 11. Schema File Locations

### 11.1 New Schemas (to be created)

| Schema | Path |
|--------|------|
| Evidence Bundle | `docs/system_law/schemas/evidence_pack/evidence_bundle.schema.json` |
| TDA Metrics | `docs/system_law/schemas/evidence_pack/tda_metrics.schema.json` |
| P4 Calibration Report | `docs/system_law/schemas/phase_x_p4/p4_calibration_report.schema.json` |
| P4 Divergence Distribution | `docs/system_law/schemas/phase_x_p4/p4_divergence_distribution.schema.json` |
| Audit Attestation | `docs/system_law/schemas/evidence_pack/audit_attestation.schema.json` |
| Compliance Narrative | `docs/system_law/schemas/evidence_pack/compliance_narrative.schema.json` |

### 11.2 Existing Schemas (referenced)

| Schema | Path |
|--------|------|
| P3 Synthetic Raw | `docs/system_law/schemas/first_light/first_light_synthetic_raw.schema.json` |
| P3 Stability Report | `docs/system_law/schemas/first_light/first_light_stability_report.schema.json` |
| P3 Red Flag Matrix | `docs/system_law/schemas/first_light/first_light_red_flag_matrix.schema.json` |
| P3 Metrics Windows | `docs/system_law/schemas/first_light/first_light_metrics_windows.schema.json` |
| P4 Divergence Log | `docs/system_law/schemas/phase_x_p4/p4_divergence_log.schema.json` |
| P4 Twin Trajectory | `docs/system_law/schemas/phase_x_p4/p4_twin_trajectory.schema.json` |

---

## 11. Performance Summary (Advisory Cross-Check)

### 11.1 Overview

The **Performance Summary** provides a compact cross-check of performance governance for First Light evidence packs. This summary is **advisory only** and does not gate any decisions in Phase X.

**Status:** ACTIVE — Included in Evidence Pack for Phase X.

**Purpose:** External reviewers can use this summary to understand performance risk context without requiring deep knowledge of the performance governance subsystem.

### 11.2 Evidence Binding Location

The performance summary attaches to the main evidence structure at:

```
evidence["governance"]["uplift_perf_summary"]
```

### 11.3 Summary Schema

```json
{
  "schema_version": "1.0.0",
  "perf_risk": "LOW" | "MEDIUM" | "HIGH",
  "slices_with_regressions": ["slice_name_1", "slice_name_2", ...],
  "slices_blocking_uplift": ["slice_name_1", ...]
}
```

### 11.4 Field Descriptions

| Field | Type | Description |
|-------|------|-------------|
| `schema_version` | string | Schema version identifier (currently "1.0.0") |
| `perf_risk` | string | Overall performance risk level: `LOW` (nominal), `MEDIUM` (monitoring), or `HIGH` (attention required) |
| `slices_with_regressions` | array | List of slice names where performance regressions were detected during the First Light run |
| `slices_blocking_uplift` | array | List of slice names where performance issues are blocking uplift (requires both perf regression AND high budget risk) |

### 11.5 Interpretation Guide

**For External Reviewers:**

- **`perf_risk: "LOW"`** — Performance subsystem reports nominal conditions. No performance concerns identified.
- **`perf_risk: "MEDIUM"`** — Performance subsystem detected some regressions or elevated risk. Review `slices_with_regressions` for details. This is advisory; other dimensions (budget, metrics) may still indicate readiness.
- **`perf_risk: "HIGH"`** — Performance subsystem recommends caution. Check `slices_blocking_uplift` for slices where performance is actively blocking uplift decisions.

**Context for First Light Runs:**

In a First Light evidence pack, the performance summary reflects performance governance assessment across the P3/P4 run cycle. It aggregates:
- Cycle execution latency trends
- Component-level SLO compliance (scoring, derivation, verification, persistence, policy)
- Regression detection against baseline thresholds

**Advisory Nature:**

This summary is **not a gate**. It provides context for:
- Understanding performance risk posture
- Identifying slices that may need performance optimization before production deployment
- Cross-checking against other evidence dimensions (budget, metrics, stability)

The Uplift Council (Section 12) aggregates performance data with budget and metrics to make unified uplift decisions. This summary is a standalone cross-check for external review.

### 11.6 Example

```json
{
  "schema_version": "1.0.0",
  "perf_risk": "HIGH",
  "slices_with_regressions": ["slice_uplift_goal", "slice_uplift_sparse"],
  "slices_blocking_uplift": ["slice_uplift_goal"]
}
```

**Interpretation:** The performance subsystem detected regressions on two slices (`slice_uplift_goal` and `slice_uplift_sparse`). One slice (`slice_uplift_goal`) has performance issues that, combined with budget risk, are blocking uplift. An external reviewer should read this as: "Performance subsystem recommends caution on `slice_uplift_goal`; investigate before production deployment."

### 11.7 Performance Calibration Summary

The **Performance Calibration Summary** provides Δp trajectory analysis from CAL-EXP-1 and CAL-EXP-2 calibration experiments. This summary classifies the uplift shape observed during calibration runs.

**Status:** ACTIVE — Included in Evidence Pack for Phase X.

**Purpose:** External reviewers can use this summary to understand the learning curve characteristics observed during calibration experiments, providing context for performance governance assessment.

#### 11.7.1 Evidence Binding Location

The calibration summary attaches to the main evidence structure at:

```
evidence["governance"]["perf_calibration_summary"]
```

#### 11.7.2 Summary Schema

```json
{
  "schema_version": "1.0.0",
  "experiments": {
    "cal_exp1": {
      "windows_count": 10,
      "trajectory_length": 10
    },
    "cal_exp2": {
      "windows_count": 20,
      "trajectory_length": 20
    }
  },
  "uplift_shapes": {
    "cal_exp1": "monotonic" | "plateau" | "oscillatory",
    "cal_exp2": "monotonic" | "plateau" | "oscillatory"
  },
  "delta_p_trajectories": {
    "cal_exp1": [0.1, 0.2, 0.3, ...],
    "cal_exp2": [0.3, 0.31, 0.29, ...]
  },
  "calibration_readiness_hints": {
    "cal_exp1": {
      "hint_schema_version": "1.0.0",
      "hint": "READY_FOR_EXTENDED_RUN" | "NEEDS_PARAMETER_TUNING" | "UNSTABLE_CALIBRATION",
      "basis": "delta_p_trajectory_shape",
      "scope_note": "ADVISORY_ONLY_NO_GATE"
    },
    "cal_exp2": {
      "hint_schema_version": "1.0.0",
      "hint": "READY_FOR_EXTENDED_RUN" | "NEEDS_PARAMETER_TUNING" | "UNSTABLE_CALIBRATION",
      "basis": "delta_p_trajectory_shape",
      "scope_note": "ADVISORY_ONLY_NO_GATE"
    }
  }
}
```

#### 11.7.3 Field Descriptions

| Field | Type | Description |
|-------|------|-------------|
| `schema_version` | string | Schema version identifier (currently "1.0.0") |
| `experiments` | object | Metadata about each calibration experiment (CAL-EXP-1, CAL-EXP-2) |
| `uplift_shapes` | object | Uplift shape classification per experiment: `monotonic` (consistent trend), `plateau` (stable), or `oscillatory` (alternating) |
| `delta_p_trajectories` | object | Raw Δp values per experiment window (for detailed analysis) |
| `calibration_readiness_hints` | object | Calibration readiness hint per experiment, each containing `hint` (string), `basis` (string), and `scope_note` (string) |

#### 11.7.4 Uplift Shape Interpretation

**For External Reviewers:**

- **`monotonic`** — Δp trajectory shows a consistent increasing or decreasing trend. Indicates stable learning dynamics during calibration.
- **`plateau`** — Δp trajectory remains relatively stable (low variance). Indicates convergence or stable performance during calibration window.
- **`oscillatory`** — Δp trajectory shows alternating increases and decreases. May indicate policy instability, noise, or regime transitions during calibration.

**Context for Calibration Experiments:**

The calibration summary analyzes Δp (delta_p) values computed from calibration experiment windows. These values represent the divergence between real and twin model predictions, providing insight into learning curve dynamics.

**Heuristic Pattern Tags (Non-Authoritative):** Some CAL-EXP-1 window reports may additionally include per-window `pattern_tag` labels and a `pattern_tagging` metadata block. These are **heuristic, provisional** SHADOW-mode annotations intended only to aid interpretation; they must not be treated as validated causal diagnoses or used as gates. If required window fields are missing, implementations must degrade to `pattern_tag: "NONE"` and record `pattern_tag_note: "INSUFFICIENT_DATA"` to avoid overclaim.

#### 11.7.5 Calibration Readiness Hints

The **calibration readiness hints** provide advisory guidance about calibration readiness based on observed uplift shapes. These hints are **advisory only** and do not gate any decisions.

**Transparency Fields:**

Each readiness hint includes three fields for auditor transparency:

- **`hint`**: The readiness hint string (`READY_FOR_EXTENDED_RUN`, `NEEDS_PARAMETER_TUNING`, or `UNSTABLE_CALIBRATION`)
- **`basis`**: Evidence basis string indicating what the hint is based on (currently `"delta_p_trajectory_shape"`). This field clarifies that the hint is derived from the shape classification of the Δp trajectory, not from other performance metrics or governance signals.
- **`scope_note`**: Scope clarification string (always `"ADVISORY_ONLY_NO_GATE"`). This field explicitly states that the hint is advisory only and does not gate any decisions, preventing overclaiming about the hint's authority.

These transparency fields ensure that auditors understand both what the hint is (based on Δp trajectory shape) and what it is not (a gating signal or authoritative decision). The `basis` field prevents confusion about the evidence source, and the `scope_note` field prevents misinterpretation of the hint as a blocking or allowing signal.

**Readiness Mapping:**

| Uplift Shape | Readiness Hint | Interpretation |
|--------------|----------------|----------------|
| `monotonic` (decreasing) | `READY_FOR_EXTENDED_RUN` | Divergence improving consistently; calibration appears stable and ready for extended runs |
| `monotonic` (increasing) | `READY_FOR_EXTENDED_RUN` | Consistent trend observed; calibration dynamics appear predictable |
| `plateau` | `NEEDS_PARAMETER_TUNING` | Trajectory has converged to stable state; may benefit from parameter adjustments to resume learning |
| `oscillatory` | `UNSTABLE_CALIBRATION` | Trajectory shows alternating patterns; calibration may be unstable and requires investigation |

**Status JSON Integration:**

A one-line status hint can be extracted using `extract_calibration_readiness_status()`, which returns the most concerning hint across all experiments (priority: `UNSTABLE_CALIBRATION` > `NEEDS_PARAMETER_TUNING` > `READY_FOR_EXTENDED_RUN`). This provides a compact status indicator for status JSON files.

**Advisory Nature:**

The readiness hints are **not a gate**. They provide observational context for:
- Understanding calibration experiment outcomes
- Identifying learning curve patterns
- Providing guidance for next steps (extended runs, parameter tuning, investigation)
- Cross-checking against other performance metrics

**Explicit: No Gating in Phase X/P5 POC**

The readiness hints are explicitly **not a gate** in Phase X or P5 POC. They are:
- Purely observational
- Advisory only
- Do not influence calibration experiment execution
- Do not block or allow any operations
- Provide context for external review only

#### 11.7.6 Example

```json
{
  "schema_version": "1.0.0",
  "experiments": {
    "cal_exp1": {
      "windows_count": 10,
      "trajectory_length": 10
    },
    "cal_exp2": {
      "windows_count": 20,
      "trajectory_length": 20
    }
  },
  "uplift_shapes": {
    "cal_exp1": "monotonic",
    "cal_exp2": "plateau"
  },
  "delta_p_trajectories": {
    "cal_exp1": [0.5, 0.4, 0.3, 0.2, 0.1, 0.09, 0.08, 0.07, 0.06, 0.05],
    "cal_exp2": [0.3, 0.31, 0.29, 0.30, 0.30, 0.29, 0.31, 0.30, 0.30, 0.30]
  },
  "calibration_readiness_hints": {
    "cal_exp1": {
      "hint": "READY_FOR_EXTENDED_RUN",
      "basis": "delta_p_trajectory_shape",
      "scope_note": "ADVISORY_ONLY_NO_GATE"
    },
    "cal_exp2": {
      "hint": "NEEDS_PARAMETER_TUNING",
      "basis": "delta_p_trajectory_shape",
      "scope_note": "ADVISORY_ONLY_NO_GATE"
    }
  }
}
```

**Interpretation:** CAL-EXP-1 shows a monotonic decreasing trajectory (divergence improving from 0.5 to 0.05), indicating stable learning dynamics. The readiness hint (`READY_FOR_EXTENDED_RUN`) is based on `delta_p_trajectory_shape` and is explicitly marked as `ADVISORY_ONLY_NO_GATE`, meaning it provides guidance but does not gate any decisions. CAL-EXP-2 shows a plateau trajectory (stable around 0.30), indicating convergence. The readiness hint (`NEEDS_PARAMETER_TUNING`) is also based on `delta_p_trajectory_shape` and is advisory only. An external reviewer should read this as: "CAL-EXP-1 shows promising calibration dynamics based on Δp trajectory shape; CAL-EXP-2 has converged and may benefit from parameter adjustments. Both hints are advisory only and do not gate any decisions."

**Status JSON Integration Example:**

```json
{
  "calibration_readiness_status": "NEEDS_PARAMETER_TUNING"
}
```

The status JSON provides a one-line hint using priority ordering (most concerning hint across all experiments). In this example, `NEEDS_PARAMETER_TUNING` is selected because it is more concerning than `READY_FOR_EXTENDED_RUN` (priority: `UNSTABLE_CALIBRATION` > `NEEDS_PARAMETER_TUNING` > `READY_FOR_EXTENDED_RUN`).

---

## 12. Budget Storyline Summary (Advisory Narrative)

### 12.1 Overview

The **Budget Storyline Summary** provides a compact, temporal narrative of budget invariant health across First Light runs. This summary is **advisory only** and does not gate any decisions in Phase X.

**Status:** ACTIVE — Included in Evidence Pack for Phase X.

**Purpose:** External reviewers can use this summary to understand budget governance trends and early-warning signals as a temporal narrative, rather than relying solely on point-in-time health checks. This summary represents the "Energy Law" of First-Light runs, tracking how budget invariants evolve over time.

### 12.2 Evidence Binding Location

The budget storyline summary attaches to the main evidence structure at:

```
evidence["governance"]["budget_storyline_summary"]
```

### 12.3 Summary Schema

```json
{
  "schema_version": "1.0.0",
  "combined_status": "OK" | "WARN" | "BLOCK",
  "stability_index": 0.0-1.0,
  "episodes_count": 0,
  "projection_class": "STABLE" | "DRIFTING" | "VOLATILE" | "UNKNOWN",
  "key_structural_events": [
    "INV-BUD-1 failure detected (3 occurrence(s))",
    "stability_index dropped below 0.95 at run 50",
    ...
  ]
}
```

### 12.4 Field Descriptions

| Field | Type | Description |
|-------|------|-------------|
| `schema_version` | string | Schema version identifier (currently "1.0.0") |
| `combined_status` | string | Aggregated status: `OK` (healthy), `WARN` (monitoring), or `BLOCK` (attention required) |
| `stability_index` | float | Stability metric in [0.0, 1.0], where 1.0 indicates perfect consistency across recent runs |
| `episodes_count` | int | Number of budget health episodes identified in the storyline (runs with concerning patterns) |
| `projection_class` | string | BNH-Φ projected stability trajectory: `STABLE` (≥80 health), `DRIFTING` (70-80), `VOLATILE` (<70), or `UNKNOWN` (insufficient data) |
| `key_structural_events` | array | First 5 most significant structural events (invariant violations, stability drops, etc.) in chronological order |

### 12.5 Interpretation Guide

**For External Reviewers:**

The budget storyline summary should be read as a **temporal narrative** rather than a single scalar health check. Key interpretation points:

- **`combined_status: "BLOCK"` with otherwise OK budget tiles** — This indicates an early-warning, long-horizon signal. The budget invariants timeline has detected degradation patterns (e.g., recent FAIL status, low stability_index < 0.5) that may not yet be reflected in point-in-time metrics. This is **not a gate**; it flags a trend requiring human review before production deployment.

- **`projection_class`** — Read this as a forward-looking trajectory. `STABLE` suggests budget health is expected to remain strong; `DRIFTING` indicates potential concerns ahead; `VOLATILE` suggests significant instability in recent runs. This projection uses BNH-Φ (Budget Narrative Hardening) deterministic algorithms (linear extrapolation, threshold rules) based on the last 5 health scores and trend statuses.

- **`key_structural_events`** — These are the most significant events in the budget invariant timeline, ordered chronologically. They tell the story of how budget health evolved. Example: "INV-BUD-1 failure detected (3 occurrence(s))" indicates post-exhaustion processing violations occurred three times across the run sequence.

- **`stability_index`** — A consistency metric: 1.0 means all recent runs had the same status (perfect stability), lower values indicate mixed OK/WARN/FAIL patterns. Values below 0.7 suggest volatile behavior requiring investigation.

**Context for First Light Runs:**

In a First Light evidence pack, the budget storyline summary reflects budget invariant health across the P3/P4 run cycle. It aggregates:
- INV-BUD-1 through INV-BUD-5 invariant checks (no post-exhaustion processing, hard caps, monotonic budget, timeout tracking, skipped statement tracking)
- Stability trends across multiple runs
- Projected future trajectory using BNH-Φ algorithms
- Key structural events that shaped the budget health narrative

**Advisory Nature:**

This summary is **not a gate**. It provides context for:
- Understanding budget governance trend evolution
- Identifying early-warning signals before they become critical
- Reading budget health as a temporal narrative (what happened, what's projected)
- Cross-checking against other evidence dimensions (performance, metrics, stability)

**Example Scenario:**

A First Light run shows `combined_status: "BLOCK"` but individual budget tiles (budget_invariants, budget_invariants_director_panel) may show `status: "OK"` or `status_light: "GREEN"`. This discrepancy indicates:

1. **Point-in-time status is OK** — Current budget metrics meet thresholds
2. **Temporal trend is concerning** — Recent runs show FAIL status or declining stability_index
3. **Early-warning signal** — The system is detecting degradation before it becomes critical
4. **Requires human review** — Not an automatic block, but a flag for investigation

The `key_structural_events` and `projection_class` fields provide the narrative context: what happened, and what's likely to happen next.

---

## 13. PRNG First-Light Summary (Long-Horizon Footnote)

### 13.1 Overview

The **PRNG First-Light Summary** provides a compact, long-horizon stability signal about PRNG (Pseudo-Random Number Generator) governance drift across multiple runs. This summary is **purely observational** and does not gate any decisions in Phase X or early P5.

**Status:** ACTIVE — Included in Evidence Pack for Phase X.

**Purpose:** External reviewers can use this summary to understand PRNG regime stability as a long-horizon footnote, providing context about randomness governance trends without requiring deep knowledge of the PRNG subsystem.

### 13.2 Evidence Binding Location

The PRNG First-Light summary attaches to the main evidence structure at:

```
evidence["governance"]["prng_governance"]["first_light_summary"]
```

### 13.3 Summary Schema

```json
{
  "schema_version": "1.0.0",
  "drift_status": "STABLE" | "DRIFTING" | "VOLATILE",
  "frequent_violations": {
    "R1": 5,
    "R2": 3,
    ...
  },
  "status": "OK" | "WARN" | "BLOCK",
  "blocking_rules": ["R1", "R2", ...],
  "total_runs": 42
}
```

### 13.4 Field Descriptions

| Field | Type | Description |
|-------|------|-------------|
| `schema_version` | string | Schema version identifier (currently "1.0.0") |
| `drift_status` | string | Long-horizon drift classification: `STABLE` (no frequent violations), `DRIFTING` (1-2 frequent violations), or `VOLATILE` (≥3 frequent violations) |
| `frequent_violations` | object | Dictionary mapping rule IDs to occurrence counts. Only rules that appeared as violations in ≥3 runs are included. |
| `status` | string | Current governance status: `OK` (no blocking rules), `WARN` (frequent violations but no blocks), or `BLOCK` (blocking rules detected) |
| `blocking_rules` | array | List of rule IDs that caused BLOCK status (from runs with BLOCK severity violations) |
| `total_runs` | integer | Total number of PRNG governance runs analyzed |

### 13.5 Interpretation Guide

**For External Reviewers:**

The PRNG First-Light summary should be read as a **long-horizon stability footnote** rather than a single-run gate. Key interpretation points:

- **`drift_status`, `frequent_violations`, `blocking_rules`, and `total_runs` matter primarily over many runs, not a single First Light** — These fields aggregate patterns across multiple PRNG governance evaluations. A single First Light run may show `status: "OK"` even if the long-horizon summary shows `drift_status: "DRIFTING"` or `frequent_violations: {"R1": 5}`. This is expected: the summary reflects regime stability over time, not point-in-time health.

- **For early P5, this is context about PRNG regime stability, not a go/no-go signal** — The PRNG First-Light summary provides narrative context about how the PRNG governance regime has behaved across historical runs. It helps reviewers understand:
  - Whether PRNG policy violations are isolated incidents or recurring patterns
  - Which rules (R1, R2, R3, etc.) tend to trigger frequently
  - Whether the PRNG subsystem shows signs of drift or volatility over time

- **This summary is intended to be low-weight in any future fusion logic** — The PRNG First-Light summary is more like a stability footnote than a primary gate. It should not drive go/no-go decisions in isolation. Other evidence dimensions (budget, performance, metrics, stability) carry primary weight.

**Context for First Light Runs:**

In a First Light evidence pack, the PRNG summary reflects PRNG governance drift analysis across historical runs (not just the current First Light run). It aggregates:
- Policy rule violations across multiple runs (R1: no drifted evidence, R2: incompatible block, R3: hardcoded seeds, R4: namespace collisions, R5: missing attestation)
- Frequency analysis: which rules appear as violations in ≥3 runs (frequent violations)
- Blocking rule extraction: which rules caused BLOCK status in any run
- Drift classification: STABLE (0 frequent violations), DRIFTING (1-2), or VOLATILE (≥3)

**Advisory Nature:**

This summary is **not a gate**. It provides context for:
- Understanding PRNG regime stability trends
- Identifying recurring policy violation patterns
- Reading PRNG governance as a long-horizon narrative
- Cross-checking against other evidence dimensions (budget, performance, metrics)

**Example Scenario:**

A First Light run may show individual PRNG governance checks passing (`status: "OK"`), but the long-horizon summary shows `drift_status: "DRIFTING"` with `frequent_violations: {"R4": 4}`. This indicates:

1. **Current run is OK** — No PRNG policy violations in this specific First Light run
2. **Historical pattern shows drift** — Rule R4 (namespace collisions) has appeared as a violation in 4 previous runs
3. **Regime stability context** — The PRNG subsystem shows some drift, but not yet volatile
4. **Advisory footnote** — This is context for reviewers, not a blocking condition

The `frequent_violations` and `blocking_rules` fields provide the narrative context: which rules tend to trigger, and whether any blocking conditions were detected historically.

### 13.6 Example

```json
{
  "schema_version": "1.0.0",
  "drift_status": "DRIFTING",
  "frequent_violations": {
    "R4": 4
  },
  "status": "OK",
  "blocking_rules": [],
  "total_runs": 20
}
```

**Interpretation:** The PRNG subsystem shows DRIFTING status with R4 (namespace collisions) appearing as a violation in 4 out of 20 runs. No blocking rules were detected. Current status is OK. This is a stability footnote indicating some drift in namespace collision patterns, but not yet volatile enough to raise concerns.

### 13.7 PRNG Drift Ledger (Long-Horizon Signal)

The **PRNG Drift Ledger** provides a long-horizon aggregation of PRNG governance tiles across multiple First Light runs (mock vs real). This ledger is **advisory only** and does not gate any decisions in Phase X or early P5.

**Evidence Binding Location:**

The PRNG drift ledger attaches to the main evidence structure at:

```
evidence["governance"]["prng_drift_ledger"]
```

**Ledger Structure:**

The ledger contains:
- `schema_version`: "1.0.0"
- `total_runs`: Total number of PRNG governance tiles/runs analyzed
- `volatile_runs`: Number of tiles with `drift_status="VOLATILE"` (≥3 frequent violations)
- `drifting_runs`: Number of tiles with `drift_status="DRIFTING"` (1-2 frequent violations)
- `stable_runs`: Number of tiles with `drift_status="STABLE"` (no frequent violations)
- `frequent_rules`: Dictionary mapping rule_id to count of appearances across all tiles (rules that appeared as blocking_rules in one or more tiles)

**Interpretation for External Reviewers:**

The ledger is designed to be compared across multiple First Light runs (mock vs real). A single run's PRNG tile is not sufficient to judge anything about P5 behavior. The ledger aggregates drift patterns over time, providing long-horizon context for deterministic randomness governance.

- **`stable_runs`** — Number of runs where PRNG governance showed no frequent violations. A high ratio of `stable_runs / total_runs` indicates consistent deterministic randomness behavior.
- **`drifting_runs`** — Number of runs where 1-2 rules appeared frequently (≥3 times). This suggests intermittent governance issues that should be monitored but may not indicate systemic problems.
- **`volatile_runs`** — Number of runs where ≥3 rules appeared frequently. This indicates more significant governance drift and warrants closer review of the `frequent_rules` dictionary to identify which rules are consistently problematic.
- **`frequent_rules`** — Dictionary showing which rule IDs (e.g., "R1", "R2", "R3") appeared as blocking rules across multiple tiles. Higher counts indicate rules that are repeatedly violated across runs.

**For early P5, this is context only, not a gate.** The ledger provides long-horizon visibility into PRNG governance trends, comparison baseline between P3 (synthetic) and P4 (real) runs, identification of frequently violated rules that may need attention, and historical context for deterministic randomness compliance. The ledger does not block or gate any decisions; it is purely observational and intended to inform reviewers about PRNG governance health over time.

### 13.8 PRNG Drift Delta (Calibration Comparison)

The **PRNG Drift Delta** report compares PRNG drift ledgers across multiple phases (P3 mock, P4 mock, P5 real) to identify drift patterns and calibration behavior during warm-start experiments.

**Evidence Binding Location:**

The PRNG drift delta report attaches to the main evidence structure at:

```
evidence["governance"]["prng_drift_delta"]
```

**Delta Report Schema:**

```json
{
  "schema_version": "1.0.0",
  "phase_ledgers": {
    "p3_mock": {...},
    "p4_mock": {...},
    "p5_real": {...}
  },
  "deltas": {
    "p3_to_p4": {
      "total_runs_delta": 5,
      "volatile_runs_delta": 1,
      "drifting_runs_delta": 1,
      "stable_runs_delta": 3,
      "frequent_rules_delta": {
        "R1": 2,
        "R3": 1
      }
    },
    "p4_to_p5": {...},
    "p3_to_p5": {...}
  },
  "drift_status_transitions": [
    {
      "from_phase": "p3_mock",
      "to_phase": "p4_mock",
      "from_status": "STABLE",
      "to_status": "DRIFTING",
      "status_changed": true
    }
  ],
  "summary": {
    "phases_analyzed": ["p3_mock", "p4_mock", "p5_real"],
    "total_transitions": 2,
    "status_changes": 1,
    "current_status": "DRIFTING"
  }
}
```

**Field Descriptions:**

| Field | Type | Description |
|-------|------|-------------|
| `schema_version` | string | Schema version identifier (currently "1.0.0") |
| `phase_ledgers` | object | Individual PRNG drift ledgers for each phase (p3_mock, p4_mock, p5_real) |
| `deltas` | object | Delta comparisons between phases (p3_to_p4, p4_to_p5, p3_to_p5) |
| `deltas.*.total_runs_delta` | integer | Difference in total_runs between phases |
| `deltas.*.volatile_runs_delta` | integer | Difference in volatile_runs between phases |
| `deltas.*.drifting_runs_delta` | integer | Difference in drifting_runs between phases |
| `deltas.*.stable_runs_delta` | integer | Difference in stable_runs between phases |
| `deltas.*.frequent_rules_delta` | object | Dictionary mapping rule_id to count difference (sorted deterministically) |
| `drift_status_transitions` | array | List of drift status changes between consecutive phases |
| `summary` | object | Summary statistics about the comparison |

**Interpretation for External Reviewers:**

The PRNG drift delta report enables comparison of PRNG governance behavior across calibration phases:

- **P3→P4 Delta**: Shows how PRNG governance changes when moving from synthetic (P3) to shadow mock (P4) runs. Expected: minimal change if mock telemetry accurately reflects synthetic conditions.
- **P4→P5 Delta**: Shows how PRNG governance changes when moving from shadow mock (P4) to real telemetry (P5). Expected: may show increased drift if real telemetry introduces new PRNG usage patterns.
- **P3→P5 Delta**: Direct comparison between synthetic and real runs, bypassing the shadow mock phase.

**Expected PRNG Behavior During Warm-Start Calibration:**

During CAL-EXP-1 and CAL-EXP-2 warm-start calibration experiments:

1. **P3 Mock Phase**: PRNG governance should be STABLE or DRIFTING with minimal violations. Synthetic telemetry provides controlled conditions for baseline establishment.

2. **P4 Mock Phase**: PRNG governance should remain similar to P3, as mock telemetry should preserve deterministic behavior. Small increases in drift are acceptable if they reflect realistic mock conditions.

3. **P5 Real Phase**: PRNG governance may show increased drift compared to P3/P4, as real telemetry may introduce:
   - New PRNG usage patterns not present in synthetic/mock runs
   - Environmental factors affecting seed derivation
   - Runtime conditions that trigger different code paths

**Advisory Nature:**

The drift delta report is **not a gate**. It provides:
- Calibration context for understanding PRNG behavior transitions
- Identification of rules that become problematic in real telemetry
- Baseline comparison for warm-start calibration validation
- Long-horizon context for deterministic randomness governance

For early P5, this is context only. The report helps reviewers understand how PRNG governance evolves across calibration phases but does not block or gate any decisions.

### 13.8.1 PRNG Window Audit Table (Co-Registered Reconciliation)

The **PRNG Window Audit Table** provides a co-registered per-window view aligning calibration metrics (delta_p, divergence_rate) with PRNG drift metrics (drift_status, volatile_runs, blocking_rules) for reconciliation analysis. The table is bounded to a maximum of 10 windows to keep the audit artifact manageable for review. When a calibration experiment has more than 10 windows, the table uses a "first_last_even_spacing" selection strategy: it always includes the first window (baseline), the last window (final state), and evenly spaced middle windows to provide representative coverage across the experiment timeline. The `selected_window_indices` field in the table metadata explicitly lists which windows were included, allowing reviewers to understand which windows are present and which are omitted. Missing windows should be interpreted as excluded for brevity, not as missing data; the full window sequence remains available in the calibration report's `windows` array.

### 13.9 PRNG Regime Timeseries (Windowed Progression)

#### 13.9.1 Overview

The **PRNG Regime Timeseries** provides a windowed view of PRNG drift status progression across calibration experiment windows. This timeseries is **purely observational** and provides context for understanding how PRNG governance drift evolves over the course of a calibration experiment.

**Status:** ACTIVE — Included in CAL-EXP reports for Phase X.

**Purpose:** External reviewers can use this timeseries to understand how PRNG drift patterns change across time windows during calibration experiments, identifying trends and regime shifts.

#### 13.9.2 CAL-EXP Report Attachment Location

The PRNG regime timeseries attaches to calibration experiment reports at:

**Canonical Location (Preferred):**
```
report["governance"]["prng_regime_timeseries"]
```

**Legacy Location (Backward Compatibility):**
```
report["prng_regime_timeseries"]
```

**Note:** The canonical location is preferred. The legacy location is preserved for backward compatibility but should not be used for new reports.

#### 13.9.3 Timeseries Schema

```json
{
  "schema_version": "1.0.0",
  "window_size": 20,
  "windows": [
    {
      "window_index": 0,
      "drift_status": "STABLE",
      "frequent_rules_top5": ["R1", "R2"],
      "volatile_count": 0,
      "drifting_count": 0,
      "stable_count": 5
    },
    {
      "window_index": 1,
      "drift_status": "DRIFTING",
      "frequent_rules_top5": ["R1"],
      "volatile_count": 0,
      "drifting_count": 3,
      "stable_count": 2
    }
  ],
  "first_window_status": "STABLE",
  "last_window_status": "DRIFTING",
  "status_changed": true,
  "volatile_window_count": 0
}
```

#### 13.9.4 Field Descriptions

| Field | Type | Description |
|-------|------|-------------|
| `schema_version` | string | Schema version identifier (currently "1.0.0") |
| `window_size` | integer | Number of cycles per window |
| `windows` | array | List of window dicts, each containing window-level drift metrics |
| `windows[].window_index` | integer | 0-based window index |
| `windows[].drift_status` | string | Most common drift_status in this window ("STABLE" \| "DRIFTING" \| "VOLATILE") |
| `windows[].frequent_rules_top5` | array | Top 5 most frequent rule IDs in this window (sorted by count descending, then by ID) |
| `windows[].volatile_count` | integer | Number of tiles with drift_status="VOLATILE" in this window |
| `windows[].drifting_count` | integer | Number of tiles with drift_status="DRIFTING" in this window |
| `windows[].stable_count` | integer | Number of tiles with drift_status="STABLE" in this window |
| `first_window_status` | string | Drift status of the first window (summary field) |
| `last_window_status` | string | Drift status of the last window (summary field) |
| `status_changed` | boolean | Whether drift status changed between first and last window (summary field) |
| `volatile_window_count` | integer | Number of windows with drift_status="VOLATILE" (summary field) |
| `extraction_source` | string | Provenance field: `"GOVERNANCE_PATH"` \| `"LEGACY_PATH"` \| `"MISSING"` (PROVENANCE LAW v1) |
| `schema_version` | string | Schema version echoed from timeseries object (defaults to `"1.0.0"`) |
| `source_paths_checked` | array | Integrity invariant: list of paths checked during extraction (deterministic ordering) |
| `mirrored_from` | string | Provenance field (manifest only): `"GOVERNANCE_PATH"` \| `"LEGACY_PATH"` (only present when legacy path used) |

#### 13.9.5 Source of Truth

**Canonical Location:**
- **Preferred**: `report["governance"]["prng_regime_timeseries"]` (canonical location in CAL-EXP reports)
- **Legacy Fallback**: `report["prng_regime_timeseries"]` (backward compatibility)
- **Manifest Mirroring**: Evidence pack builder (`scripts/build_first_light_evidence_pack.py`) automatically mirrors the timeseries from CAL-EXP reports into `manifest["governance"]["prng_regime_timeseries"]` for status extraction. If only the legacy path exists, it is mirrored and marked with `mirrored_from: "LEGACY_PATH"`.

**Status Extraction Provenance:**
- The status generator (`scripts/generate_first_light_status.py`) extracts the timeseries and includes provenance fields:
  - `extraction_source`: `"GOVERNANCE_PATH"` | `"LEGACY_PATH"` | `"MISSING"` (indicates which path was used)
  - `schema_version`: Echoed from the timeseries object when present
- These fields appear in `signals["prng_regime"]["timeseries"]` to provide traceability about where the data came from.

**Traceability:**
- Reviewers can check `extraction_source` in the status JSON to understand whether the canonical or legacy path was used.
- If `extraction_source` is `"LEGACY_PATH"`, the data was extracted from the legacy location (backward compatibility).
- If `extraction_source` is `"GOVERNANCE_PATH"`, the data was extracted from the canonical governance location (preferred).

#### 13.9.6 Interpretation Guide

**For External Reviewers:**

The PRNG regime timeseries should be read as **context only for early P5**, not a go/no-go signal. Key interpretation points:

- **`first_window_status` vs `last_window_status`** — These indicate the drift status at the beginning and end of the calibration experiment. If they differ, it suggests the PRNG regime evolved during the experiment.

- **`status_changed`** — This boolean indicates whether the drift status changed between the first and last window. `true` means the regime shifted (e.g., from STABLE to DRIFTING), while `false` means it remained consistent. This is **interpretation only**—it provides context about regime stability trends, not a blocking condition.

- **`volatile_window_count`** — This counts how many windows had VOLATILE drift status. A high count suggests more volatility across the experiment, while 0 indicates no volatile windows.

- **`windows[]`** — The per-window progression shows how drift patterns evolved. Reviewers can plot `drift_status` vs `window_index` to visualize regime shifts.

**Context for Calibration Experiments:**

In a calibration experiment (e.g., CAL-EXP-2), the PRNG regime timeseries aggregates PRNG governance tiles across time windows. It provides:

- Windowed drift status progression (how drift patterns change over time)
- Identification of volatile windows (which windows had VOLATILE status)
- Summary metrics for quick assessment (first/last status, status change, volatile count)

**Advisory Nature:**

This timeseries is **not a gate**. It provides context for:
- Understanding PRNG drift evolution during calibration experiments
- Identifying regime shifts (status changes between windows)
- Reading PRNG drift as a temporal progression (how patterns change over time)
- Cross-checking against other evidence dimensions (budget, performance, metrics)

**Example Scenario:**

A CAL-EXP-2 timeseries shows:
- `first_window_status: "STABLE"`, `last_window_status: "DRIFTING"`
- `status_changed: true`
- `volatile_window_count: 2` (windows 3 and 4 were VOLATILE)

This indicates:
1. **Regime started stable** — First window showed STABLE drift status
2. **Regime shifted to drifting** — Last window showed DRIFTING status
3. **Status changed** — The regime evolved during the experiment
4. **Some volatile windows** — Two windows had VOLATILE status

This is context for reviewers: the PRNG regime became less stable over the course of the experiment, with some volatile periods. This is observational context, not a blocking condition.

#### 13.9.6 Example

```json
{
  "schema_version": "1.0.0",
  "window_size": 20,
  "windows": [
    {
      "window_index": 0,
      "drift_status": "STABLE",
      "frequent_rules_top5": [],
      "volatile_count": 0,
      "drifting_count": 0,
      "stable_count": 5
    },
    {
      "window_index": 1,
      "drift_status": "DRIFTING",
      "frequent_rules_top5": ["R1"],
      "volatile_count": 0,
      "drifting_count": 3,
      "stable_count": 2
    },
    {
      "window_index": 2,
      "drift_status": "VOLATILE",
      "frequent_rules_top5": ["R1", "R2"],
      "volatile_count": 2,
      "drifting_count": 1,
      "stable_count": 2
    }
  ],
  "first_window_status": "STABLE",
  "last_window_status": "VOLATILE",
  "status_changed": true,
  "volatile_window_count": 1
}
```

**Interpretation:** The PRNG regime started STABLE, progressed to DRIFTING, and ended VOLATILE. Status changed during the experiment. One window (window 2) was VOLATILE. This suggests the PRNG regime became less stable over time, but this is context only, not a gate.

---

### 13.10 Scenario Drift Cluster View (Calibration Analysis)

The **Scenario Drift Cluster View** provides a cross-experiment analysis of drift patterns across calibration experiments (CAL-EXP-1, CAL-EXP-2, CAL-EXP-3). It aggregates scenario-level drift maps from multiple experiments to identify persistent drift clusters.

**Purpose:** Enable target selection and analysis by identifying which slices/scenarios consistently show poly-cause drift across calibration experiments. This view helps reviewers focus on persistent drift patterns rather than transient issues.

**Location in Evidence Pack:**

The scenario drift cluster view attaches to the main evidence structure at:

```
evidence["governance"]["scenario_drift_cluster_view"]
```

**Schema:**

```json
{
  "schema_version": "1.0.0",
  "slice_frequency": {
    "slice_a": 3,
    "slice_b": 2,
    "slice_c": 1
  },
  "high_risk_slices": ["slice_a", "slice_b", "slice_c", "slice_d", "slice_e"],
  "experiments_analyzed": 3,
  "persistence_buckets": {
    "appears_in_1": ["slice_c"],
    "appears_in_2": ["slice_b"],
    "appears_in_3": ["slice_a"]
  },
  "persistence_score": 0.833333
}
```

**Field Descriptions:**

| Field | Type | Description |
|-------|------|-------------|
| `schema_version` | string | Schema version for forward compatibility |
| `slice_frequency` | object | Dictionary mapping slice names to count of experiments where they appeared in `slices_with_poly_cause` |
| `high_risk_slices` | array | Top N slices (default: 5) sorted by frequency, then alphabetically |
| `experiments_analyzed` | integer | Number of scenario drift maps aggregated |
| `persistence_buckets` | object | Slices grouped by appearance count: `appears_in_1`, `appears_in_2`, `appears_in_3` (all sorted) |
| `persistence_score` | float | Mean of (experiment_count / experiments_analyzed) across all slices, rounded to 6 decimal places |

**Status Signal Integration:**

The scenario drift cluster view is extracted into status signals at `signals["scenario_drift_cluster"]` with the following structure:

```json
{
  "schema_version": "1.0.0",
  "mode": "SHADOW",
  "experiments_analyzed": 3,
  "high_risk_slices": ["slice_a", "slice_b"],
  "persistence_score": 0.833333,
  "drivers": ["DRIVER_PERSISTENCE_SCORE_HIGH", "DRIVER_HIGH_RISK_SLICES_PRESENT"],
  "extraction_source": "MANIFEST"
}
```

**Driver Codes:**

The `drivers` array contains reason-code drivers that explain why a warning may be generated. Only these two driver codes are valid:

- **`DRIVER_PERSISTENCE_SCORE_HIGH`**: Emitted when `persistence_score >= 0.5`, indicating slices appear consistently across experiments.
- **`DRIVER_HIGH_RISK_SLICES_PRESENT`**: Emitted when `len(high_risk_slices) > 0`, indicating slices with poly-cause drift are present.

Driver codes are deterministically ordered: persistence first, slices second.

**Warning Format:**

When drivers are present, a single advisory warning is generated with the following format:

```
Scenario drift cluster: persistence_score=<value>; slices: <slice1>, <slice2>, <slice3> (+N more); drivers: <driver1>, <driver2> (experiments_analyzed=<count>)
```

The warning includes:
- `persistence_score` value (rounded to 3 decimal places)
- Up to 3 slice names (with "+N more" if additional slices exist)
- Driver codes for auditability
- `experiments_analyzed` count

**Persistence Score Semantics:**

The `persistence_score` field is a descriptive heuristic computed as the mean of `(experiment_count / experiments_analyzed)` across all slices that appear in the cluster view. This score provides a normalized measure of how consistently slices appear across calibration experiments, where a score of 1.0 indicates all slices appeared in all experiments, and lower scores indicate more sporadic appearance patterns. The persistence score is **not a gate** and does not influence any governance decisions; it is purely descriptive and intended to help reviewers identify persistent drift patterns that may warrant deeper investigation. The score is rounded to 6 decimal places for consistency and determinism.

**Extraction Source Provenance:**

The `extraction_source` field tracks where the cluster view was extracted from:
- **`MANIFEST`**: Extracted from `manifest["governance"]["scenario_drift_cluster_view"]` (canonical location)
- **`EVIDENCE_JSON`**: Extracted from `evidence.json["governance"]["scenario_drift_cluster_view"]` (fallback)
- **`MISSING`**: Cluster view not found in either location

**Usage Notes:**

1. **Target Selection** — Use `high_risk_slices` to identify which scenarios warrant deeper investigation during calibration review.
2. **Pattern Analysis** — Slices appearing in multiple experiments indicate systemic drift issues rather than transient problems.
3. **Not a Gate** — The cluster view is **purely observational** and does not gate any decisions. It is for analysis and target selection only.

**Example Scenario:**

A calibration suite runs CAL-EXP-1, CAL-EXP-2, and CAL-EXP-3. The cluster view shows:

```json
{
  "schema_version": "1.0.0",
  "slice_frequency": {
    "slice_arithmetic_simple": 3,
    "slice_arithmetic_complex": 2,
    "slice_algebra_basic": 1
  },
  "high_risk_slices": ["slice_arithmetic_simple", "slice_arithmetic_complex", "slice_algebra_basic"],
  "experiments_analyzed": 3,
  "persistence_buckets": {
    "appears_in_1": ["slice_algebra_basic"],
    "appears_in_2": ["slice_arithmetic_complex"],
    "appears_in_3": ["slice_arithmetic_simple"]
  },
  "persistence_score": 0.666667
}
```

**Interpretation:** `slice_arithmetic_simple` appears with poly-cause drift in all 3 experiments, indicating a persistent drift pattern. `slice_arithmetic_complex` appears in 2 experiments, suggesting intermittent issues. This view enables reviewers to prioritize investigation of `slice_arithmetic_simple` during calibration analysis.

---

## 14. Epistemic Panel (Calibration View)

### 14.1 Overview

The **Epistemic Panel** provides a calibration-ready cross-experiment view of epistemic alignment consistency across multiple calibration experiments (CAL-EXP-1, CAL-EXP-2, CAL-EXP-3). This panel is **advisory only** and does not gate any decisions in Phase X or early P5.

**Purpose:** External reviewers can use this panel to understand whether the epistemic alignment model consistently tracks with evidence quality across calibration experiments, identifying cases where the epistemic model and observed behavior disagree.

**Evidence Binding Location:**

The epistemic panel attaches to the main evidence structure at:

```
evidence["governance"]["epistemic_panel"]
```

### 14.2 Panel Schema

**Schema Version:** `1.0.0`

**Fields:**

| Field | Type | Description |
|-------|------|-------------|
| `schema_version` | string | Schema version identifier ("1.0.0") |
| `num_experiments` | integer | Total number of calibration experiments analyzed |
| `num_consistent` | integer | Number of experiments with CONSISTENT status (epistemic and evidence quality aligned) |
| `num_inconsistent` | integer | Number of experiments with INCONSISTENT status (epistemic and evidence quality disagree) |
| `num_unknown` | integer | Number of experiments with UNKNOWN status (evidence quality data unavailable) |
| `experiments_inconsistent` | array[object] | List of inconsistent experiments, each with `cal_id` and `reason` (sorted by cal_id) |

### 14.3 Field Interpretations

#### `num_experiments`
- Total count of calibration experiments included in the panel
- Typically 3 for CAL-EXP-1, CAL-EXP-2, CAL-EXP-3

#### `num_consistent`
- Count of experiments where epistemic alignment trajectory is consistent with evidence quality trajectory
- **Example:** Epistemic alignment degraded and evidence quality also degraded → CONSISTENT
- **Example:** Epistemic alignment improved and evidence quality is IMPROVING → CONSISTENT

#### `num_inconsistent`
- Count of experiments where epistemic alignment trajectory disagrees with evidence quality trajectory
- **Example:** Epistemic alignment degraded from HIGH→LOW while evidence quality trajectory is IMPROVING → INCONSISTENT
- **Example:** Epistemic alignment degraded from HIGH→MEDIUM while evidence quality trajectory is STABLE → INCONSISTENT

#### `num_unknown`
- Count of experiments where evidence quality data was not available for consistency checking
- Cannot determine consistency without evidence quality trajectory data

#### `experiments_inconsistent`
- List of experiments with INCONSISTENT status
- Each entry contains:
  - `cal_id`: Calibration experiment identifier (e.g., "CAL-EXP-1")
  - `reason`: Brief neutral explanation of the inconsistency
- Sorted by `cal_id` for deterministic ordering

### 14.4 Auditor Interpretation

**High inconsistency counts** indicate that the epistemic model frequently disagrees with observed evidence quality:

**Example Scenario A: High Inconsistency Rate**

```json
{
  "schema_version": "1.0.0",
  "num_experiments": 3,
  "num_consistent": 1,
  "num_inconsistent": 2,
  "num_unknown": 0,
  "experiments_inconsistent": [
    {
      "cal_id": "CAL-EXP-1",
      "reason": "Epistemic alignment degraded from HIGH (P3) to LOW (P4) while evidence quality trajectory is IMPROVING."
    },
    {
      "cal_id": "CAL-EXP-2",
      "reason": "Epistemic alignment degraded from HIGH (P3) to MEDIUM (P4) while evidence quality trajectory is STABLE."
    }
  ]
}
```

**Interpretation:**
- In 2 out of 3 calibration experiments, the epistemic alignment model indicated degradation while evidence quality was improving or stable
- This suggests the epistemic model may be:
  - Overly conservative (detecting issues not reflected in actual behavior)
  - Detecting early warning signals not yet visible in evidence quality metrics
  - Misaligned with the evidence quality assessment methodology
- **Auditor Action:** Investigate why the epistemic model and evidence quality assessments disagree. Review the specific experiments listed in `experiments_inconsistent` to understand the nature of the disagreement.

**Example Scenario B: Low Inconsistency Rate**

```json
{
  "schema_version": "1.0.0",
  "num_experiments": 3,
  "num_consistent": 3,
  "num_inconsistent": 0,
  "num_unknown": 0,
  "experiments_inconsistent": []
}
```

**Interpretation:**
- All calibration experiments show consistent alignment between epistemic signals and evidence quality
- The epistemic model appears to be tracking system behavior accurately
- **Auditor Action:** No action required. The epistemic model and evidence quality assessments are in agreement.

### 14.5 Calibration Cross-Check (Not a Gate)

**CRITICAL:** The epistemic panel is a **calibration cross-check**, not a direct gate:

- **Observational only:** The panel does not block or gate any calibration decisions
- **Advisory context:** Provides context for understanding epistemic model accuracy
- **External reviewer lens:** Designed for external safety auditors to assess epistemic alignment independently
- **No automated enforcement:** No systems depend on panel values for control decisions

**For external auditors:** The panel provides a cross-experiment consistency check. High inconsistency rates warrant investigation into why the epistemic model and evidence quality assessments disagree, but do not automatically invalidate either signal.

---

## 15. Topology Stress Summary (First Light)

### 14.1 Overview

The **Topology Stress Summary** provides a cross-phase topology pressure assessment combining P3 (synthetic) and P4 (real shadow) topology observations. This summary is **purely observational** and does not gate any decisions in Phase X.

**Purpose:** External reviewers can use this summary to understand topology stress patterns across synthetic and real execution phases, identifying where topology pressure differs between synthetic and real runs.

**Evidence Binding:**

The topology stress summary attaches to the evidence structure at:

```
evidence["governance"]["first_light_topology_stress"]
```

**Binding Fields:**

| Field | Type | Description |
|-------|------|-------------|
| `schema_version` | string | Schema version: "1.0.0" |
| `p3_pressure_band` | string | P3 topology pressure band: "LOW" \| "MEDIUM" \| "HIGH" |
| `p4_pressure_band` | string | P4 topology pressure band: "LOW" \| "MEDIUM" \| "HIGH" |
| `pressure_hotspots` | array | Merged, deduplicated list of pressure hotspots (max 5) |

### 14.2 Field Definitions

#### 14.2.1 `p3_pressure_band`

The `p3_pressure_band` field indicates the topology pressure level observed during the P3 (synthetic) First Light run. This band is derived from the `topology_pressure_summary` field in the P3 stability report.

**Interpretation:**
- **LOW**: Synthetic topology shows minimal structural stress; depth trends, branching volatility, and risk envelope are all within nominal ranges.
- **MEDIUM**: Synthetic topology shows moderate structural stress; one or more pressure components (depth, branching, risk) are elevated but not critical.
- **HIGH**: Synthetic topology shows significant structural stress; multiple pressure components are elevated, indicating potential topology instability.

#### 14.2.2 `p4_pressure_band`

The `p4_pressure_band` field indicates the topology pressure level observed during the P4 (real shadow) First Light run. This band is derived from the `topology_pressure_calibration` field in the P4 calibration report.

**Interpretation:**
- **LOW**: Real topology shows minimal structural stress; real execution topology metrics align with synthetic expectations.
- **MEDIUM**: Real topology shows moderate structural stress; real execution shows some deviation from synthetic topology patterns.
- **HIGH**: Real topology shows significant structural stress; real execution topology differs substantially from synthetic expectations.

#### 14.2.3 `pressure_hotspots`

The `pressure_hotspots` field contains a merged and deduplicated list of topology pressure observations from both P3 and P4 phases. The list is limited to a maximum of 5 hotspots for human legibility; this limit does not imply that only 5 hotspots matter—it is a presentation constraint to keep the summary concise for external reviewers.

**Hotspot Examples:**
- "Depth trend contributing to pressure"
- "Branching volatility contributing to pressure"
- "Risk envelope contributing to pressure"

**Interpretation:**
- Hotspots identify which topology components (depth, branching, risk) are contributing to elevated pressure.
- Multiple hotspots indicate multi-dimensional topology stress.
- Empty list indicates no specific component-level pressure signals.

### 14.3 Cross-Phase Interpretation Guidance

The topology stress summary enables reviewers to compare topology pressure between synthetic (P3) and real (P4) execution phases. The following patterns are diagnostically significant:

#### 14.3.1 P3=LOW + P4=LOW

**Interpretation:** Both synthetic and real topology show minimal stress.

**Reviewer Action:** This is the expected baseline. No topology concerns.

#### 14.3.2 P3=LOW + P4=MEDIUM

**Interpretation:** Real topology shows moderate stress not present in synthetic runs.

**Reviewer Action:** Investigate why real execution topology differs from synthetic. Possible causes:
- Real execution encounters topology patterns not captured in synthetic generation
- Real telemetry reveals structural stress not visible in synthetic state space
- Calibration drift between synthetic and real topology metrics

#### 14.3.3 P3=LOW + P4=HIGH

**Interpretation:** Real topology shows significant stress not present in synthetic runs.

**Reviewer Action:** **Investigate immediately.** This pattern indicates:
- Real execution topology stress is much higher than synthetic; investigate root cause
- Synthetic topology generation may not capture real-world structural complexity
- Real telemetry may reveal topology instability not visible in synthetic runs
- Consider whether synthetic topology models need recalibration

#### 14.3.4 P3=MEDIUM + P4=MEDIUM

**Interpretation:** Both phases show moderate topology stress.

**Reviewer Action:** This indicates consistent topology stress across phases. Review pressure hotspots to identify common stress sources.

#### 14.3.5 P3=HIGH + P4=LOW

**Interpretation:** Synthetic topology shows high stress, but real execution shows low stress.

**Reviewer Action:** Investigate why synthetic topology stress does not manifest in real execution. Possible causes:
- Synthetic stress tests are more aggressive than real execution patterns
- Real execution topology is more stable than synthetic generation suggests
- Synthetic topology models may be overly conservative

#### 14.3.6 P3=HIGH + P4=HIGH

**Interpretation:** Both phases show significant topology stress.

**Reviewer Action:** **High priority investigation.** This pattern indicates:
- Topology stress is consistent across synthetic and real execution
- Structural stress is inherent to the system topology, not phase-specific
- Review topology design and consider topology-aware policy adjustments

### 14.4 Example

```json
{
  "schema_version": "1.0.0",
  "p3_pressure_band": "MEDIUM",
  "p4_pressure_band": "HIGH",
  "pressure_hotspots": [
    "Depth trend contributing to pressure",
    "Risk envelope contributing to pressure"
  ]
}
```

**Interpretation:** P3 (synthetic) shows MEDIUM topology pressure, while P4 (real shadow) shows HIGH topology pressure. The pressure hotspots indicate that depth trends and risk envelope are contributing to pressure. This pattern (P3=MEDIUM, P4=HIGH) suggests that real execution topology stress is elevated compared to synthetic expectations, warranting investigation into why real topology differs from synthetic.

### 14.5 Topology Stress Across Calibration Experiments

The **Topology Stress Panel** provides an aggregate view of topology stress across multiple calibration experiments. This panel is designed as a data feed for heatmap visualization, enabling reviewers to compare topology stress patterns across different calibration runs.

**Purpose:** External reviewers can use this panel to visualize topology stress trends across calibration experiments, identifying which experiments show elevated topology pressure and whether stress patterns are consistent or variable.

**Evidence Binding:**

The topology stress panel attaches to the evidence structure at:

```
evidence["governance"]["topology_stress_panel"]
```

**Panel Structure:**

| Field | Type | Description |
|-------|------|-------------|
| `schema_version` | string | Schema version: "1.0.0" |
| `panel_type` | string | Panel type identifier: "topology_stress_heatmap" |
| `experiments` | array | List of per-experiment stress snapshots |
| `summary` | object | Aggregate statistics across experiments |

**Per-Experiment Snapshot:**

Each entry in `experiments` contains:

| Field | Type | Description |
|-------|------|-------------|
| `cal_id` | string | Calibration experiment identifier |
| `p3_pressure_band` | string | P3 topology pressure band for this experiment |
| `p4_pressure_band` | string | P4 topology pressure band for this experiment |
| `pressure_hotspots` | array | Pressure hotspots for this experiment (max 5) |

**Panel Summary:**

The `summary` field provides aggregate statistics:

| Field | Type | Description |
|-------|------|-------------|
| `total_experiments` | integer | Total number of experiments in panel |
| `p3_band_distribution` | object | Count of experiments per P3 pressure band |
| `p4_band_distribution` | object | Count of experiments per P4 pressure band |
| `common_hotspots` | array | Common hotspots across experiments (max 10) |

**Interpretation:**

- **Heatmap Visualization:** The panel is designed as a data feed for heatmap visualization tools. It does not perform plotting itself, but provides structured data that visualization tools can consume.
- **Not a Gate:** The panel is purely observational and does not gate any decisions. It is a visualization aid, not a governance signal.
- **Cross-Experiment Comparison:** Reviewers can use the panel to identify:
  - Which calibration experiments show consistent topology stress
  - Whether topology stress correlates with specific experiment parameters
  - Common hotspots across multiple experiments (indicating systemic topology stress)

**Example:**

```json
{
  "schema_version": "1.0.0",
  "panel_type": "topology_stress_heatmap",
  "experiments": [
    {
      "cal_id": "cal_exp1",
      "p3_pressure_band": "MEDIUM",
      "p4_pressure_band": "HIGH",
      "pressure_hotspots": ["Depth trend contributing"]
    },
    {
      "cal_id": "cal_exp2",
      "p3_pressure_band": "LOW",
      "p4_pressure_band": "MEDIUM",
      "pressure_hotspots": []
    }
  ],
  "summary": {
    "total_experiments": 2,
    "p3_band_distribution": {"LOW": 1, "MEDIUM": 1, "HIGH": 0},
    "p4_band_distribution": {"LOW": 0, "MEDIUM": 1, "HIGH": 1},
    "common_hotspots": ["Depth trend contributing"]
  }
}
```

**Interpretation:** The panel shows two calibration experiments. `cal_exp1` shows MEDIUM P3 and HIGH P4 pressure, while `cal_exp2` shows LOW P3 and MEDIUM P4 pressure. The summary indicates that P3 pressure is distributed across LOW and MEDIUM bands, while P4 pressure is distributed across MEDIUM and HIGH bands. The common hotspot "Depth trend contributing" appears in at least one experiment.

**Hotspot Ledger:**

The topology stress panel includes a `hotspot_ledger` field that tracks which slices (or hotspot identifiers) recur as hotspots across multiple calibration experiments. This ledger provides a cross-experiment view of recurring topology stress patterns, enabling reviewers to identify slices that consistently appear as topology pressure hotspots.

The hotspot ledger structure:

| Field | Type | Description |
|-------|------|-------------|
| `schema_version` | string | Schema version: "1.0.0" |
| `hotspot_counts` | object | Dictionary mapping slice/hotspot name to count of occurrences across experiments (keys sorted alphabetically for determinism) |
| `top_hotspots` | array | Top 10 hotspots by count, sorted by count (descending) with deterministic tie-breaking by name |
| `num_experiments` | integer | Total number of experiments analyzed |

**Interpretation:** The hotspot ledger answers the question "which slices recur as hotspots across experiments?" by counting how many times each hotspot appears across all calibration experiments. A slice that appears in multiple experiments indicates recurring topology stress, while a slice that appears in only one experiment may indicate experiment-specific stress. The `top_hotspots` list provides a quick view of the most frequently recurring hotspots, while `hotspot_counts` provides the complete distribution. The ledger is purely observational and does not gate any decisions.

**Example:**

```json
{
  "hotspot_ledger": {
    "schema_version": "1.0.0",
    "hotspot_counts": {
      "Depth trend": 3,
      "Risk envelope": 2,
      "slice_uplift_tree": 2,
      "Branching volatility": 1
    },
    "top_hotspots": [
      "Depth trend",
      "Risk envelope",
      "slice_uplift_tree",
      "Branching volatility"
    ],
    "num_experiments": 3
  }
}
```

**Interpretation:** The hotspot ledger shows that "Depth trend" appears as a hotspot in all 3 experiments, indicating it is a recurring topology stress pattern. "Risk envelope" and "slice_uplift_tree" each appear in 2 experiments, while "Branching volatility" appears in only 1 experiment. This suggests that depth trends are a systemic topology stress concern across calibration experiments, while branching volatility may be experiment-specific.

### 14.6 SHADOW MODE Contract

The topology stress summary and panel operate under SHADOW MODE:

| Invariant | Description |
|-----------|-------------|
| **TOPO-STRESS-INV-01** | Topology stress summary is purely observational |
| **TOPO-STRESS-INV-02** | No governance decisions depend on stress summary values |
| **TOPO-STRESS-INV-03** | Stress summary does not gate any execution or policy decisions |
| **TOPO-STRESS-INV-04** | All stress observations are logged only, never enforced |
| **TOPO-STRESS-INV-05** | Topology stress panel is a visualization data feed, not a gate |

---

## 14. Evidence Failure Shortlist (Phase X)

### 14.1 Overview

The **Evidence Failure Shortlist** provides a compact, curated index of the most concerning evidence quality episodes across calibration experiments (CAL-EXP-1, CAL-EXP-2, CAL-EXP-3). This shortlist is **purely observational** and does not gate any decisions in Phase X.

**Status:** ACTIVE — Included in Evidence Pack for Phase X.

**Purpose:** External reviewers can use this shortlist as a reviewer convenience index to quickly identify the worst evidence quality episodes across multiple calibration experiments, without requiring deep knowledge of the evidence quality subsystem.

### 14.2 Evidence Binding Location

The evidence failure shortlist attaches to the main evidence structure at:

```
evidence["governance"]["evidence_failure_shortlist"]
```

### 14.3 Shortlist Schema

```json
{
  "schema_version": "1.0.0",
  "items": [
    {
      "rank": 1,
      "cal_id": "cal_exp1",
      "episode_id": "cal_exp1_episode_1",
      "trajectory_class": "DEGRADING",
      "predicted_band": "LOW",
      "cycles_until_risk": 0,
      "flags": ["regression detected", "evidence quality degrading"]
    },
    ...
  ],
  "total_shelves": 3
}
```

### 14.4 Field Descriptions

| Field | Type | Description |
|-------|------|-------------|
| `schema_version` | string | Schema version identifier (currently "1.0.0") |
| `items` | array | List of top N most concerning episodes, sorted by severity (most concerning first) |
| `total_shelves` | integer | Total number of calibration experiment shelves processed |

**Item Fields:**

| Field | Type | Description |
|-------|------|-------------|
| `rank` | integer | Rank within the shortlist (1 = most concerning) |
| `cal_id` | string | Calibration experiment identifier (e.g., "cal_exp1", "CAL-EXP-1") |
| `episode_id` | string | Synthetic episode identifier (e.g., "cal_exp1_episode_1") |
| `trajectory_class` | string | Evidence quality trajectory: `DEGRADING`, `OSCILLATING`, `STABLE`, `IMPROVING` |
| `predicted_band` | string | Predicted quality band: `LOW`, `MEDIUM`, `HIGH` |
| `cycles_until_risk` | integer \| null | Estimated cycles until quality risk materializes (lower = more concerning) |
| `flags` | array | List of concerning signals/flags (sorted alphabetically) |

### 14.5 Severity Ordering

The shortlist orders episodes by severity heuristic (most concerning first):

1. **predicted_band**: `LOW` > `MEDIUM` > `HIGH` (most concerning first)
2. **cycles_until_risk**: Lower values are more concerning (0 is most concerning)
3. **trajectory_class**: `DEGRADING` > `OSCILLATING` > `STABLE` > `IMPROVING`

### 14.6 Interpretation Guide

**For External Reviewers:**

The evidence failure shortlist should be read as a **reviewer convenience index** rather than a gate or decision-making tool. Key interpretation points:

- **This is a curation aid, not a gate** — The shortlist helps reviewers quickly identify concerning episodes but does not block or gate any decisions
- **Items are ranked by severity** — The top-ranked items represent the most concerning evidence quality episodes across all calibration experiments
- **Shortlist is truncated to top N** — By default, only the top 10 most concerning episodes are included (configurable via `max_items`)
- **Flags provide context** — Each item's `flags` array contains neutral explanations of why the episode is concerning (e.g., "regression detected", "evidence quality degrading")

**Use Cases:**

- Quick identification of worst evidence quality episodes across CAL-EXP-1/2/3
- Prioritizing review efforts on most concerning calibration experiments
- Understanding evidence quality trends across multiple experiments
- Context for evidence quality governance assessment

**For early P5, this is context only, not a gate.** The shortlist provides visibility into evidence quality concerns across calibration experiments, helps prioritize review efforts, and provides context for evidence quality governance assessment. The shortlist does not block or gate any decisions; it is purely observational and intended to inform reviewers about evidence quality health across experiments.

### 14.7 Per-Experiment Shelf Files

Each calibration experiment emits a per-experiment failure shelf file:

```
calibration/evidence_failure_shelf_{cal_id}.json
```

These files contain the enriched failure shelf for each experiment (with `cal_id`, `episode_id`, `rank` metadata). The global shortlist aggregates these per-experiment shelves into a unified top N list.

---

## 15. Governance Final Check Integration (Phase Y)

### 15.1 Overview

The **Governance Final Check** artifacts from CLAUDE K (Last-Mile Governance Checker) will be integrated into the Evidence Pack upon exit from SHADOW MODE. This section specifies the target structure for Phase Y integration.

**Status:** DESIGN SPECIFICATION ONLY — No implementation until SHADOW MODE exit.

### 15.2 Target Pack Structure Extension

```
evidence_pack_{run_id}/
├── ...existing artifacts...
├── governance/
│   ├── final_check/
│   │   ├── summary.json              # Aggregate check statistics
│   │   ├── checks.jsonl              # All GovernanceFinalCheckResult records
│   │   ├── blocks.jsonl              # BLOCK decisions only (filtered view)
│   │   ├── waivers_applied.json      # Waiver usage summary
│   │   └── overrides_applied.json    # Override usage summary
│   ├── gate_stats/
│   │   ├── g0_catastrophic.json      # Gate-specific statistics
│   │   ├── g1_hard.json
│   │   ├── g2_invariant.json
│   │   ├── g3_safe_region.json
│   │   ├── g4_soft.json
│   │   └── g5_advisory.json
│   └── audit_chain/
│       ├── chain_verification.json   # Chain integrity verification result
│       └── audit_log.jsonl           # Raw audit records (hash-linked)
```

### 15.3 Artifact Requirements (Phase Y)

| Artifact | Required | Format | Schema Reference |
|----------|----------|--------|------------------|
| `governance/final_check/summary.json` | YES | JSON | `governance_final_check_summary.schema.json` |
| `governance/final_check/checks.jsonl` | YES | JSONL | `governance_final_check.schema.json` |
| `governance/final_check/blocks.jsonl` | YES | JSONL | `governance_final_check.schema.json` |
| `governance/gate_stats/*.json` | YES | JSON | `governance_gate_stats.schema.json` |
| `governance/audit_chain/chain_verification.json` | YES | JSON | `audit_chain_verification.schema.json` |

### 15.4 Evidence Binding Contract

The governance evidence attaches to the main evidence structure at:

```
evidence["governance"]["final_check"]
```

**Binding Fields:**

| Field | Type | Description |
|-------|------|-------------|
| `summary` | object | Aggregate statistics (allow/block rates, gate pass rates) |
| `checks` | array | Full `GovernanceFinalCheckResult` records |
| `blocks` | array | Filtered view: BLOCK decisions only |
| `gate_stats` | object | Per-gate statistics keyed by gate ID |
| `waivers_applied` | object | Waiver usage counts by waiver ID |
| `overrides_applied` | object | Override usage counts by override ID |
| `audit_chain` | object | Chain verification status and height |

### 15.5 NDAA Mapping Extension

| NDAA Requirement | Governance Evidence | Compliance Mechanism |
|------------------|---------------------|----------------------|
| **Risk-informed strategy** | `gate_stats/*.json`, `summary.json` | Quantified gate pass/fail rates inform risk posture |
| **Technical controls** | `blocks.jsonl`, gate evaluations | 6-gate hierarchy enforces safety invariants |
| **Human override capability** | `waivers_applied.json`, `overrides_applied.json` | Explicit waiver/override audit trail |
| **Auditability** | `audit_chain/`, hash-linked records | Immutable append-only governance log |

### 15.6 Phase Y Activation Criteria

Governance artifacts will be included in Evidence Pack when:

1. **SHADOW MODE Validation Complete** — Divergence analysis shows twin-real alignment within tolerance
2. **Gate Calibration Verified** — No systematic false positives/negatives in gate evaluation
3. **Audit Chain Integrity** — Hash chain verification passes for all recorded checks
4. **Waiver/Override Policy Approved** — Human oversight framework ratified

**Note:** Until Phase Y activation, governance artifacts are collected but NOT included in the canonical Evidence Pack bundle.

---

## 16. Budget Storyline in P5 Calibration

### 16.1 Overview

The **Budget Storyline Summary** serves as an "Energy Drift Monitor" for P5 calibration experiments (CAL-EXP-1, CAL-EXP-2, CAL-EXP-3). It tracks budget invariant health across calibration windows, providing temporal coherence evidence that complements divergence metrics.

**Status:** ACTIVE — Included in CAL-EXP reports for Phase X.

**Purpose:** Enable correlation analysis between budget invariant stability and twin-vs-real divergence patterns. The budget storyline provides early-warning signals about budget governance drift that may correlate with calibration instability.

### 16.2 CAL-EXP Report Integration

Budget storyline summaries are embedded in CAL-EXP reports at:

```
cal_exp{1,2,3}_report.json["budget_storyline_summary"]
```

**Required Fields:**
- `experiment_id`: "CAL-EXP-1", "CAL-EXP-2", or "CAL-EXP-3"
- `run_id`: Unique run identifier
- `combined_status`: "OK" | "WARN" | "BLOCK"
- `stability_index`: float [0.0, 1.0]
- `episodes_count`: int
- `projection_class`: "STABLE" | "DRIFTING" | "VOLATILE"
- `key_structural_events`: List[str] (first 5 events)
- `window_start`, `window_end`: Optional[int] (for CAL-EXP-2/3 window bounds)

### 16.3 Window-Level Budget Confounding Annotations

Each 50-cycle window in CAL-EXP reports can be annotated with budget storyline data to support downstream calibration analysis. The `annotate_cal_exp_windows_with_budget_storyline()` helper attaches the following fields to each window:

| Field | Type | Description |
|-------|------|-------------|
| `budget_combined_status` | string | Budget invariant status: `OK` (healthy), `WARN` (monitoring), or `BLOCK` (attention required) |
| `budget_stability_index` | float | Budget stability metric [0.0, 1.0] from the budget storyline summary |
| `budget_projection_class` | string | BNH-Φ projected stability: `STABLE`, `DRIFTING`, `VOLATILE`, or `UNKNOWN` |
| `budget_confounded` | bool | Confounding flag (see logic below) |
| `budget_confound_reason` | string \| null | Explanation for confounding status (see reason codes below) |
| `confound_stability_threshold` | float | Stability index threshold used for confounding calculation (default: 0.95) |

**Confounding Flag Logic (Strict Mode, Default):**

The default `strict_confounding=True` mode uses an AND-rule to prevent over-flagging:

```
budget_confounded = (budget_combined_status in {"WARN", "BLOCK"}) AND 
                   (budget_stability_index < confound_stability_threshold)
```

This requires **BOTH conditions** to be true, ensuring that only windows with both degraded status AND low stability are flagged as confounded.

- **`False` (not confounded)**: Either `combined_status: "OK"` OR `stability_index >= threshold`. Divergence patterns in this window are unlikely to be explained by budget drift.
- **`True` (confounded)**: Both `combined_status in {"WARN", "BLOCK"}` AND `stability_index < threshold`. Downstream calibration analysis should consider budget drift as a potential confounding factor.

**Legacy Mode (strict_confounding=False):**

The legacy OR-rule is available for backward compatibility:

```
budget_confounded = (budget_combined_status != "OK") OR 
                   (budget_stability_index < confound_stability_threshold)
```

This flags windows if **EITHER** condition is true. Use `strict_confounding=False` only for compatibility with legacy analysis pipelines.

**Reason Codes:**

The `budget_confound_reason` field explains why a window is or is not confounded:

| Reason Code | Condition | Confounded? |
|-------------|-----------|-------------|
| `"STATUS_AND_LOW_STABILITY"` | Both WARN/BLOCK AND stability_index < threshold | Yes |
| `"STATUS_WARN"` | Status is WARN (strict mode: stability_index >= threshold) | No (strict) / Yes (legacy) |
| `"STATUS_BLOCK"` | Status is BLOCK (strict mode: stability_index >= threshold) | No (strict) / Yes (legacy) |
| `"LOW_STABILITY_INDEX"` | stability_index < threshold (status is OK) | No (strict) / Yes (legacy) |
| `null` | Status is OK AND stability_index >= threshold | No |

**Threshold Configuration:**

The `confound_stability_threshold` parameter (default: 0.95) determines the stability index cutoff. This threshold is recorded in each window's `confound_stability_threshold` field for transparency and reproducibility.

**Canonical Truth Table Generator:**

A machine-readable truth table can be generated deterministically using:

```bash
python scripts/emit_budget_confounding_truth_table.py
python scripts/emit_budget_confounding_truth_table.py --threshold 0.90
python scripts/emit_budget_confounding_truth_table.py --json
```

The truth table enumerates all 6 canonical combinations and provides a reference for both strict and legacy modes. This table is byte-identical across runs, ensuring no semantic drift.

**Deterministic Ordering:**

Windows are sorted by `window_idx` to ensure deterministic ordering. The `key_structural_events` field in the budget storyline summary is truncated to the first 5 events for consistency.

**Validation Warnings:**

The `validate_budget_confounding_defaults()` function returns structured warnings with stable codes:

| Code | Condition |
|------|-----------|
| `BUDGET-DEF-001` | `budget_confounded` present but `budget_confound_reason` missing |
| `BUDGET-DEF-002` | Budget fields present but `confound_stability_threshold` missing |

Warnings are deterministically ordered by `window_index` for stable output.

**Interpretation Guidance:**

**IMPORTANT**: The `budget_confounded` flag is a **label for interpretation guidance**, not a gate and not an exclusion criterion. It provides context for downstream calibration analysis but does not:
- Automatically exclude windows from analysis
- Trigger any gating logic
- Mandate any specific actions
- Override scientific judgment

Use confounded windows to:
- Flag potential confounding factors in divergence analysis
- Segment analysis by budget health (confounded vs. non-confounded windows)
- Correlate budget drift patterns with calibration instability
- Inform human review of calibration results

**For External Auditors:**

When reviewing CAL-EXP reports with annotated windows, auditors should:

1. **Identify confounded windows**: Look for windows with `budget_confounded: true` to identify calibration windows where budget drift may explain divergence patterns. Check `budget_confound_reason` to understand which condition(s) triggered the flag.

2. **Correlate budget status with divergence**: Compare `budget_combined_status` and `budget_stability_index` against `divergence_rate` to identify correlations between budget health and calibration stability. Use `budget_confound_reason` to segment analysis by reason type.

3. **Interpret projection class**: The `budget_projection_class` provides forward-looking context—windows with `VOLATILE` projection may indicate ongoing budget instability that could affect future calibration windows.

4. **Consider confounding in analysis**: When `budget_confounded: true`, divergence patterns should be interpreted with caution. High divergence in confounded windows may be partially explained by budget drift rather than calibration issues. However, remember that confounded windows are not excluded from analysis—they provide context for interpretation.

**Example Scenario (Strict Mode):**

A CAL-EXP-1 report shows windows 3-5 with elevated `divergence_rate` (0.08-0.12). These windows have:
- `budget_combined_status: "WARN"`
- `budget_stability_index: 0.92` (below 0.95 threshold)
- `budget_confounded: true`
- `budget_confound_reason: "STATUS_AND_LOW_STABILITY"`

An auditor should:
- Flag these windows as potentially confounded by budget drift (both WARN status AND low stability)
- Compare divergence in confounded windows (3-5) vs. non-confounded windows (1-2, 6+)
- If non-confounded windows show low divergence, budget drift is a likely contributing factor
- If all windows show similar divergence patterns, calibration issues are more likely
- Use `confound_stability_threshold` field to verify the threshold used (should be 0.95 by default)

**Example Scenario (Non-Confounded):**

A CAL-EXP-2 report shows windows 2-4 with:
- `budget_combined_status: "WARN"`
- `budget_stability_index: 0.97` (above 0.95 threshold)
- `budget_confounded: false` (strict mode: WARN alone is not confounded)
- `budget_confound_reason: "STATUS_WARN"`

An auditor should note that while status is WARN, stability is high, so the window is not flagged as confounded in strict mode. This prevents over-flagging while still recording the WARN status for review.

### 16.4 Cross-Plot Analysis

The `scripts/plot_budget_vs_divergence.py` tool generates cross-plot datasets comparing:
- **X-axis**: `stability_index` (budget invariant stability)
- **Y-axis**: `divergence_rate` (twin-vs-real divergence)

**Extended Output Fields:**

The cross-plot output includes per-window budget annotations:
- `stability_index`: Budget stability metric (for X-axis plotting)
- `budget_combined_status`: Budget status for correlation analysis
- `budget_projection_class`: Forward-looking stability projection
- `budget_confounded`: Confounding flag for filtering or color-coding
- `budget_confound_reason`: Reason code explaining confounding status

Windows are sorted deterministically by `(experiment_id, window_idx)` for consistent output ordering.

This enables visualization of correlations between budget health and calibration stability across experiment windows. Confounded windows can be visually distinguished (e.g., different markers or colors) to separate budget-driven divergence from calibration-driven divergence. The `budget_confound_reason` field enables filtering or grouping by specific reason codes.

### 16.5 P5 Acceptance Gate Integration (Phase Y Design Stub)

**Status:** DESIGN SPECIFICATION ONLY — Phase Y integration, no gating in Phase X/P5 POC.

**Future Integration (Phase Y):**

The budget storyline summary will be consumed by the **P5AcceptanceGate** (Gate 2/3) as an advisory modifier:

1. **Gate 2 (Hard Gate)** — Budget storyline `combined_status: "BLOCK"` with `stability_index < 0.5` may contribute to gate decision, but not as a hard block. The gate will consider budget drift as a contributing factor alongside other signals.

2. **Gate 3 (Invariant Gate)** — Budget storyline `key_structural_events` indicating persistent INV-BUD-1 violations may trigger additional scrutiny in invariant checks.

**Advisory Nature (Phase X/P5 POC):**

During Phase X and P5 POC, the budget storyline summary is **purely observational**:
- No gating decisions based on budget storyline
- No automatic blocks or warnings
- Data collected for analysis and correlation studies
- Human review recommended for `combined_status: "BLOCK"` cases

**Activation Criteria (Phase Y):**

Budget storyline integration into P5AcceptanceGate will activate when:
- Phase Y governance framework is deployed
- Cross-plot analysis demonstrates reliable correlation between budget drift and calibration instability
- P5AcceptanceGate design is finalized with budget storyline as advisory modifier
- Governance team approves budget storyline as contributing signal (not hard gate)

---

## 17. Curriculum Coherence Calibration Panel (P5)

### 17.1 Overview

The **Curriculum Coherence Calibration Panel** provides a cross-experiment alignment witness for P5 calibration experiments (CAL-EXP-1/2/3). This panel aggregates curriculum coherence snapshots across multiple calibration experiments into a single summary, enabling reviewers to assess curriculum alignment consistency across the calibration suite.

**Status:** ACTIVE — Included in Evidence Pack for Phase X P5 calibration.

**Purpose:** External reviewers can use this panel to understand curriculum coherence as a cross-experiment signal, identifying alignment patterns across CAL-EXP-1/2/3 without requiring deep knowledge of individual experiment details.

### 17.2 Evidence Binding Location

The curriculum coherence panel attaches to the main evidence structure at:

```
evidence["governance"]["curriculum_coherence_panel"]
```

**Canonical Location:**

The panel should be placed in `manifest.json` under `manifest["governance"]["curriculum_coherence_panel"]` for canonical truth. The status extraction logic in `generate_first_light_status.py` prefers manifest over `evidence.json` fallback:

- **Primary:** `manifest["governance"]["curriculum_coherence_panel"]` (canonical)
- **Fallback:** `evidence.json["governance"]["curriculum_coherence_panel"]` (legacy support)

When extracted into `first_light_status.json`, the signal includes provenance fields:

- **`extraction_source`** (enum-locked): `"MANIFEST"` | `"EVIDENCE_JSON"` | `"MISSING"`
  - `"MANIFEST"` — Panel found in manifest.json (canonical)
  - `"EVIDENCE_JSON"` — Panel found in evidence.json fallback (legacy)
  - `"MISSING"` — Panel not found (optional, not an error)
  - **Enum coercion:** Unknown values are coerced to `"MISSING"` with advisory warning `CURR-PROV-002`

- **`panel_schema_version`**: Schema version from panel (or `"UNKNOWN"` if absent)

- **`schema_version_present`** (boolean): Audit clarity field indicating whether `schema_version` was present in the panel

**Warning Codes:**
- `CURR-PROV-001`: Emitted exactly once when `extraction_source == "EVIDENCE_JSON"` to encourage canonical manifest use
- `CURR-PROV-002`: Emitted when extraction_source is unknown and coerced to `"MISSING"` for enum safety

### 17.3 Panel Schema

```json
{
  "schema_version": "1.0.0",
  "num_experiments": 3,
  "num_ok": 2,
  "num_warn": 1,
  "num_block": 0,
  "num_high_drift": 1,
  "median_alignment_score": 0.9
}
```

### 17.4 Field Descriptions

| Field | Type | Description |
|-------|------|-------------|
| `schema_version` | string | Schema version identifier (currently "1.0.0") |
| `num_experiments` | integer | Total number of calibration experiments included in the panel |
| `num_ok` | integer | Count of experiments with `integrity_status == "OK"` |
| `num_warn` | integer | Count of experiments with `integrity_status == "WARN"` |
| `num_block` | integer | Count of experiments with `integrity_status == "BLOCK"` |
| `num_high_drift` | integer | Count of experiments with `drift_band == "HIGH_DRIFT"` |
| `median_alignment_score` | float | Median alignment score across all experiments (0.0-1.0) |

### 17.5 Interpretation Guide

**For External Reviewers:**

The curriculum coherence panel should be read as a **cross-experiment alignment witness**. It aggregates individual experiment snapshots (from `build_cal_exp_curriculum_coherence_snapshot()`) to provide a unified view of curriculum coherence across the calibration suite.

**Key Interpretation Points:**

- **`num_ok`, `num_warn`, `num_block`** — These counts indicate how many experiments show each integrity status. A panel with `num_ok: 3, num_warn: 0, num_block: 0` indicates all experiments show clean alignment. A panel with `num_block > 0` indicates at least one experiment has critical breaks.

- **`num_high_drift`** — This count indicates how many experiments show high taxonomy drift. A panel with `num_high_drift: 2` out of 3 experiments suggests taxonomy evolution is occurring across multiple experiments, which may indicate systematic taxonomy changes.

- **`median_alignment_score`** — This provides a central tendency measure of alignment across experiments. A median of 0.9+ indicates strong overall alignment; a median below 0.7 suggests alignment concerns across the calibration suite.

**Context for P5 Calibration:**

In a P5 calibration evidence pack, the curriculum coherence panel reflects taxonomy integrity across CAL-EXP-1/2/3. Each experiment snapshot captures:
- Alignment score and integrity status from P3-style taxonomy analysis
- Drift band and projected horizon from P4-style taxonomy calibration

The panel aggregates these snapshots to answer: "Is curriculum coherence consistent across calibration experiments?"

**Advisory Nature:**

This panel is **evidence-only, not a gating signal**. It provides context for:
- Understanding curriculum coherence as a cross-experiment witness
- Identifying experiments with alignment concerns
- Reading taxonomy health as a unified calibration suite narrative
- Cross-checking against other evidence dimensions (budget, performance, metrics, stability)

The panel does **not** block or gate any decisions. It is purely observational and provides external reviewers with a single-page summary of curriculum coherence across calibration experiments.

**Explicit: No Gating in Phase X/P5 POC**

This panel is explicitly **not a gate** in Phase X or P5 POC. It is:
- Purely observational
- Advisory only
- Does not influence calibration experiment execution
- Does not block or allow any operations
- Provides context for external review only

### 17.6 Example

```json
{
  "schema_version": "1.0.0",
  "num_experiments": 3,
  "num_ok": 2,
  "num_warn": 1,
  "num_block": 0,
  "num_high_drift": 1,
  "median_alignment_score": 0.85
}
```

**Interpretation:** The calibration suite includes 3 experiments. Two show clean alignment (`num_ok: 2`), one shows warnings (`num_warn: 1`), and none show critical breaks (`num_block: 0`). One experiment shows high drift (`num_high_drift: 1`). The median alignment score is 0.85, indicating generally strong alignment with some variation. An external reviewer should investigate the experiment with warnings and high drift to understand why it differs from the others.

---

## 18. Metric Readiness Panel (Calibration View)

### 18.1 Overview

The **Metric Readiness Panel** provides cross-experiment readiness witness for P5 calibration experiments (CAL-EXP-1, CAL-EXP-2, CAL-EXP-3). This panel aggregates metric readiness signals across calibration experiments, providing a unified view of readiness status for external reviewers.

**Status:** ACTIVE — Included in Evidence Pack for Phase X.

**Purpose:** Enable external reviewers to understand metric readiness posture across calibration experiments. The panel provides:
- Cross-experiment readiness aggregation (OK/WARN/BLOCK counts)
- Global norm ranges across experiments
- Poly-fail detection across experiments
- Trivially readable summary for evidence pack review

**SHADOW MODE CONTRACT:**
- **No gating**: The panel is purely observational and does not gate any decisions
- **Advisory only**: All signals are advisory and do not influence execution or policy decisions
- **No enforcement**: No systems depend on panel values for control decisions
- **External reviewer lens**: Designed for external safety auditors to assess readiness independently

### 18.2 Canonical Source and Extraction

**Canonical Source:** The metric readiness panel is extracted from the evidence pack manifest first, with fallback to `evidence.json` if not found in the manifest.

**Extraction Order:**
1. **MANIFEST** (canonical): `manifest["governance"]["metric_readiness_panel"]`
2. **EVIDENCE_JSON** (fallback): `evidence.json["governance"]["metric_readiness_panel"]`
3. **MISSING**: Panel not found in either location

**Extraction Source Tracking:**
The status signal includes `extraction_source` field indicating where the panel was loaded from:
- `"MANIFEST"`: Panel loaded from manifest (canonical source)
- `"EVIDENCE_JSON"`: Panel loaded from evidence.json fallback (advisory warning generated)
- `"MISSING"`: Panel not found (signal not included in status)

**Advisory Warning:**
When `extraction_source == "EVIDENCE_JSON"`, exactly one neutral warning is generated:
```
"Metric readiness panel loaded from evidence.json fallback (manifest missing). Advisory only."
```

This warning indicates that the panel was loaded from the fallback source rather than the canonical manifest, which may indicate a manifest generation issue but does not block or gate any operations.

### 18.3 Evidence Binding Location

The metric readiness panel attaches to the main evidence structure at:

```
evidence["governance"]["metric_readiness_panel"]
```

### 18.4 Panel Schema

```json
{
  "schema_version": "1.0.0",
  "num_experiments": 3,
  "num_ok": 1,
  "num_warn": 1,
  "num_block": 1,
  "num_poly_fail": 0,
  "global_norm_range": {
    "p3_min": 0.2,
    "p3_max": 0.8,
    "p4_min": 0.2,
    "p4_max": 0.75
  },
  "top_driver_cal_ids": ["CAL-EXP-3", "CAL-EXP-2", "CAL-EXP-1"]
}
```

### 18.5 Status Signal Schema

The status signal in `first_light_status.json` includes:

```json
{
  "signals": {
    "metric_readiness_panel": {
      "schema_version": "1.0.0",
      "mode": "SHADOW",
      "extraction_source": "MANIFEST",
      "panel_schema_version": "1.0.0",
      "num_ok": 1,
      "num_warn": 1,
      "num_block": 1,
      "num_poly_fail": 0,
      "p4_global_norm_range": {
        "min": 0.2,
        "max": 0.75
      },
      "top_driver_cal_ids": ["CAL-EXP-3", "CAL-EXP-2", "CAL-EXP-1"],
      "top_driver_cal_ids_top3": ["CAL-EXP-1", "CAL-EXP-2", "CAL-EXP-3"]
    }
  }
}
```

**Field Descriptions:**

| Field | Type | Description |
|-------|------|-------------|
| `schema_version` | string | Status signal schema version (currently "1.0.0") |
| `mode` | string | Constant: "SHADOW" (indicates observational mode) |
| `extraction_source` | string | Source of panel: "MANIFEST" \| "EVIDENCE_JSON" \| "MISSING" |
| `panel_schema_version` | string | Panel schema version (passthrough from panel, or "UNKNOWN" if missing) |
| `num_ok` | integer | Count of experiments with OK status |
| `num_warn` | integer | Count of experiments with WARN status |
| `num_block` | integer | Count of experiments with BLOCK status |
| `num_poly_fail` | integer | Count of experiments with poly-fail detected |
| `p4_global_norm_range` | object | P4 global norm range (min/max across experiments) |
| `top_driver_cal_ids` | array | Full list of top driver cal_ids (for backward compatibility) |
| `top_driver_cal_ids_top3` | array | Explicit truncation to top 3 cal_ids, sorted deterministically |

### 18.6 GGFL Adapter Output

The GGFL adapter (`metric_readiness_panel_for_alignment_view()`) produces:

```json
{
  "signal_type": "SIG-READY",
  "status": "ok",
  "conflict": false,
  "drivers": ["DRIVER_BLOCK_PRESENT", "DRIVER_POLY_FAIL_PRESENT"],
  "summary": "Metric readiness panel: 3 experiment(s), 1 BLOCK, 1 poly-fail detected",
  "shadow_mode_invariants": {
    "advisory_only": true,
    "no_enforcement": true,
    "conflict_invariant": true
  }
}
```

**Driver Reason Codes:**
- `DRIVER_BLOCK_PRESENT`: At least one experiment has BLOCK status
- `DRIVER_POLY_FAIL_PRESENT`: At least one experiment has poly-fail detected
- `DRIVER_LOW_P4_NORM_RANGE`: P4 global norm min < 0.35 (documented threshold)

**Driver Ordering:** Deterministic ordering: BLOCK → POLY_FAIL → LOW_NORM (max 3 drivers)

**Shadow Mode Invariants:**
- `advisory_only: true`: Signal is purely advisory, no enforcement
- `no_enforcement: true`: No systems enforce based on this signal
- `conflict_invariant: true`: Conflict field is always `false` (invariant)

### 18.7 Interpretation Guide

**For External Reviewers:**
- The panel aggregates readiness across CAL-EXP-1/2/3
- `num_block > 0` indicates at least one experiment has critical readiness issues
- `num_poly_fail > 0` indicates simultaneous degradation across readiness, drift, and budget
- `top_driver_cal_ids_top3` lists the top 3 experiments driving concerns (sorted deterministically)
- `extraction_source` indicates data provenance (MANIFEST is canonical)

**Advisory Nature:**
- **No gating**: Panel does not block or gate any calibration decisions
- **Observational only**: Provides context for understanding readiness posture
- **External reviewer lens**: Designed for external safety auditors to assess readiness independently

---

## 19. Version History

| Version | Date | Changes |
|---------|------|---------|
| 1.0.0 | 2025-12-10 | Initial Evidence Pack Specification |
| 1.1.0 | 2025-12-11 | Added Section 12: Governance Final Check Integration (Phase Y) |
| 1.2.0 | 2025-12-11 | Added Section 2.4: Status File Pairing with `first_light_status.json` |
| 1.3.0 | 2025-12-11 | Added Section 5.6: P5 Real Telemetry Divergence Report slot reservation |
| 1.4.0 | 2025-12-11 | Added Section 13.7: PRNG Drift Ledger (Long-Horizon Signal) |
| 1.5.0 | 2025-12-11 | Added Section 13.8: PRNG Drift Delta (Calibration Comparison) |
| 1.6.0 | 2025-12-11 | Added Section 13.9: Scenario Drift Cluster View (Calibration Analysis) |
| 1.6.0 | 2025-12-11 | Added Section 11.7: Performance Calibration Summary (Δp Trajectory Analysis) |
| 1.7.0 | 2025-12-11 | Extended Section 11.7: Added calibration readiness hints (READY_FOR_EXTENDED_RUN, NEEDS_PARAMETER_TUNING, UNSTABLE_CALIBRATION) |
| 1.4.0 | 2025-12-11 | Added Section 13: PRNG First-Light Summary (Long-Horizon Footnote) |
| 1.5.0 | 2025-12-11 | Added Section 14: Topology Stress Summary (First Light) |
| 1.6.0 | 2025-12-11 | Added Section 14.5: Topology Stress Across Calibration Experiments (Heatmap Panel) |
| 1.7.0 | 2025-12-11 | Added Section 17: Curriculum Coherence Calibration Panel (P5) |
| 1.8.0 | 2025-12-11 | Added Section 18: Metric Readiness Panel (Calibration View) |

---

*Document Status: SPECIFICATION — Schema-Only (No Code)*
*Classification: INTERNAL — Pre-Launch Review Alignment*
