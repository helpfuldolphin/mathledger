diff --git a/.github/workflows/perf-gate.yml b/.github/workflows/perf-gate.yml
new file mode 100644
index 0000000..6e3aef0
--- /dev/null
+++ b/.github/workflows/perf-gate.yml
@@ -0,0 +1,310 @@
+name: Performance Gate
+
+on:
+  pull_request:
+    branches: [main, integrate/*]
+    paths:
+      - 'backend/**'
+      - 'tools/perf/**'
+      - '.github/workflows/perf-gate.yml'
+  push:
+    branches: [main]
+
+jobs:
+  performance-benchmarks:
+    runs-on: ubuntu-latest
+    timeout-minutes: 15
+    
+    steps:
+      - name: Checkout code
+        uses: actions/checkout@v4
+      
+      - name: Set up Python 3.11
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.11'
+      
+      - name: Install dependencies
+        run: |
+          python -m pip install --upgrade pip
+          pip install -r backend/requirements.txt
+      
+      - name: Run performance benchmarks
+        run: |
+          python tools/perf/perf_bench.py --all
+      
+      - name: Export performance summary
+        id: perf_summary
+        run: |
+          python tools/perf/export_perf_summary.py
+          
+          # Extract metrics for GitHub output
+          SPEEDUP=$(python -c "import json; print(json.load(open('artifacts/perf/perf_summary.json'))['overall_speedup'])")
+          STATUS=$(python -c "import json; print(json.load(open('artifacts/perf/perf_summary.json'))['ci_status'])")
+          MESSAGE=$(python -c "import json; print(json.load(open('artifacts/perf/perf_summary.json'))['ci_message'])")
+          
+          echo "speedup=$SPEEDUP" >> $GITHUB_OUTPUT
+          echo "status=$STATUS" >> $GITHUB_OUTPUT
+          echo "message=$MESSAGE" >> $GITHUB_OUTPUT
+      
+      - name: Check performance gate
+        run: |
+          python -c "
+          import json
+          import sys
+          
+          with open('artifacts/perf/perf_summary.json') as f:
+              summary = json.load(f)
+          
+          print(f\"\\n{summary['ci_message']}\")
+          print(f\"Overall speedup: {summary['overall_speedup']}× (target: {summary['target_speedup']}×)\")
+          print(f\"\\nDetailed metrics:\")
+          for name, metrics in summary['metrics'].items():
+              print(f\"  {name}: {metrics['speedup']}× speedup ({metrics['improvement_pct']}% improvement)\")
+          
+          if not summary['speedup_achieved']:
+              print(f\"\\n❌ FAIL: Performance target not met\")
+              sys.exit(1)
+          
+          print(f\"\\n✅ PASS: Performance target exceeded\")
+          "
+      
+      - name: Upload performance artifacts
+        if: always()
+        uses: actions/upload-artifact@v4
+        with:
+          name: performance-results
+          path: |
+            artifacts/perf/**/bench.json
+            artifacts/perf/**/report.txt
+            artifacts/perf/perf_summary.json
+          retention-days: 30
+      
+      - name: Comment PR with results
+        if: github.event_name == 'pull_request'
+        uses: actions/github-script@v6
+        with:
+          script: |
+            const fs = require('fs');
+            const summary = JSON.parse(fs.readFileSync('artifacts/perf/perf_summary.json', 'utf8'));
+            
+            const body = `## Performance Benchmark Results
+            
+            ${summary.ci_message}
+            
+            **Overall Speedup:** ${summary.overall_speedup}× (target: ${summary.target_speedup}×)
+            
+            ### Detailed Metrics
+            
+            | Operation | Baseline (ms) | Current (ms) | Speedup | Improvement |
+            |-----------|---------------|--------------|---------|-------------|
+            ${Object.entries(summary.metrics).map(([name, m]) => 
+              `| ${name.replace(/_/g, ' ')} | ${m.baseline_ms} | ${m.current_ms} | ${m.speedup}× | ${m.improvement_pct}% |`
+            ).join('\\n')}
+            
+            ### Cache Performance
+            
+            - **Hit Rate:** ${summary.cache.hit_rate_pct}%
+            
+            ${summary.speedup_achieved ? '✅ Performance gate: **PASSED**' : '❌ Performance gate: **FAILED**'}
+            `;
+            
+            github.rest.issues.createComment({
+              issue_number: context.issue.number,
+              owner: context.repo.owner,
+              repo: context.repo.repo,
+              body: body
+            });
+  
+  perf-gate-sealed:
+    runs-on: ubuntu-latest
+    timeout-minutes: 10
+    
+    steps:
+      - name: Checkout code
+        uses: actions/checkout@v4
+      
+      - uses: astral-sh/setup-uv@v1
+      - run: uv sync
+      
+      - name: Run 3x benchmarks with variance and seal pack
+        run: |
+          mkdir -p artifacts/perf
+          
+          uv run python -c "
+          import json
+          import time
+          import sys
+          import os
+          import hashlib
+          from datetime import datetime
+          
+          sys.path.insert(0, os.getcwd())
+          
+          from backend.axiom_engine.rules import apply_modus_ponens, _cached_normalize, _apply_modus_ponens_cached
+          
+          # Generate synthetic dataset
+          def generate_dataset(size):
+              statements = set()
+              atoms_count = size // 2
+              for i in range(1, atoms_count + 1):
+                  statements.add(f'p{i}')
+                  statements.add(f'p{i}->q{i}')
+              return statements
+          
+          statements = generate_dataset(5000)
+          
+          # Run 3 iterations with cache clearing
+          optimized_times = []
+          for run in range(3):
+              _cached_normalize.cache_clear()
+              _apply_modus_ponens_cached.cache_clear()
+              
+              start_time = time.perf_counter()
+              result = apply_modus_ponens(statements)
+              end_time = time.perf_counter()
+              optimized_times.append(end_time - start_time)
+          
+          # Calculate statistics
+          baseline_time_ms = 227.29  # Phase 2 baseline
+          avg_optimized = sum(optimized_times) / len(optimized_times)
+          variance = sum((t - avg_optimized) ** 2 for t in optimized_times) / len(optimized_times)
+          stddev = variance ** 0.5
+          speedup = baseline_time_ms / (avg_optimized * 1000)
+          speedup_variance = (stddev / avg_optimized) * speedup
+          
+          # Test cache effectiveness
+          _cached_normalize.cache_clear()
+          test_formulas = [
+              'p -> q', '(p) -> (q)', ' p -> q ',
+              'r /\\\\ s', '(r) /\\\\ (s)', ' r /\\\\ s ',
+              'a \\\\/ b', ' a \\\\/ b ', '(a \\\\/ b)'
+          ] * 100
+          
+          results = [_cached_normalize(f) for f in test_formulas]
+          cache_info = _cached_normalize.cache_info()
+          cache_hit_rate = cache_info.hits / (cache_info.hits + cache_info.misses) if cache_info.hits + cache_info.misses > 0 else 0
+          
+          # Create perf pack
+          perf_pack = {
+              'perf_uplift': {
+                  'baseline_ms': baseline_time_ms,
+                  'optimized_avg_ms': avg_optimized * 1000,
+                  'optimized_stddev_ms': stddev * 1000,
+                  'speedup': speedup,
+                  'speedup_variance': speedup_variance,
+                  'runs': optimized_times,
+                  'dataset_size': len(statements)
+              },
+              'cache_diagnostics': {
+                  'final_cache_state': {
+                      'hits': cache_info.hits,
+                      'misses': cache_info.misses,
+                      'size': cache_info.currsize,
+                      'maxsize': cache_info.maxsize
+                  },
+                  'hit_rate': cache_hit_rate,
+                  'test_formulas_count': len(test_formulas),
+                  'unique_formulas': len(set(test_formulas))
+              },
+              'timestamp': datetime.utcnow().isoformat() + 'Z',
+              'git_sha': os.environ.get('GITHUB_SHA', 'unknown')
+          }
+          
+          # Export RFC 8785 canonical JSON
+          canonical_json = json.dumps(perf_pack, sort_keys=True, separators=(',', ':'), ensure_ascii=True)
+          
+          with open('artifacts/perf/perf_pack.json', 'w') as f:
+              f.write(canonical_json)
+          
+          # Compute SHA256
+          pack_hash = hashlib.sha256(canonical_json.encode('utf-8')).hexdigest()
+          
+          # Validate gates
+          uplift_pass = speedup >= 3.0
+          cache_pass = cache_hit_rate >= 0.2
+          
+          # Print pass-lines
+          print(f'[PASS] Perf Uplift {speedup:.2f}x (±{speedup_variance:.2f})' if uplift_pass else f'[FAIL] Perf Uplift {speedup:.2f}x < 3.0x')
+          print(f'[PASS] Cache Hit {cache_hit_rate:.2%}' if cache_pass else f'[ABSTAIN] Cache Hit {cache_hit_rate:.2%} < 20%')
+          print(f'[PASS] Perf Pack: {pack_hash}')
+          
+          # Exit with appropriate code
+          if not uplift_pass:
+              print(f'ERROR: Speedup {speedup:.2f}x does not meet 3.0x threshold')
+              sys.exit(1)
+          
+          if not cache_pass:
+              print(f'WARNING: Cache hit rate {cache_hit_rate:.2%} < 20% threshold')
+              print('ABSTAIN: Cache effectiveness below threshold, but not blocking')
+          "
+      
+      - name: Upload perf pack artifacts
+        uses: actions/upload-artifact@v4
+        if: always()
+        with:
+          name: perf-pack-sealed-${{ github.run_number }}
+          path: artifacts/perf/perf_pack.json
+          retention-days: 30
+      
+      - name: Generate Job Summary
+        if: always()
+        run: |
+          python -c "
+          import json
+          import os
+          import hashlib
+          
+          if not os.path.exists('artifacts/perf/perf_pack.json'):
+              print('ERROR: perf_pack.json not found')
+              exit(1)
+          
+          with open('artifacts/perf/perf_pack.json', 'r') as f:
+              content = f.read()
+              perf_pack = json.loads(content)
+          
+          pack_hash = hashlib.sha256(content.encode('utf-8')).hexdigest()
+          
+          uplift = perf_pack['perf_uplift']
+          cache = perf_pack['cache_diagnostics']
+          
+          speedup = uplift['speedup']
+          speedup_var = uplift['speedup_variance']
+          cache_hit = cache['hit_rate']
+          
+          summary = f'''# Perf Gate Results
+          
+          ## Performance Uplift
+          
+          | Metric | Value | Status |
+          |--------|-------|--------|
+          | Speedup | {speedup:.2f}x ±{speedup_var:.2f} | {'✅ PASS' if speedup >= 3.0 else '❌ FAIL'} |
+          | Baseline | {uplift['baseline_ms']:.2f}ms | - |
+          | Optimized | {uplift['optimized_avg_ms']:.2f}ms ±{uplift['optimized_stddev_ms']:.2f}ms | - |
+          
+          ## Cache Effectiveness
+          
+          | Metric | Value | Status |
+          |--------|-------|--------|
+          | Hit Rate | {cache_hit * 100:.2f}% | {'✅ PASS' if cache_hit >= 0.2 else '⚠️ ABSTAIN'} |
+          | Hits | {cache['final_cache_state']['hits']} | - |
+          | Misses | {cache['final_cache_state']['misses']} | - |
+          
+          ## Perf Pack Seal
+          
+          - **SHA256**: \`{pack_hash}\`
+          - **Timestamp**: {perf_pack['timestamp']}
+          - **Git SHA**: \`{perf_pack['git_sha']}\`
+          
+          ## Pass-Lines
+          
+          \`\`\`
+          [{'PASS' if speedup >= 3.0 else 'FAIL'}] Perf Uplift {speedup:.2f}x (±{speedup_var:.2f})
+          [{'PASS' if cache_hit >= 0.2 else 'ABSTAIN'}] Cache Hit {cache_hit * 100:.2f}%
+          [PASS] Perf Pack: {pack_hash}
+          \`\`\`
+          '''
+          
+          with open(os.environ['GITHUB_STEP_SUMMARY'], 'w') as f:
+              f.write(summary)
+          "
