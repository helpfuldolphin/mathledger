\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{enumitem}

% Import Macros and Variables
\input{macros}

% Theorem environments
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{remark}{Remark}

\title{\textbf{\projectname}: A Verifiable Learning Substrate with Ledger-Attested Feedback}
\author{The MathLedger Research Fleet}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
Contemporary AI systems achieve extraordinary performance yet remain opaque and non-verifiable, creating a crisis of trust for safety-critical deployment. We introduce \projectname{}, a substrate for \emph{verifiable machine cognition} that integrates formal verification, cryptographic attestation, and learning dynamics into a single epistemic loop. The system implements \emph{Reflexive Formal Learning} (RFL), a symbolic analogue of gradient descent where updates are driven by verified proof events rather than statistical loss.

Phase I experiments validate the measurement and governance substrate under controlled conditions. All runs produced the expected fail-closed behavior; no convergence or capability claims are made. The contribution is infrastructural: a working prototype of ledger-attested learning that enables auditability at scale.

\textbf{Keywords:} verifiable learning, formal verification, cryptographic attestation, reflexive feedback, fail-closed governance
\end{abstract}

%=============================================================================
\section{Introduction: The Verifiability Gap}
\label{sec:intro}

Modern large language models are universal approximators of text, not of truth. Hallucination is structurally baked into density-estimation objectives; conventional evaluations penalize abstention and reward confident output regardless of correctness \cite{marcus2020}. In safety-critical domains---finance, law, infrastructure, policy---this creates an untenable gap between capability and trust.

The AI industry is discovering a structural constraint:
\begin{quote}
\emph{Performance without verifiability is not deployable at scale.}
\end{quote}

Mathematics offers a way out: verifiable reasoning with machine-checkable proofs. \projectname{} converts mathematics into a \emph{living protocol} for learning under formal law.

\subsection{What Problem Does This Address?}

Existing approaches to improving AI reliability fall into three categories, each with limitations:

\begin{enumerate}[leftmargin=1.5em]
\item \textbf{Reward shaping (RLHF, DPO):} Human preferences guide learning, but preferences are noisy, inconsistent, and gameable. The feedback signal is statistical, not verifiable.

\item \textbf{Verifier-guided generation:} Proof assistants check outputs post-hoc, but rejected outputs provide no structured learning signal. The verifier is a filter, not a teacher.

\item \textbf{Benchmark scaling:} Larger test sets reduce variance but do not establish correctness. Passing benchmarks does not imply understanding.
\end{enumerate}

\projectname{} takes a different approach: \emph{the verifier's binary decision becomes the learning signal itself}. Every update is justified by a formally verified proof or explicit abstention, recorded in an immutable ledger. This creates a closed epistemic loop where learning is constrained to verified truth.

\subsection{The Chain of Verifiable Cognition}

The system implements an end-to-end pipeline:
\[
\text{Input} \to \text{Proof-or-Abstain} \to \text{Ledger Attestation} \to \text{Dual Commitment} \to \text{Policy Update}
\]

Each component is cryptographically bound:
\begin{itemize}[leftmargin=1.5em]
\item \textbf{Proof-or-Abstain:} The Lean kernel verifies reasoning or the system explicitly abstains. No middle ground.
\item \textbf{Ledger Attestation:} Verified events are sealed into a monotone, append-only ledger with Merkle roots.
\item \textbf{Dual Commitment:} Both reasoning artifacts ($r_t$) and interface state ($u_t$) are committed: $H_t = \mathrm{Hash}(\texttt{EPOCH:} \| r_t \| u_t)$.
\item \textbf{Policy Update:} Reflexive Formal Learning (RFL) adjusts the policy based on verification outcomes.
\end{itemize}

This architecture enables a new primitive: \emph{learning from verified truth rather than statistical loss}.

\subsection{What Is Genuinely New}

\projectname{} combines three elements not previously integrated:

\begin{enumerate}[leftmargin=1.5em]
\item \textbf{Ledger-attested learning signals:} Unlike reward models or human feedback, the learning signal is a cryptographically committed verification outcome.

\item \textbf{Fail-closed governance:} The system cannot silently degrade. Either verification succeeds and learning proceeds, or the system abstains and logs the failure.

\item \textbf{Auditability as infrastructure:} Every update has a replayable provenance chain. Post-hoc analysis can reconstruct exactly what was learned and why.
\end{enumerate}

This paper reports Phase I experiments that validate the substrate. No capability or convergence claims are made.

%=============================================================================
\section{System Architecture}
\label{sec:architecture}

\subsection{Pipeline Overview}

\input{sections/02_methodology}

\subsection{The Monotone Ledger}

\begin{definition}[Monotone Ledger]
A ledger $\mathcal{L}$ is a sequence of blocks $(B_1, B_2, \ldots)$ where each $B_t$ contains verified proofs with: (i) canonical statement hashes, (ii) Lean verification status, and (iii) a Merkle root $R_t$ over sorted proof IDs. The ledger is \emph{monotone} if $\bigcup_{i \le t} B_i \subseteq \bigcup_{i \le t+1} B_i$.
\end{definition}

Monotonicity ensures that verified knowledge only grows. Statements cannot be retracted; only new proofs can be added.

\subsection{Dual Attestation}

At each epoch $t$, the system commits to two roots:
\begin{itemize}[leftmargin=1.5em]
\item $r_t$: Reasoning root over canonicalized proof artifacts
\item $u_t$: UI root over interface state (DOM, logs, user confirmations)
\end{itemize}

These are bound by $H_t = \mathrm{Hash}(\texttt{EPOCH:} \| r_t \| u_t)$ with prefix-free domain separation. The tuple $(r_t, u_t, H_t)$ is the \emph{epistemic fingerprint} of the epoch---the only scalar permitted as a summary of what occurred.

%=============================================================================
\section{Reflexive Formal Learning: Formal Anchor}
\label{sec:rfl}

Reflexive Formal Learning (RFL) is a symbolic analogue of gradient descent operating on verification outcomes rather than numerical errors.

\subsection{Core Definitions}

Let $\Pi$ be the space of symbolic reasoning policies and $P_\pi$ the event distribution induced by policy $\pi$.

\begin{definition}[Verification Outcome]
For reasoning event $e_t$, the verifier produces:
\[
\mathcal{V}(e_t) \in \{1, 0, \bot\}
\]
where $1$ = verified proof, $0$ = failed proof, $\bot$ = abstention.
\end{definition}

\begin{definition}[Epistemic Risk]
\label{def:epistemic-risk}
The epistemic risk of policy $\pi$ is:
\[
\mathcal{J}(\pi) = \mathbb{E}_{e \sim P_\pi}[\mathbf{1}\{\mathcal{V}(e) \neq 1\}] = \Pr_{e \sim P_\pi}[\mathcal{V}(e) \neq 1]
\]
This measures the probability mass on non-verified events (failures and abstentions).
\end{definition}

\subsection{The RFL Update Rule}

At each step $t$:
\begin{equation}
\label{eq:rfl-update}
\pi_{t+1} = \pi_t \oplus \eta \cdot \Phi(\mathcal{V}(e_t), \pi_t)
\end{equation}
where $\oplus$ is algebraic composition on policy space and $\Phi: \{1, 0, \bot\} \times \Pi \to \Delta\Pi$ maps verification outcomes to policy adjustments.

The intuition is:
\begin{quote}
\emph{Policies that cause fewer failures and abstentions become more likely; policies that cause them become less likely.}
\end{quote}

\begin{proposition}[RFL as Stochastic Approximation]
\label{prop:sa}
Under standard regularity conditions (bounded updates, filtration adaptivity, summable step sizes), the RFL dynamics constitute a stochastic approximation process on $\mathcal{J}(\pi)$. The learning signal arises from verified truth rather than statistical loss.
\end{proposition}

\begin{remark}
Proposition~\ref{prop:sa} establishes that RFL has the mathematical structure of a learning algorithm. It does not claim convergence in finite time or under Phase I conditions. Full convergence theory is developed in the companion research paper (rp.tex).
\end{remark}

\subsection{Abstention as First-Class Outcome}

Unlike reward-based systems that penalize abstention, RFL treats it as informative:
\begin{itemize}[leftmargin=1.5em]
\item Abstention prevents false positives (hallucinations committed to ledger)
\item Abstention rates provide signal about policy quality
\item High abstention with stable $\mathcal{J}(\pi)$ indicates the policy is appropriately cautious
\end{itemize}

This inverts the standard incentive structure: the system is rewarded for knowing what it does not know.

%=============================================================================
\section{Phase I Experimental Results}
\label{sec:results}

\input{sections/03_results}

\subsection{Interpreting Phase I Outcomes}

The Phase I results establish three facts:

\begin{enumerate}[leftmargin=1.5em]
\item \textbf{The measurement substrate works.} $\Delta p$ (success rate proxy) is computable per cycle. Variance between arms is measurable.

\item \textbf{Fail-closed governance triggers correctly.} F5.2 (variance ratio out of bounds) and F5.3 (windowed drift excessive) fired in all runs, capping claims at L0.

\item \textbf{Non-convergence is informative, not a failure.} Phase I was designed to validate infrastructure, not demonstrate capability. The fact that fail-close triggers fired correctly \emph{is} the success condition.
\end{enumerate}

%=============================================================================
\section{Discussion: Why This Matters}
\label{sec:discussion}

Phase I experiments aimed to characterize the behavior of the \projectname{} substrate under various configurations. The \Ht{} dynamics (Figure~\ref{fig:rfl_ht_convergence}) were observed to understand system states.

\subsection{Comparison to Adjacent Work}

\projectname{} occupies a distinct position in the landscape of verifiable AI:

\begin{table}[htbp]
\centering
\begin{tabular}{l|c|c|c}
\textbf{Approach} & \textbf{Learning Signal} & \textbf{Auditability} & \textbf{Fail-Closed} \\
\hline
RLHF & Human preference & Low & No \\
Verifier-guided & Post-hoc filter & Medium & No \\
Proof-carrying code & None (static) & High & Yes \\
\textbf{\projectname{} (RFL)} & \textbf{Verified truth} & \textbf{High} & \textbf{Yes} \\
\end{tabular}
\caption{Comparison of approaches to reliable AI. \projectname{} uniquely combines verified learning signals with fail-closed governance.}
\label{tab:comparison}
\end{table}

\subsection{Layer-3 Infrastructure}

\projectname{} is not a proof generator or a user-facing application. It is \emph{Layer-3 infrastructure}: the flight data recorder for AI reasoning.

\begin{itemize}[leftmargin=1.5em]
\item \textbf{Layer 1 (Human):} Users pose queries, interpret results, make decisions
\item \textbf{Layer 2 (Engine):} AI models generate formal artifacts
\item \textbf{Layer 3 (Ledger):} \projectname{} provides immutable provenance and attestation
\end{itemize}

The system does not compete with proof generators; it makes their outputs trustworthy at scale.

%=============================================================================
\section{Explicit Non-Claims and Scope Boundaries}
\label{sec:non-claims}

To maintain epistemic discipline, we explicitly state what Phase I does \emph{not} establish:

\subsection{What Phase I Does NOT Establish}

\begin{itemize}[leftmargin=1.5em]
\item \textbf{Capability claims:} No claim that the system ``understands'' or ``reasons'' in any general sense.
\item \textbf{Convergence:} No claim that RFL converges under Phase I conditions. All runs failed the variance gate.
\item \textbf{Threshold validity:} Thresholds are frozen parameters, not validated optima.
\item \textbf{Generalization:} No out-of-distribution testing was performed.
\item \textbf{Real-world applicability:} Only synthetic harness data was used.
\end{itemize}

\subsection{SHADOW Mode Semantics}

All Phase I experiments operate in SHADOW mode: verification results are \emph{observational and non-blocking}. The system records what happened but does not gate production decisions.

\subsection{Phase Quarantine}

Phase I and Phase II are strictly separated:
\begin{itemize}[leftmargin=1.5em]
\item \textbf{Phase I:} Assumes ideal verifier, hermetic environment, synthetic data
\item \textbf{Phase II:} Tests governance stability under auxiliary perturbation (frozen but not executed)
\end{itemize}

No Phase II claims are made in this work. Phase II specification is frozen pending execution authorization.

%=============================================================================
\section{Future Work}
\label{sec:future_work}

Future work will focus on integrating RFL to observe if reflexive feedback can dampen oscillatory states in the decision boundary and achieve measurable reductions in abstention rates and improvements in convergence latency.

\subsection{Phase II Calibration}

Phase II of the calibration program addresses governance stability: specifically, whether the governance verdict (failure codes, claim level) is invariant under perturbation of auxiliary parameters not part of the frozen predicate set. The Phase II specification is frozen, but execution has not yet occurred. No claims regarding governance invariance or sensitivity are made in this work. Phase II results, when available, will be reported separately and will not retroactively modify the Phase I conclusions presented here.

%=============================================================================
\section{Conclusion}
\label{sec:conclusion}

\projectname{} demonstrates that ledger-attested learning is technically feasible. Phase I successfully established:

\begin{enumerate}[leftmargin=1.5em]
\item A working pipeline from proof generation through dual attestation to policy feedback
\item Measurement infrastructure for $\Delta p$ and variance metrics
\item Fail-closed governance that correctly triggers under out-of-bounds conditions
\item Explicit non-claims and scope boundaries that enable honest assessment
\end{enumerate}

The contribution is infrastructural, not empirical. We have built the substrate; demonstrating capability on that substrate is future work.

The system stands as proof-of-concept for a new paradigm: \emph{learning from verified truth}. Whether this paradigm scales to complex reasoning remains an open question. What Phase I establishes is that the question can now be asked with rigor.

%=============================================================================
\appendix

\section{Evidence Pack}
\label{app:evidence}

The following manifests provide cryptographic verification of the experimental runs.

\begin{table}[h]
    \centering
    \begin{tabular}{l l}
        \toprule
        \textbf{Artifact} & \textbf{SHA-256 (Short)} \\
        \midrule
        \Ht{} Snapshot & \valHtShortHash \\
        Mirror Audit Report & \texttt{artifacts/mirror/mirror\_report.json} \\
        Drift Table & \texttt{drift\_table.json} \\
        \bottomrule
    \end{tabular}
    \caption{Cryptographic manifest of key experimental artifacts.}
    \label{tab:manifest}
\end{table}

All artifacts are available in the evidence directory: \texttt{docs/evidence/cal\_exp\_3/}.

%=============================================================================
\bibliographystyle{plain}
\bibliography{references}

\end{document}
