# Policy Learning Radar & Governance Hooks

**Status:** Phase III Complete âœ…  
**Module:** `rfl.policy_telemetry`  
**Tests:** 29 tests passing  
**Contract:** Pure, deterministic, metadata-only

## Overview

The Policy Learning Radar provides comprehensive monitoring and governance hooks for RFL policy evolution. It enables:

1. **Per-slice telemetry** - Extract key metrics from policy state snapshots
2. **Drift detection** - Compare policy evolution across curriculum slices
3. **Governance hooks** - Classify policy status for oversight (OK/ATTENTION/VOLATILE)
4. **Global health monitoring** - Track system-wide policy surface health (OK/WARN/HOT)

All functions are **pure** (no side effects) and **deterministic** (same inputs â†’ same outputs), ensuring compatibility with RFL's determinism guarantees.

## API Reference

### PolicyStateSnapshot

```python
@dataclass
class PolicyStateSnapshot:
    """Immutable snapshot of policy state for telemetry extraction."""
    schema_version: str      # Version identifier (e.g., "v1")
    slice_name: str          # Curriculum slice identifier
    update_count: int        # Number of updates applied (â‰¥ 0)
    weights: Dict[str, float]  # Feature name â†’ weight value
    clamped: bool = False    # Whether clamping was applied
    clamp_count: int = 0     # Number of clamp events (â‰¥ 0)
```

**Validation:**
- `update_count` and `clamp_count` must be non-negative
- `weights` must be a dictionary
- Raises `ValueError` or `TypeError` on invalid inputs

### build_policy_telemetry_snapshot

```python
def build_policy_telemetry_snapshot(
    snapshot: PolicyStateSnapshot
) -> Dict[str, Any]:
    """
    Extract telemetry metrics from policy state snapshot.
    
    Returns:
        - schema_version: str
        - slice_name: str
        - update_count: int
        - weight_norm_l1: float          # âˆ‘|w_i|
        - weight_norm_l2: float          # âˆš(âˆ‘w_iÂ²)
        - nonzero_weights: int           # Count of |w_i| > 1e-9
        - top_k_positive_features: List[Tuple[str, float]]  # K=3
        - top_k_negative_features: List[Tuple[str, float]]  # K=3
        - clamped: bool
        - clamp_count: int
    """
```

**Example:**
```python
snapshot = PolicyStateSnapshot(
    schema_version="v1",
    slice_name="baseline",
    update_count=10,
    weights={"len": -0.5, "depth": 0.3, "success": 1.2},
)

telemetry = build_policy_telemetry_snapshot(snapshot)
# telemetry["weight_norm_l2"] â†’ 1.334...
# telemetry["top_k_positive_features"] â†’ [("success", 1.2), ("depth", 0.3)]
# telemetry["top_k_negative_features"] â†’ [("len", -0.5)]
```

### compare_policy_snapshots

```python
def compare_policy_snapshots(
    before: PolicyStateSnapshot,
    after: PolicyStateSnapshot
) -> Dict[str, Any]:
    """
    Compare two policy snapshots to detect changes.
    
    Returns:
        - slice_name: str
        - l2_distance: float             # âˆš(âˆ‘(w_after - w_before)Â²)
        - l1_distance: float             # âˆ‘|w_after - w_before|
        - num_sign_flips: int            # Features that changed sign
        - top_features_changed: List[Tuple[str, float, float, float]]
          # [(feature, before_val, after_val, delta), ...] (K=3)
    """
```

**Example:**
```python
before = PolicyStateSnapshot(
    schema_version="v1", slice_name="test",
    update_count=5, weights={"len": 0.5, "depth": 0.3}
)
after = PolicyStateSnapshot(
    schema_version="v1", slice_name="test",
    update_count=10, weights={"len": -0.5, "depth": 0.8}
)

comparison = compare_policy_snapshots(before, after)
# comparison["l2_distance"] â†’ ~1.077
# comparison["num_sign_flips"] â†’ 1 (len: + â†’ -)
# comparison["top_features_changed"] â†’ [("len", 0.5, -0.5, -1.0), ...]
```

### build_policy_drift_radar

```python
def build_policy_drift_radar(
    comparisons: Sequence[Dict[str, Any]]
) -> Dict[str, Any]:
    """
    Build drift radar from sequence of policy comparisons.
    
    Args:
        comparisons: List of dicts from compare_policy_snapshots()
    
    Returns:
        - slices: List[Dict]             # Per-slice metrics
        - slices_with_large_drift: List[str]  # L2 > 0.5
        - max_l2_distance: float         # Maximum across all slices
        - num_slices_analyzed: int
    """
```

**Drift Threshold:** L2 distance > 0.5 indicates significant policy change

**Example:**
```python
comparisons = [comparison1, comparison2, comparison3]
radar = build_policy_drift_radar(comparisons)

# radar["max_l2_distance"] â†’ 1.2
# radar["slices_with_large_drift"] â†’ ["medium", "advanced"]
```

### summarize_policy_for_governance

```python
def summarize_policy_for_governance(
    radar: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Generate governance summary from drift radar.
    
    Status Classification:
        - "OK": No large drift detected
        - "ATTENTION": 1-2 slices with large drift
        - "VOLATILE": 3+ slices with large drift
    
    Returns:
        - has_large_drift: bool
        - slices_with_large_drift: List[str]
        - status: str  # "OK" | "ATTENTION" | "VOLATILE"
    """
```

**Example:**
```python
gov_summary = summarize_policy_for_governance(radar)

if gov_summary["status"] == "VOLATILE":
    print(f"âš ï¸ Review slices: {gov_summary['slices_with_large_drift']}")
```

### summarize_policy_for_global_health

```python
def summarize_policy_for_global_health(
    radar: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Generate global health summary from drift radar.
    
    Health Status Levels:
        - "OK": max_l2_distance â‰¤ 0.5
        - "WARN": 0.5 < max_l2_distance â‰¤ 1.0
        - "HOT": max_l2_distance > 1.0
    
    Returns:
        - policy_surface_ok: bool
        - max_l2_distance: float
        - num_slices_analyzed: int
        - status: str  # "OK" | "WARN" | "HOT"
    """
```

**Example:**
```python
health_summary = summarize_policy_for_global_health(radar)

if health_summary["status"] == "HOT":
    print(f"ðŸ”¥ Max drift: {health_summary['max_l2_distance']:.3f}")
```

## Complete Example

```python
from rfl.policy_telemetry import (
    PolicyStateSnapshot,
    build_policy_telemetry_snapshot,
    compare_policy_snapshots,
    build_policy_drift_radar,
    summarize_policy_for_governance,
    summarize_policy_for_global_health,
)

# 1. Create snapshots
before = PolicyStateSnapshot(
    schema_version="v1",
    slice_name="baseline",
    update_count=10,
    weights={"len": 0.5, "depth": 0.3, "success": 1.0},
)

after = PolicyStateSnapshot(
    schema_version="v1",
    slice_name="baseline",
    update_count=20,
    weights={"len": -0.8, "depth": 0.9, "success": 1.5},
)

# 2. Extract telemetry
telemetry = build_policy_telemetry_snapshot(after)
print(f"L2 norm: {telemetry['weight_norm_l2']:.3f}")

# 3. Detect drift
comparison = compare_policy_snapshots(before, after)
print(f"Drift: L2={comparison['l2_distance']:.3f}")

# 4. Build radar
radar = build_policy_drift_radar([comparison])

# 5. Check governance
gov_summary = summarize_policy_for_governance(radar)
print(f"Status: {gov_summary['status']}")

# 6. Check health
health_summary = summarize_policy_for_global_health(radar)
print(f"Health: {health_summary['status']}")
```

## Testing

Run the comprehensive test suite:

```bash
python3 -m pytest tests/rfl/test_policy_telemetry.py -v
```

Test coverage:
- âœ… Snapshot validation (4 tests)
- âœ… Telemetry extraction (5 tests)
- âœ… Policy comparison (5 tests)
- âœ… Drift radar (4 tests)
- âœ… Governance summaries (3 tests)
- âœ… Global health (5 tests)
- âœ… Integration & determinism (3 tests)

**Total: 29 tests, all passing**

## Contracts & Guarantees

### Determinism

All functions are **deterministic**:
```python
# Same inputs always produce identical outputs
telemetry1 = build_policy_telemetry_snapshot(snapshot)
telemetry2 = build_policy_telemetry_snapshot(snapshot)
assert telemetry1 == telemetry2  # Always true
```

### Purity

All functions are **pure** (no side effects):
- No state mutation
- No I/O operations
- No external dependencies
- No randomness

### Metadata-Only

**Critical:** This module performs **metadata extraction only**. It does **NOT**:
- âŒ Change policy learning behavior
- âŒ Modify policy weights
- âŒ Trigger policy updates
- âŒ Affect experiment outcomes

It **only**:
- âœ… Reads policy state
- âœ… Computes metrics
- âœ… Provides monitoring data

## Integration with RFL Runner

The Policy Learning Radar integrates with the RFL runner to provide real-time monitoring:

```python
from rfl.runner import RFLRunner
from rfl.policy_telemetry import PolicyStateSnapshot, build_policy_telemetry_snapshot

runner = RFLRunner(config)

# After policy updates, create snapshot
snapshot = PolicyStateSnapshot(
    schema_version="v1",
    slice_name="baseline",
    update_count=runner.policy_update_count,
    weights=runner.policy_weights.copy(),
    clamped=False,  # From guardrail logic
    clamp_count=0,
)

# Extract telemetry
telemetry = build_policy_telemetry_snapshot(snapshot)

# Monitor drift, governance, health...
```

## Thresholds & Calibration

### Drift Detection
- **Large Drift:** L2 distance > 0.5
- **Rationale:** Empirically calibrated for 3-parameter policies

### Governance Status
- **OK:** 0 slices with large drift
- **ATTENTION:** 1-2 slices with large drift
- **VOLATILE:** 3+ slices with large drift

### Health Status
- **OK:** max_l2_distance â‰¤ 0.5
- **WARN:** 0.5 < max_l2_distance â‰¤ 1.0
- **HOT:** max_l2_distance > 1.0

## Phase III Deliverables

âœ… **TASK 1:** Policy telemetry snapshot contract  
âœ… **TASK 2:** Policy drift radar across slices  
âœ… **TASK 3:** Governance / global health summaries  

**Definition of Done:**
- âœ… All functions implemented
- âœ… 29 tests passing
- âœ… Existing 56 tests still passing
- âœ… Pure & deterministic
- âœ… Metadata-only
- âœ… Code review passed
- âœ… Security scan: 0 alerts

## See Also

- `/rfl/runner.py` - RFL runner implementation
- `/rfl/audit.py` - RFL Law compliance auditing
- `/tests/rfl/test_policy_telemetry.py` - Comprehensive test suite
- `/tmp/demo_policy_telemetry.py` - Usage demonstration

---

**Phase III Complete** â€” Policy Learning Radar operational and governance-ready.
