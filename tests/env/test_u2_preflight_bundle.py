#!/usr/bin/env python3
"""
test_u2_preflight_bundle.py - Tests for U2 Pre-Flight Bundle Orchestrator

PHASE II â€” NOT RUN IN PHASE I

Tests cover:
- Full bundle PASS scenario
- Each stage failure scenario
- Proper manifest flagging (AUDIT_ELIGIBLE vs NOT_ELIGIBLE)
- CLI exit codes
"""

import json
import os
import sys
import tempfile
import unittest
from pathlib import Path
from unittest.mock import patch, MagicMock

# Add scripts path
sys.path.insert(0, str(Path(__file__).parent.parent.parent / "scripts"))

import u2_preflight_bundle as bundle
from u2_preflight_bundle import (
    CheckStatus,
    CheckResult,
    StageResult,
    PreflightConfig,
    AuditEligibility,
    run_preflight_bundle,
    generate_bundle_report,
    pf_101_env_mode_set,
    pf_102_mode_value_valid,
    pf_103_lock_file_check,
    pf_104_operator_identity,
    pf_105_phase2_confirmation,
    pf_201_prereg_file_exists,
    pf_203_seed_matches_prereg,
    pf_301_cache_root_writable,
    pf_304_no_symlinks,
    pf_307_run_id_valid,
    pf_401_seed_format,
    pf_405_no_banned_randomness,
    pf_501_cycle_limit_valid,
    pf_502_snapshot_interval,
    pf_601_run_isolation,
    pf_701_db_connectivity,
    pf_801_nfr001,
)


class TestCheckResult(unittest.TestCase):
    """Tests for CheckResult dataclass."""

    def test_check_result_pass(self):
        result = CheckResult(
            id="PF-TEST",
            stage=1,
            status=CheckStatus.PASS,
            message="Test passed"
        )
        self.assertTrue(result.passed())
        self.assertFalse(result.is_critical_failure())

    def test_check_result_fail(self):
        result = CheckResult(
            id="PF-TEST",
            stage=1,
            status=CheckStatus.FAIL,
            message="Test failed"
        )
        self.assertFalse(result.passed())
        self.assertTrue(result.is_critical_failure())

    def test_check_result_to_dict(self):
        result = CheckResult(
            id="PF-101",
            stage=1,
            status=CheckStatus.PASS,
            message="Mode set",
            data={"mode": "uplift_experiment"}
        )
        d = result.to_dict()
        self.assertEqual(d["id"], "PF-101")
        self.assertEqual(d["stage"], 1)
        self.assertEqual(d["status"], "PASS")
        self.assertEqual(d["data"]["mode"], "uplift_experiment")


class TestStage1EnvironmentBootstrap(unittest.TestCase):
    """Tests for Stage 1: Environment Bootstrap checks."""

    def setUp(self):
        self.temp_dir = tempfile.mkdtemp()
        self.config = PreflightConfig(
            run_dir=Path(self.temp_dir) / "test-run",
            run_id="test-run-123",
            operator_id="test@operator.io",
            confirm_phase2=True,
        )

    def tearDown(self):
        import shutil
        shutil.rmtree(self.temp_dir, ignore_errors=True)

    @patch.dict(os.environ, {}, clear=True)
    def test_pf_101_env_mode_not_set(self):
        result = pf_101_env_mode_set(self.config)
        self.assertEqual(result.status, CheckStatus.FAIL)
        self.assertIn("not set", result.message)

    @patch.dict(os.environ, {"RFL_ENV_MODE": "uplift_experiment"})
    def test_pf_101_env_mode_set(self):
        result = pf_101_env_mode_set(self.config)
        self.assertEqual(result.status, CheckStatus.PASS)

    @patch.dict(os.environ, {"RFL_ENV_MODE": "invalid_mode"})
    def test_pf_102_invalid_mode(self):
        result = pf_102_mode_value_valid(self.config)
        self.assertEqual(result.status, CheckStatus.FAIL)
        self.assertIn("Invalid mode", result.message)

    @patch.dict(os.environ, {"RFL_ENV_MODE": "phase1-hermetic"})
    def test_pf_102_valid_mode(self):
        result = pf_102_mode_value_valid(self.config)
        self.assertEqual(result.status, CheckStatus.PASS)

    def test_pf_103_no_lock_file(self):
        # Run dir doesn't exist yet, so no lock file
        result = pf_103_lock_file_check(self.config)
        self.assertEqual(result.status, CheckStatus.PASS)

    def test_pf_103_lock_file_exists(self):
        self.config.run_dir.mkdir(parents=True, exist_ok=True)
        (self.config.run_dir / ".mode_lock").write_text("{}")
        result = pf_103_lock_file_check(self.config)
        self.assertEqual(result.status, CheckStatus.FAIL)
        self.assertIn("conflict", result.message)

    def test_pf_104_operator_id_missing(self):
        config = PreflightConfig(
            run_dir=self.config.run_dir,
            run_id="test",
            operator_id=None,
        )
        result = pf_104_operator_identity(config)
        self.assertEqual(result.status, CheckStatus.FAIL)

    def test_pf_104_operator_id_present(self):
        result = pf_104_operator_identity(self.config)
        self.assertEqual(result.status, CheckStatus.PASS)

    def test_pf_105_no_confirmation(self):
        config = PreflightConfig(
            run_dir=self.config.run_dir,
            run_id="test",
            confirm_phase2=False,
        )
        result = pf_105_phase2_confirmation(config)
        self.assertEqual(result.status, CheckStatus.FAIL)

    def test_pf_105_confirmed(self):
        result = pf_105_phase2_confirmation(self.config)
        self.assertEqual(result.status, CheckStatus.PASS)


class TestStage2PreregistrationBinding(unittest.TestCase):
    """Tests for Stage 2: Preregistration Binding checks."""

    def setUp(self):
        self.temp_dir = tempfile.mkdtemp()
        self.prereg_file = Path(self.temp_dir) / "PREREG_UPLIFT_U2.yaml"
        self.prereg_file.write_text("experiments:\n  - id: U2_EXP_001\n")
        self.prereg_hash = bundle.hashlib.sha256(self.prereg_file.read_bytes()).hexdigest()

        self.config = PreflightConfig(
            run_dir=Path(self.temp_dir) / "run",
            run_id="test-run",
            prereg_file=self.prereg_file,
        )

    def tearDown(self):
        import shutil
        shutil.rmtree(self.temp_dir, ignore_errors=True)

    def test_pf_201_prereg_exists(self):
        result = pf_201_prereg_file_exists(self.config)
        self.assertEqual(result.status, CheckStatus.PASS)

    def test_pf_201_prereg_missing(self):
        config = PreflightConfig(
            run_dir=self.config.run_dir,
            run_id="test",
            prereg_file=Path("/nonexistent/file.yaml"),
        )
        result = pf_201_prereg_file_exists(config)
        self.assertEqual(result.status, CheckStatus.FAIL)

    @patch.dict(os.environ, {"U2_MASTER_SEED": ""})
    def test_pf_203_seed_mismatch(self):
        os.environ["U2_MASTER_SEED"] = "b" * 64
        result = pf_203_seed_matches_prereg(self.config)
        self.assertEqual(result.status, CheckStatus.FAIL)
        self.assertIn("mismatch", result.message)

    def test_pf_203_seed_matches(self):
        os.environ["U2_MASTER_SEED"] = self.prereg_hash
        result = pf_203_seed_matches_prereg(self.config)
        self.assertEqual(result.status, CheckStatus.PASS)


class TestStage3DirectoryFilesystem(unittest.TestCase):
    """Tests for Stage 3: Directory & Filesystem checks."""

    def setUp(self):
        self.temp_dir = tempfile.mkdtemp()
        self.config = PreflightConfig(
            run_dir=Path(self.temp_dir) / "run",
            run_id="test-run-uuid-12345678",
        )

    def tearDown(self):
        import shutil
        shutil.rmtree(self.temp_dir, ignore_errors=True)

    @patch.dict(os.environ, {"MATHLEDGER_CACHE_ROOT": ""}, clear=False)
    def test_pf_301_cache_root_not_set(self):
        os.environ.pop("MATHLEDGER_CACHE_ROOT", None)
        result = pf_301_cache_root_writable(self.config)
        self.assertEqual(result.status, CheckStatus.FAIL)

    def test_pf_301_cache_root_exists(self):
        os.environ["MATHLEDGER_CACHE_ROOT"] = self.temp_dir
        result = pf_301_cache_root_writable(self.config)
        self.assertEqual(result.status, CheckStatus.PASS)

    def test_pf_304_no_symlinks(self):
        os.environ["MATHLEDGER_CACHE_ROOT"] = self.temp_dir
        os.environ["MATHLEDGER_SNAPSHOT_ROOT"] = self.temp_dir
        os.environ["MATHLEDGER_EXPORT_ROOT"] = self.temp_dir
        result = pf_304_no_symlinks(self.config)
        self.assertEqual(result.status, CheckStatus.PASS)

    def test_pf_307_valid_run_id(self):
        result = pf_307_run_id_valid(self.config)
        # Not a UUID, but no path traversal - should warn or pass
        self.assertIn(result.status, [CheckStatus.PASS, CheckStatus.WARN])

    def test_pf_307_path_traversal(self):
        config = PreflightConfig(
            run_dir=Path(self.temp_dir),
            run_id="../../../etc/passwd",
        )
        result = pf_307_run_id_valid(config)
        self.assertEqual(result.status, CheckStatus.FAIL)


class TestStage4PRNGDeterminism(unittest.TestCase):
    """Tests for Stage 4: PRNG & Determinism checks."""

    def setUp(self):
        self.temp_dir = tempfile.mkdtemp()
        self.config = PreflightConfig(
            run_dir=Path(self.temp_dir) / "run",
            run_id="test-run",
        )

    def tearDown(self):
        import shutil
        shutil.rmtree(self.temp_dir, ignore_errors=True)

    @patch.dict(os.environ, {"U2_MASTER_SEED": ""}, clear=False)
    def test_pf_401_seed_not_set(self):
        os.environ.pop("U2_MASTER_SEED", None)
        result = pf_401_seed_format(self.config)
        self.assertEqual(result.status, CheckStatus.FAIL)

    def test_pf_401_seed_wrong_length(self):
        os.environ["U2_MASTER_SEED"] = "abc"
        result = pf_401_seed_format(self.config)
        self.assertEqual(result.status, CheckStatus.FAIL)
        self.assertIn("64 hex", result.message)

    def test_pf_401_seed_valid(self):
        os.environ["U2_MASTER_SEED"] = "a" * 64
        result = pf_401_seed_format(self.config)
        self.assertEqual(result.status, CheckStatus.PASS)

    def test_pf_405_no_rfl_dir(self):
        # If backend/rfl doesn't exist, should warn
        result = pf_405_no_banned_randomness(self.config)
        self.assertIn(result.status, [CheckStatus.PASS, CheckStatus.WARN])


class TestStage5BudgetLimits(unittest.TestCase):
    """Tests for Stage 5: Budget & Resource Limits checks."""

    def setUp(self):
        self.config = PreflightConfig(
            run_dir=Path(tempfile.mkdtemp()) / "run",
            run_id="test-run",
        )

    @patch.dict(os.environ, {"U2_CYCLE_LIMIT": ""}, clear=False)
    def test_pf_501_cycle_limit_not_set(self):
        os.environ.pop("U2_CYCLE_LIMIT", None)
        result = pf_501_cycle_limit_valid(self.config)
        self.assertEqual(result.status, CheckStatus.FAIL)

    def test_pf_501_cycle_limit_too_high(self):
        os.environ["U2_CYCLE_LIMIT"] = "999999"
        result = pf_501_cycle_limit_valid(self.config)
        self.assertEqual(result.status, CheckStatus.FAIL)

    def test_pf_501_cycle_limit_valid(self):
        os.environ["U2_CYCLE_LIMIT"] = "1000"
        result = pf_501_cycle_limit_valid(self.config)
        self.assertEqual(result.status, CheckStatus.PASS)

    def test_pf_502_interval_exceeds_limit(self):
        os.environ["U2_CYCLE_LIMIT"] = "100"
        os.environ["U2_SNAPSHOT_INTERVAL"] = "200"
        result = pf_502_snapshot_interval(self.config)
        self.assertEqual(result.status, CheckStatus.FAIL)

    def test_pf_502_interval_valid(self):
        os.environ["U2_CYCLE_LIMIT"] = "1000"
        os.environ["U2_SNAPSHOT_INTERVAL"] = "100"
        result = pf_502_snapshot_interval(self.config)
        self.assertEqual(result.status, CheckStatus.PASS)


class TestStage6Isolation(unittest.TestCase):
    """Tests for Stage 6: Isolation Verification checks."""

    def setUp(self):
        self.temp_dir = tempfile.mkdtemp()
        self.config = PreflightConfig(
            run_dir=Path(self.temp_dir) / "run",
            run_id="test-run",
        )

    def tearDown(self):
        import shutil
        shutil.rmtree(self.temp_dir, ignore_errors=True)

    def test_pf_601_run_isolation_valid(self):
        os.environ["MATHLEDGER_CACHE_ROOT"] = self.temp_dir
        result = pf_601_run_isolation(self.config)
        self.assertEqual(result.status, CheckStatus.PASS)


class TestStage7Infrastructure(unittest.TestCase):
    """Tests for Stage 7: Infrastructure Connectivity checks."""

    def setUp(self):
        self.config = PreflightConfig(
            run_dir=Path(tempfile.mkdtemp()) / "run",
            run_id="test-run",
        )

    @patch.dict(os.environ, {"RFL_ENV_MODE": "phase1-hermetic"})
    def test_pf_701_skip_non_experiment_mode(self):
        result = pf_701_db_connectivity(self.config)
        self.assertEqual(result.status, CheckStatus.SKIP)

    @patch.dict(os.environ, {"RFL_ENV_MODE": "uplift_experiment", "DATABASE_URL": ""})
    def test_pf_701_db_url_not_set(self):
        os.environ.pop("DATABASE_URL", None)
        result = pf_701_db_connectivity(self.config)
        self.assertEqual(result.status, CheckStatus.FAIL)


class TestStage8NFRCompliance(unittest.TestCase):
    """Tests for Stage 8: NFR Compliance checks."""

    def setUp(self):
        self.config = PreflightConfig(
            run_dir=Path(tempfile.mkdtemp()) / "run",
            run_id="test-run",
        )

    def test_pf_801_nfr001_pass(self):
        os.environ["RFL_ENV_MODE"] = "uplift_experiment"
        result = pf_801_nfr001(self.config)
        self.assertEqual(result.status, CheckStatus.PASS)

    @patch.dict(os.environ, {"RFL_ENV_MODE": ""}, clear=False)
    def test_pf_801_nfr001_fail(self):
        os.environ.pop("RFL_ENV_MODE", None)
        result = pf_801_nfr001(self.config)
        self.assertEqual(result.status, CheckStatus.FAIL)


class TestFullBundleExecution(unittest.TestCase):
    """Tests for full pre-flight bundle execution."""

    def setUp(self):
        self.temp_dir = tempfile.mkdtemp()

        # Create prereg file
        self.prereg_file = Path(self.temp_dir) / "PREREG_UPLIFT_U2.yaml"
        self.prereg_file.write_text("experiments:\n  - id: U2_EXP_001\n")
        self.prereg_hash = bundle.hashlib.sha256(self.prereg_file.read_bytes()).hexdigest()

        # Create required directories
        cache_dir = Path(self.temp_dir) / "cache"
        snapshot_dir = Path(self.temp_dir) / "snapshots"
        export_dir = Path(self.temp_dir) / "exports"
        cache_dir.mkdir()
        snapshot_dir.mkdir()
        export_dir.mkdir()

        self.valid_env = {
            "RFL_ENV_MODE": "uplift_experiment",
            "U2_RUN_ID": "test-run-id",
            "U2_MASTER_SEED": self.prereg_hash,
            "U2_CYCLE_LIMIT": "1000",
            "U2_SNAPSHOT_INTERVAL": "100",
            "MATHLEDGER_CACHE_ROOT": str(cache_dir),
            "MATHLEDGER_SNAPSHOT_ROOT": str(snapshot_dir),
            "MATHLEDGER_EXPORT_ROOT": str(export_dir),
            "PYTHONHASHSEED": "0",
        }

        self.config = PreflightConfig(
            run_dir=cache_dir / "u2" / "test-run-id",
            run_id="test-run-id",
            prereg_file=self.prereg_file,
            operator_id="test@operator.io",
            confirm_phase2=True,
            dry_run=True,  # Don't create actual files
        )

    def tearDown(self):
        import shutil
        shutil.rmtree(self.temp_dir, ignore_errors=True)

    @patch.dict(os.environ, {}, clear=True)
    def test_bundle_pass_scenario(self):
        """Test full bundle execution with valid configuration."""
        os.environ.update(self.valid_env)

        stage_results, all_checks = run_preflight_bundle(self.config)

        # Should have 10 stages
        self.assertEqual(len(stage_results), 10)

        # Count failures
        fail_count = sum(1 for c in all_checks if c.status == CheckStatus.FAIL)

        # In dry run mode with valid config, most checks should pass
        # Some may fail due to missing DB/Redis, but core checks should pass
        report = generate_bundle_report(self.config, stage_results, all_checks)

        # Report should be generated
        self.assertIn("audit_eligible", report)
        self.assertIn("summary", report)
        self.assertEqual(report["summary"]["total_stages"], 10)

    @patch.dict(os.environ, {}, clear=True)
    def test_bundle_fail_no_env_mode(self):
        """Test bundle fails when RFL_ENV_MODE is not set."""
        env = self.valid_env.copy()
        del env["RFL_ENV_MODE"]
        os.environ.update(env)

        stage_results, all_checks = run_preflight_bundle(self.config)

        # PF-101 should fail
        pf_101 = next((c for c in all_checks if c.id == "PF-101"), None)
        self.assertIsNotNone(pf_101)
        self.assertEqual(pf_101.status, CheckStatus.FAIL)

        # Report should show NOT_ELIGIBLE
        report = generate_bundle_report(self.config, stage_results, all_checks)
        self.assertEqual(report["audit_eligible"]["status"], "NOT_ELIGIBLE")
        self.assertIn("PF-101", report["audit_eligible"]["reasons"])

    @patch.dict(os.environ, {}, clear=True)
    def test_bundle_fail_invalid_seed(self):
        """Test bundle fails with invalid seed format."""
        env = self.valid_env.copy()
        env["U2_MASTER_SEED"] = "invalid-seed"
        os.environ.update(env)

        stage_results, all_checks = run_preflight_bundle(self.config)

        # PF-401 should fail
        pf_401 = next((c for c in all_checks if c.id == "PF-401"), None)
        self.assertIsNotNone(pf_401)
        self.assertEqual(pf_401.status, CheckStatus.FAIL)

    @patch.dict(os.environ, {}, clear=True)
    def test_bundle_fail_no_operator(self):
        """Test bundle fails without operator ID."""
        os.environ.update(self.valid_env)

        config = PreflightConfig(
            run_dir=self.config.run_dir,
            run_id=self.config.run_id,
            operator_id=None,  # Missing!
            confirm_phase2=True,
            dry_run=True,
        )

        stage_results, all_checks = run_preflight_bundle(config)

        # PF-104 should fail
        pf_104 = next((c for c in all_checks if c.id == "PF-104"), None)
        self.assertIsNotNone(pf_104)
        self.assertEqual(pf_104.status, CheckStatus.FAIL)


class TestManifestFlagging(unittest.TestCase):
    """Tests for proper manifest flagging."""

    def test_audit_eligible_flag(self):
        """Test that AUDIT_ELIGIBLE is only set when all critical checks pass."""
        all_pass_checks = [
            CheckResult("PF-101", 1, CheckStatus.PASS, "ok"),
            CheckResult("PF-102", 1, CheckStatus.PASS, "ok"),
            CheckResult("PF-201", 2, CheckStatus.PASS, "ok"),
        ]

        config = PreflightConfig(
            run_dir=Path("/tmp/test"),
            run_id="test",
        )

        stage_results = [
            StageResult(1, "TEST", all_pass_checks[:2], True),
            StageResult(2, "TEST2", all_pass_checks[2:], True),
        ]

        report = generate_bundle_report(config, stage_results, all_pass_checks)
        self.assertEqual(report["audit_eligible"]["status"], "AUDIT_ELIGIBLE")
        self.assertEqual(report["audit_eligible"]["reasons"], [])

    def test_not_eligible_flag_on_failure(self):
        """Test that NOT_ELIGIBLE is set when any critical check fails."""
        checks = [
            CheckResult("PF-101", 1, CheckStatus.PASS, "ok"),
            CheckResult("PF-102", 1, CheckStatus.FAIL, "mode invalid"),
            CheckResult("PF-201", 2, CheckStatus.PASS, "ok"),
        ]

        config = PreflightConfig(
            run_dir=Path("/tmp/test"),
            run_id="test",
        )

        stage_results = [
            StageResult(1, "TEST", checks[:2], False),
            StageResult(2, "TEST2", checks[2:], True),
        ]

        report = generate_bundle_report(config, stage_results, checks)
        self.assertEqual(report["audit_eligible"]["status"], "NOT_ELIGIBLE")
        self.assertIn("PF-102", report["audit_eligible"]["reasons"])

    def test_eligible_with_warnings(self):
        """Test that AUDIT_ELIGIBLE can be set even with warnings."""
        checks = [
            CheckResult("PF-101", 1, CheckStatus.PASS, "ok"),
            CheckResult("PF-406", 4, CheckStatus.WARN, "hash seed not set"),
        ]

        config = PreflightConfig(
            run_dir=Path("/tmp/test"),
            run_id="test",
        )

        stage_results = [
            StageResult(1, "TEST", checks, True),
        ]

        report = generate_bundle_report(config, stage_results, checks)
        self.assertEqual(report["audit_eligible"]["status"], "AUDIT_ELIGIBLE")
        self.assertEqual(report["summary"]["checks_warnings"], 1)


class TestCLIExitCodes(unittest.TestCase):
    """Tests for CLI exit codes."""

    def setUp(self):
        self.temp_dir = tempfile.mkdtemp()

    def tearDown(self):
        import shutil
        shutil.rmtree(self.temp_dir, ignore_errors=True)

    def run_cli(self, args):
        """Run CLI and return exit code."""
        with patch.object(sys, "argv", ["u2_preflight_bundle.py"] + args):
            try:
                bundle.main()
            except SystemExit as e:
                return e.code
        return -1

    @patch.dict(os.environ, {"U2_RUN_ID": "test-run", "MATHLEDGER_CACHE_ROOT": ""})
    def test_exit_code_2_invalid_args(self):
        """Test exit code 2 for invalid arguments."""
        os.environ.pop("MATHLEDGER_CACHE_ROOT", None)
        exit_code = self.run_cli([])
        self.assertEqual(exit_code, 2)

    @patch.dict(os.environ, {
        "RFL_ENV_MODE": "uplift_experiment",
        "U2_RUN_ID": "test-run",
        "U2_MASTER_SEED": "a" * 64,
        "U2_CYCLE_LIMIT": "1000",
        "U2_SNAPSHOT_INTERVAL": "100",
        "PYTHONHASHSEED": "0",
    })
    def test_dry_run_option(self):
        """Test that --dry-run doesn't create files."""
        cache_dir = Path(self.temp_dir) / "cache"
        snapshot_dir = Path(self.temp_dir) / "snapshots"
        export_dir = Path(self.temp_dir) / "exports"
        cache_dir.mkdir()
        snapshot_dir.mkdir()
        export_dir.mkdir()

        os.environ["MATHLEDGER_CACHE_ROOT"] = str(cache_dir)
        os.environ["MATHLEDGER_SNAPSHOT_ROOT"] = str(snapshot_dir)
        os.environ["MATHLEDGER_EXPORT_ROOT"] = str(export_dir)

        exit_code = self.run_cli([
            "--dry-run",
            "--operator-id", "test@op.io",
            "--confirm-phase2",
            "--json"
        ])

        # May fail due to missing prereg or DB, but should not be exit code 2
        self.assertNotEqual(exit_code, 2)


class TestStageOrdering(unittest.TestCase):
    """Tests to ensure stages execute in correct order."""

    def test_stage_names_complete(self):
        """Verify all 10 stages are defined."""
        from u2_preflight_bundle import STAGE_NAMES
        self.assertEqual(len(STAGE_NAMES), 10)
        self.assertEqual(list(STAGE_NAMES.keys()), list(range(1, 11)))

    def test_check_ids_follow_convention(self):
        """Verify check IDs follow PF-XXX convention."""
        # Get all check functions from module
        import inspect
        check_fns = [
            name for name, obj in inspect.getmembers(bundle)
            if name.startswith("pf_") and callable(obj)
        ]

        # All should follow naming pattern pf_XXX_description
        for fn_name in check_fns:
            parts = fn_name.split("_")
            self.assertGreaterEqual(len(parts), 2)
            # Second part should be numeric or like "101", "1001", etc.
            self.assertTrue(parts[1].isdigit() or parts[1] in ["nfr001", "nfr002", "nfr003", "nfr004", "nfr005", "nfr006", "nfr007"])


# =============================================================================
# V2 ENHANCEMENT TESTS
# =============================================================================

from u2_preflight_bundle import (
    build_preflight_bundle_snapshot,
    build_preflight_bundle_timeline,
    summarize_preflight_for_admissibility,
    load_snapshots_from_directory,
    PREFLIGHT_SNAPSHOT_SCHEMA_VERSION,
    PREFLIGHT_TIMELINE_SCHEMA_VERSION,
    ADMISSIBILITY_SUMMARY_SCHEMA_VERSION,
)


class TestBuildPreflightBundleSnapshot(unittest.TestCase):
    """Tests for build_preflight_bundle_snapshot (TASK 1)."""

    def test_snapshot_from_eligible_report(self):
        """Test snapshot creation from an eligible report."""
        report = {
            "run_id": "run-001",
            "generated_at": "2025-12-06T10:00:00Z",
            "audit_eligible": {
                "status": "AUDIT_ELIGIBLE",
                "reasons": [],
            },
            "summary": {
                "total_checks": 47,
                "checks_passed": 47,
                "checks_failed": 0,
                "checks_warnings": 0,
            },
            "config": {
                "operator_id": "operator@test.io",
            },
        }

        snapshot = build_preflight_bundle_snapshot(report)

        self.assertEqual(snapshot["schema_version"], PREFLIGHT_SNAPSHOT_SCHEMA_VERSION)
        self.assertEqual(snapshot["run_id"], "run-001")
        self.assertEqual(snapshot["eligibility"], "AUDIT_ELIGIBLE")
        self.assertEqual(snapshot["failed_pf_check_ids"], [])
        self.assertTrue(snapshot["phase_confirmed"])  # Has operator_id
        self.assertEqual(snapshot["summary"]["total_checks"], 47)
        self.assertEqual(snapshot["timestamp"], "2025-12-06T10:00:00Z")

    def test_snapshot_from_ineligible_report(self):
        """Test snapshot creation from an ineligible report."""
        report = {
            "run_id": "run-002",
            "generated_at": "2025-12-06T11:00:00Z",
            "audit_eligible": {
                "status": "NOT_ELIGIBLE",
                "reasons": ["PF-101", "PF-401"],
            },
            "summary": {
                "total_checks": 47,
                "checks_passed": 40,
                "checks_failed": 2,
                "checks_warnings": 5,
            },
            "config": {
                "operator_id": None,
            },
        }

        snapshot = build_preflight_bundle_snapshot(report)

        self.assertEqual(snapshot["eligibility"], "NOT_ELIGIBLE")
        self.assertEqual(snapshot["failed_pf_check_ids"], ["PF-101", "PF-401"])
        self.assertFalse(snapshot["phase_confirmed"])  # No operator_id
        self.assertEqual(snapshot["summary"]["checks_failed"], 2)

    def test_snapshot_has_required_fields(self):
        """Test snapshot contains all required fields."""
        report = {
            "run_id": "run-003",
            "audit_eligible": {"status": "AUDIT_ELIGIBLE", "reasons": []},
            "summary": {},
            "config": {},
        }

        snapshot = build_preflight_bundle_snapshot(report)

        required_fields = [
            "schema_version", "run_id", "eligibility",
            "failed_pf_check_ids", "phase_confirmed", "summary", "timestamp"
        ]
        for field in required_fields:
            self.assertIn(field, snapshot)


class TestBuildPreflightBundleTimeline(unittest.TestCase):
    """Tests for build_preflight_bundle_timeline (TASK 2)."""

    def test_empty_timeline(self):
        """Test timeline with no snapshots."""
        timeline = build_preflight_bundle_timeline([])

        self.assertIn("schema_version", timeline)
        self.assertEqual(timeline["total_runs"], 0)
        self.assertEqual(timeline["eligible_run_count"], 0)
        self.assertEqual(timeline["ineligible_run_count"], 0)
        self.assertEqual(timeline["ranked_failed_checks"], [])
        self.assertEqual(timeline["eligibility_rate"], 0.0)
        self.assertEqual(timeline["runs"], [])

    def test_timeline_with_all_eligible(self):
        """Test timeline where all runs are eligible."""
        snapshots = [
            {
                "run_id": "run-001",
                "eligibility": "AUDIT_ELIGIBLE",
                "failed_pf_check_ids": [],
                "timestamp": "2025-12-06T10:00:00Z",
            },
            {
                "run_id": "run-002",
                "eligibility": "AUDIT_ELIGIBLE",
                "failed_pf_check_ids": [],
                "timestamp": "2025-12-06T11:00:00Z",
            },
        ]

        timeline = build_preflight_bundle_timeline(snapshots)

        self.assertEqual(timeline["total_runs"], 2)
        self.assertEqual(timeline["eligible_run_count"], 2)
        self.assertEqual(timeline["ineligible_run_count"], 0)
        self.assertEqual(timeline["eligibility_rate"], 100.0)

    def test_timeline_with_mixed_eligibility(self):
        """Test timeline with mixed eligible/ineligible runs."""
        snapshots = [
            {
                "run_id": "run-001",
                "eligibility": "AUDIT_ELIGIBLE",
                "failed_pf_check_ids": [],
                "timestamp": "2025-12-06T10:00:00Z",
            },
            {
                "run_id": "run-002",
                "eligibility": "NOT_ELIGIBLE",
                "failed_pf_check_ids": ["PF-101", "PF-401"],
                "timestamp": "2025-12-06T11:00:00Z",
            },
            {
                "run_id": "run-003",
                "eligibility": "AUDIT_ELIGIBLE",
                "failed_pf_check_ids": [],
                "timestamp": "2025-12-06T12:00:00Z",
            },
            {
                "run_id": "run-004",
                "eligibility": "NOT_ELIGIBLE",
                "failed_pf_check_ids": ["PF-101"],
                "timestamp": "2025-12-06T13:00:00Z",
            },
        ]

        timeline = build_preflight_bundle_timeline(snapshots)

        self.assertEqual(timeline["total_runs"], 4)
        self.assertEqual(timeline["eligible_run_count"], 2)
        self.assertEqual(timeline["ineligible_run_count"], 2)
        self.assertEqual(timeline["eligibility_rate"], 50.0)

    def test_timeline_failed_checks_frequency(self):
        """Test that failed check frequencies are correctly aggregated."""
        snapshots = [
            {"run_id": "r1", "eligibility": "NOT_ELIGIBLE", "failed_pf_check_ids": ["PF-101", "PF-401"], "timestamp": "t1"},
            {"run_id": "r2", "eligibility": "NOT_ELIGIBLE", "failed_pf_check_ids": ["PF-101"], "timestamp": "t2"},
            {"run_id": "r3", "eligibility": "NOT_ELIGIBLE", "failed_pf_check_ids": ["PF-401", "PF-501"], "timestamp": "t3"},
        ]

        timeline = build_preflight_bundle_timeline(snapshots)

        # ranked_failed_checks is a list of tuples (check_id, count) sorted by count desc
        ranked = {check_id: count for check_id, count in timeline["ranked_failed_checks"]}
        self.assertEqual(ranked["PF-101"], 2)
        self.assertEqual(ranked["PF-401"], 2)
        self.assertEqual(ranked["PF-501"], 1)

    def test_timeline_sorted_by_timestamp(self):
        """Test that runs are sorted by timestamp."""
        snapshots = [
            {"run_id": "r3", "eligibility": "AUDIT_ELIGIBLE", "failed_pf_check_ids": [], "timestamp": "2025-12-06T12:00:00Z"},
            {"run_id": "r1", "eligibility": "AUDIT_ELIGIBLE", "failed_pf_check_ids": [], "timestamp": "2025-12-06T10:00:00Z"},
            {"run_id": "r2", "eligibility": "AUDIT_ELIGIBLE", "failed_pf_check_ids": [], "timestamp": "2025-12-06T11:00:00Z"},
        ]

        timeline = build_preflight_bundle_timeline(snapshots)

        run_ids = [r["run_id"] for r in timeline["runs"]]
        self.assertEqual(run_ids, ["r1", "r2", "r3"])

    def test_timeline_custom_id(self):
        """Test timeline with custom ID."""
        timeline = build_preflight_bundle_timeline([], timeline_id="my-custom-timeline")
        self.assertEqual(timeline["timeline_id"], "my-custom-timeline")


class TestSummarizePreflightForAdmissibility(unittest.TestCase):
    """Tests for summarize_preflight_for_admissibility (TASK 3)."""

    def test_admissible_verdict(self):
        """Test ADMISSIBLE verdict when eligible with phase confirmation."""
        snapshot = {
            "eligibility": "AUDIT_ELIGIBLE",
            "phase_confirmed": True,
            "failed_pf_check_ids": [],
            "run_id": "run-001",
            "timestamp": "2025-12-06T10:00:00Z",
        }

        summary = summarize_preflight_for_admissibility(snapshot)

        self.assertEqual(summary["schema_version"], ADMISSIBILITY_SUMMARY_SCHEMA_VERSION)
        self.assertTrue(summary["is_audit_eligible"])
        self.assertTrue(summary["has_phase2_confirmation"])
        self.assertEqual(summary["critical_pf_failures"], [])
        self.assertEqual(summary["admissibility_verdict"], "ADMISSIBLE")
        self.assertIn("passed", summary["verdict_reason"])

    def test_provisional_verdict(self):
        """Test PROVISIONAL verdict when eligible but no phase confirmation."""
        snapshot = {
            "eligibility": "AUDIT_ELIGIBLE",
            "phase_confirmed": False,
            "failed_pf_check_ids": [],
            "run_id": "run-002",
            "timestamp": "2025-12-06T11:00:00Z",
        }

        summary = summarize_preflight_for_admissibility(snapshot)

        self.assertTrue(summary["is_audit_eligible"])
        self.assertFalse(summary["has_phase2_confirmation"])
        self.assertEqual(summary["admissibility_verdict"], "PROVISIONAL")
        self.assertIn("missing", summary["verdict_reason"])

    def test_inadmissible_verdict_with_failures(self):
        """Test INADMISSIBLE verdict when checks failed."""
        snapshot = {
            "eligibility": "NOT_ELIGIBLE",
            "phase_confirmed": True,
            "failed_pf_check_ids": ["PF-101", "PF-401", "PF-501"],
            "run_id": "run-003",
            "timestamp": "2025-12-06T12:00:00Z",
        }

        summary = summarize_preflight_for_admissibility(snapshot)

        self.assertFalse(summary["is_audit_eligible"])
        self.assertEqual(summary["admissibility_verdict"], "INADMISSIBLE")
        self.assertIn("PF-101", summary["verdict_reason"])
        self.assertEqual(summary["critical_pf_failures"], ["PF-101", "PF-401", "PF-501"])

    def test_inadmissible_verdict_many_failures(self):
        """Test INADMISSIBLE verdict truncates failure list in reason."""
        snapshot = {
            "eligibility": "NOT_ELIGIBLE",
            "phase_confirmed": False,
            "failed_pf_check_ids": ["PF-101", "PF-102", "PF-103", "PF-104", "PF-105", "PF-106", "PF-107"],
            "run_id": "run-004",
            "timestamp": "2025-12-06T13:00:00Z",
        }

        summary = summarize_preflight_for_admissibility(snapshot)

        self.assertEqual(summary["admissibility_verdict"], "INADMISSIBLE")
        self.assertIn("+2 more", summary["verdict_reason"])

    def test_summary_has_required_fields(self):
        """Test summary contains all required fields."""
        snapshot = {
            "eligibility": "AUDIT_ELIGIBLE",
            "phase_confirmed": True,
            "failed_pf_check_ids": [],
            "run_id": "run-005",
            "timestamp": "2025-12-06T14:00:00Z",
        }

        summary = summarize_preflight_for_admissibility(snapshot)

        required_fields = [
            "schema_version", "is_audit_eligible", "has_phase2_confirmation",
            "critical_pf_failures", "run_id", "timestamp",
            "admissibility_verdict", "verdict_reason"
        ]
        for field in required_fields:
            self.assertIn(field, summary)


class TestLoadSnapshotsFromDirectory(unittest.TestCase):
    """Tests for load_snapshots_from_directory helper."""

    def setUp(self):
        self.temp_dir = tempfile.mkdtemp()

    def tearDown(self):
        import shutil
        shutil.rmtree(self.temp_dir, ignore_errors=True)

    def test_empty_directory(self):
        """Test loading from empty directory."""
        snapshots = load_snapshots_from_directory(Path(self.temp_dir))
        self.assertEqual(snapshots, [])

    def test_nonexistent_directory(self):
        """Test loading from nonexistent directory."""
        snapshots = load_snapshots_from_directory(Path("/nonexistent/path"))
        self.assertEqual(snapshots, [])

    def test_load_bundle_reports(self):
        """Test loading and converting bundle reports to snapshots."""
        # Create a run directory with a bundle report
        run_dir = Path(self.temp_dir) / "run-001"
        run_dir.mkdir()

        report = {
            "run_id": "run-001",
            "generated_at": "2025-12-06T10:00:00Z",
            "audit_eligible": {"status": "AUDIT_ELIGIBLE", "reasons": []},
            "summary": {"total_checks": 47, "checks_passed": 47, "checks_failed": 0, "checks_warnings": 0},
            "config": {"operator_id": "test@op.io"},
        }
        (run_dir / "preflight_bundle_report.json").write_text(json.dumps(report))

        snapshots = load_snapshots_from_directory(Path(self.temp_dir))

        self.assertEqual(len(snapshots), 1)
        self.assertEqual(snapshots[0]["run_id"], "run-001")
        self.assertEqual(snapshots[0]["eligibility"], "AUDIT_ELIGIBLE")

    def test_load_existing_snapshots(self):
        """Test loading pre-computed snapshot files."""
        snapshot = {
            "schema_version": "1.0.0",
            "run_id": "run-002",
            "eligibility": "NOT_ELIGIBLE",
            "failed_pf_check_ids": ["PF-101"],
            "phase_confirmed": False,
            "summary": {},
            "timestamp": "2025-12-06T11:00:00Z",
        }
        (Path(self.temp_dir) / "run-002_preflight_snapshot.json").write_text(json.dumps(snapshot))

        snapshots = load_snapshots_from_directory(Path(self.temp_dir))

        self.assertEqual(len(snapshots), 1)
        self.assertEqual(snapshots[0]["run_id"], "run-002")

    def test_ignore_invalid_json(self):
        """Test that invalid JSON files are skipped."""
        (Path(self.temp_dir) / "invalid_preflight_snapshot.json").write_text("not valid json {")

        snapshots = load_snapshots_from_directory(Path(self.temp_dir))
        self.assertEqual(snapshots, [])


class TestV2CLIOptions(unittest.TestCase):
    """Tests for v2 CLI options (--snapshot, --admissibility, --timeline-from)."""

    def setUp(self):
        self.temp_dir = tempfile.mkdtemp()

    def tearDown(self):
        import shutil
        shutil.rmtree(self.temp_dir, ignore_errors=True)

    def run_cli(self, args):
        """Run CLI and return exit code."""
        with patch.object(sys, "argv", ["u2_preflight_bundle.py"] + args):
            try:
                bundle.main()
            except SystemExit as e:
                return e.code
        return -1

    def test_timeline_from_empty_dir(self):
        """Test --timeline-from with empty directory exits with code 2."""
        exit_code = self.run_cli(["--timeline-from", self.temp_dir])
        self.assertEqual(exit_code, 2)  # No runs found

    @patch("builtins.print")
    def test_timeline_from_with_reports(self, mock_print):
        """Test --timeline-from generates timeline JSON."""
        # Create a run directory with a bundle report
        run_dir = Path(self.temp_dir) / "run-001"
        run_dir.mkdir()

        report = {
            "run_id": "run-001",
            "generated_at": "2025-12-06T10:00:00Z",
            "audit_eligible": {"status": "AUDIT_ELIGIBLE", "reasons": []},
            "summary": {"total_checks": 47, "checks_passed": 47, "checks_failed": 0, "checks_warnings": 0},
            "config": {"operator_id": "test@op.io"},
        }
        (run_dir / "preflight_bundle_report.json").write_text(json.dumps(report))

        exit_code = self.run_cli(["--timeline-from", self.temp_dir])

        self.assertEqual(exit_code, 0)
        # Should have printed timeline JSON
        mock_print.assert_called()
        output = mock_print.call_args[0][0]
        timeline = json.loads(output)
        self.assertEqual(timeline["total_runs"], 1)


# =============================================================================
# PHASE III TESTS: CROSS-RUN BUNDLE TIMELINE & DIRECTOR SIGNAL
# =============================================================================

from u2_preflight_bundle import (
    build_bundle_evolution_ledger,
    map_bundle_to_director_status,
    summarize_bundle_for_global_health,
    DirectorStatus,
    StabilityRating,
    EVOLUTION_LEDGER_SCHEMA_VERSION,
    DIRECTOR_STATUS_SCHEMA_VERSION,
    GLOBAL_HEALTH_SCHEMA_VERSION,
)


class TestBuildBundleEvolutionLedger(unittest.TestCase):
    """Tests for build_bundle_evolution_ledger (TASK 1)."""

    def test_empty_timeline(self):
        """Test ledger from empty timeline."""
        timeline = {
            "total_runs": 0,
            "runs": [],
            "failed_checks_frequency": {},
        }

        ledger = build_bundle_evolution_ledger(timeline)

        self.assertEqual(ledger["schema_version"], EVOLUTION_LEDGER_SCHEMA_VERSION)
        self.assertEqual(ledger["eligibility_curve"], [])
        self.assertEqual(ledger["frequent_blockers"], [])
        self.assertEqual(ledger["stability_rating"], StabilityRating.STABLE.value)
        self.assertEqual(ledger["stability_score"], 100.0)

    def test_all_eligible_runs(self):
        """Test ledger with all eligible runs."""
        timeline = {
            "total_runs": 5,
            "runs": [
                {"run_id": f"r{i}", "eligibility": "AUDIT_ELIGIBLE", "timestamp": f"t{i}"}
                for i in range(5)
            ],
            "failed_checks_frequency": {},
        }

        ledger = build_bundle_evolution_ledger(timeline)

        self.assertEqual(len(ledger["eligibility_curve"]), 5)
        self.assertTrue(all(e["eligible"] for e in ledger["eligibility_curve"]))
        self.assertEqual(ledger["eligibility_curve"][-1]["cumulative_rate"], 100.0)
        self.assertEqual(ledger["stability_rating"], StabilityRating.STABLE.value)

    def test_eligibility_curve_calculation(self):
        """Test cumulative rate calculation in eligibility curve."""
        timeline = {
            "total_runs": 4,
            "runs": [
                {"run_id": "r1", "eligibility": "AUDIT_ELIGIBLE", "timestamp": "t1"},
                {"run_id": "r2", "eligibility": "NOT_ELIGIBLE", "timestamp": "t2"},
                {"run_id": "r3", "eligibility": "AUDIT_ELIGIBLE", "timestamp": "t3"},
                {"run_id": "r4", "eligibility": "AUDIT_ELIGIBLE", "timestamp": "t4"},
            ],
            "failed_checks_frequency": {"PF-101": 1},
        }

        ledger = build_bundle_evolution_ledger(timeline)

        # r1: 1/1 = 100%, r2: 1/2 = 50%, r3: 2/3 = 66.67%, r4: 3/4 = 75%
        self.assertEqual(ledger["eligibility_curve"][0]["cumulative_rate"], 100.0)
        self.assertEqual(ledger["eligibility_curve"][1]["cumulative_rate"], 50.0)
        self.assertEqual(ledger["eligibility_curve"][2]["cumulative_rate"], 66.67)
        self.assertEqual(ledger["eligibility_curve"][3]["cumulative_rate"], 75.0)

    def test_frequent_blockers_sorted(self):
        """Test that frequent blockers are sorted by frequency."""
        timeline = {
            "total_runs": 10,
            "runs": [{"run_id": f"r{i}", "eligibility": "NOT_ELIGIBLE", "timestamp": f"t{i}"} for i in range(10)],
            "failed_checks_frequency": {
                "PF-101": 5,
                "PF-401": 8,
                "PF-301": 2,
            },
        }

        ledger = build_bundle_evolution_ledger(timeline)

        self.assertEqual(ledger["frequent_blockers"][0]["check_id"], "PF-401")
        self.assertEqual(ledger["frequent_blockers"][0]["failure_count"], 8)
        self.assertEqual(ledger["frequent_blockers"][1]["check_id"], "PF-101")
        self.assertEqual(ledger["frequent_blockers"][2]["check_id"], "PF-301")

    def test_window_analysis(self):
        """Test window-based analysis."""
        # 10 runs: first 5 fail, last 5 pass
        runs = [
            {"run_id": f"r{i}", "eligibility": "NOT_ELIGIBLE", "timestamp": f"t{i}"} for i in range(5)
        ] + [
            {"run_id": f"r{i}", "eligibility": "AUDIT_ELIGIBLE", "timestamp": f"t{i}"} for i in range(5, 10)
        ]

        timeline = {
            "total_runs": 10,
            "runs": runs,
            "failed_checks_frequency": {"PF-101": 5},
        }

        ledger = build_bundle_evolution_ledger(timeline)

        # Recent 5 should be 100% eligible
        self.assertEqual(ledger["window_analysis"]["recent_5_rate"], 100.0)
        # Recent 10 should be 50% eligible
        self.assertEqual(ledger["window_analysis"]["recent_10_rate"], 50.0)

    def test_stability_improving(self):
        """Test IMPROVING stability rating."""
        # Start with failures, end with successes
        runs = [
            {"run_id": f"r{i}", "eligibility": "NOT_ELIGIBLE", "timestamp": f"t{i}"} for i in range(15)
        ] + [
            {"run_id": f"r{i}", "eligibility": "AUDIT_ELIGIBLE", "timestamp": f"t{i}"} for i in range(15, 20)
        ]

        timeline = {
            "total_runs": 20,
            "runs": runs,
            "failed_checks_frequency": {"PF-101": 15},
        }

        ledger = build_bundle_evolution_ledger(timeline)

        # Recent 5 = 100%, Recent 20 = 25%
        self.assertEqual(ledger["stability_rating"], StabilityRating.IMPROVING.value)

    def test_stability_critical(self):
        """Test CRITICAL stability rating."""
        # All recent runs fail
        runs = [
            {"run_id": f"r{i}", "eligibility": "NOT_ELIGIBLE", "timestamp": f"t{i}"} for i in range(10)
        ]

        timeline = {
            "total_runs": 10,
            "runs": runs,
            "failed_checks_frequency": {"PF-101": 10},
        }

        ledger = build_bundle_evolution_ledger(timeline)

        self.assertEqual(ledger["stability_rating"], StabilityRating.CRITICAL.value)


class TestMapBundleToDirectorStatus(unittest.TestCase):
    """Tests for map_bundle_to_director_status (TASK 2)."""

    def test_snapshot_green(self):
        """Test GREEN status for eligible snapshot with phase confirmation."""
        snapshot = {
            "run_id": "run-001",
            "eligibility": "AUDIT_ELIGIBLE",
            "phase_confirmed": True,
            "failed_pf_check_ids": [],
        }

        result = map_bundle_to_director_status(snapshot)

        self.assertEqual(result["schema_version"], DIRECTOR_STATUS_SCHEMA_VERSION)
        self.assertEqual(result["status"], DirectorStatus.GREEN.value)
        self.assertEqual(result["run_id"], "run-001")
        self.assertEqual(result["recommendations"], [])

    def test_snapshot_yellow(self):
        """Test YELLOW status for eligible snapshot without phase confirmation."""
        snapshot = {
            "run_id": "run-002",
            "eligibility": "AUDIT_ELIGIBLE",
            "phase_confirmed": False,
            "failed_pf_check_ids": [],
        }

        result = map_bundle_to_director_status(snapshot)

        self.assertEqual(result["status"], DirectorStatus.YELLOW.value)
        self.assertIn("missing", result["status_reason"].lower())
        self.assertTrue(len(result["recommendations"]) > 0)

    def test_snapshot_red(self):
        """Test RED status for ineligible snapshot."""
        snapshot = {
            "run_id": "run-003",
            "eligibility": "NOT_ELIGIBLE",
            "phase_confirmed": True,
            "failed_pf_check_ids": ["PF-101", "PF-401"],
        }

        result = map_bundle_to_director_status(snapshot)

        self.assertEqual(result["status"], DirectorStatus.RED.value)
        self.assertEqual(result["metrics"]["failed_check_count"], 2)

    def test_timeline_green(self):
        """Test GREEN status for healthy timeline."""
        timeline = {
            "total_runs": 10,
            "eligibility_rate": 100.0,
            "runs": [{"run_id": f"r{i}", "eligibility": "AUDIT_ELIGIBLE"} for i in range(10)],
            "failed_checks_frequency": {},
        }

        result = map_bundle_to_director_status(timeline)

        self.assertEqual(result["status"], DirectorStatus.GREEN.value)
        self.assertIn("100.0%", result["status_reason"])

    def test_timeline_yellow(self):
        """Test YELLOW status for suboptimal timeline."""
        timeline = {
            "total_runs": 10,
            "eligibility_rate": 80.0,  # Below 95% green threshold
            "runs": [
                {"run_id": f"r{i}", "eligibility": "AUDIT_ELIGIBLE" if i < 8 else "NOT_ELIGIBLE"}
                for i in range(10)
            ],
            "failed_checks_frequency": {"PF-101": 2},
        }

        result = map_bundle_to_director_status(timeline)

        self.assertEqual(result["status"], DirectorStatus.YELLOW.value)

    def test_timeline_red_consecutive_failures(self):
        """Test RED status when consecutive failures exceed threshold."""
        timeline = {
            "total_runs": 10,
            "eligibility_rate": 70.0,
            "runs": [
                {"run_id": f"r{i}", "eligibility": "AUDIT_ELIGIBLE" if i < 7 else "NOT_ELIGIBLE"}
                for i in range(10)
            ],
            "failed_checks_frequency": {"PF-101": 3},
        }

        result = map_bundle_to_director_status(timeline)

        self.assertEqual(result["status"], DirectorStatus.RED.value)
        self.assertEqual(result["metrics"]["consecutive_failures"], 3)

    def test_timeline_red_low_rate(self):
        """Test RED status when eligibility rate below threshold."""
        timeline = {
            "total_runs": 10,
            "eligibility_rate": 50.0,  # Below 70% yellow threshold
            "runs": [
                {"run_id": f"r{i}", "eligibility": "AUDIT_ELIGIBLE" if i % 2 == 0 else "NOT_ELIGIBLE"}
                for i in range(10)
            ],
            "failed_checks_frequency": {"PF-101": 5},
        }

        result = map_bundle_to_director_status(timeline)

        self.assertEqual(result["status"], DirectorStatus.RED.value)

    def test_empty_timeline_yellow(self):
        """Test YELLOW status for empty timeline."""
        timeline = {
            "total_runs": 0,
            "runs": [],
            "failed_checks_frequency": {},
        }

        result = map_bundle_to_director_status(timeline)

        self.assertEqual(result["status"], DirectorStatus.YELLOW.value)
        self.assertIn("No runs", result["status_reason"])

    def test_unknown_format_red(self):
        """Test RED status for unknown data format."""
        result = map_bundle_to_director_status({"unknown": "data"})

        self.assertEqual(result["status"], DirectorStatus.RED.value)
        self.assertIn("Unknown", result["status_reason"])

    def test_custom_thresholds(self):
        """Test custom thresholds."""
        timeline = {
            "total_runs": 10,
            "eligibility_rate": 85.0,
            "runs": [{"run_id": f"r{i}", "eligibility": "AUDIT_ELIGIBLE"} for i in range(10)],
            "failed_checks_frequency": {},
        }

        # With default thresholds, 85% would be YELLOW
        result_default = map_bundle_to_director_status(timeline)
        self.assertEqual(result_default["status"], DirectorStatus.YELLOW.value)

        # With custom threshold, 85% can be GREEN
        custom_thresholds = {"green_min_rate": 80.0, "yellow_min_rate": 60.0, "max_consecutive_failures": 3}
        result_custom = map_bundle_to_director_status(timeline, custom_thresholds)
        self.assertEqual(result_custom["status"], DirectorStatus.GREEN.value)


class TestSummarizeBundleForGlobalHealth(unittest.TestCase):
    """Tests for summarize_bundle_for_global_health (TASK 3)."""

    def test_empty_timeline(self):
        """Test GH summary for empty timeline."""
        timeline = {
            "total_runs": 0,
            "eligible_run_count": 0,
            "runs": [],
            "failed_checks_frequency": {},
        }

        summary = summarize_bundle_for_global_health(timeline)

        self.assertEqual(summary["schema_version"], GLOBAL_HEALTH_SCHEMA_VERSION)
        self.assertTrue(summary["bundle_ok"])
        self.assertEqual(summary["historical_failure_rate"], 0.0)
        self.assertEqual(summary["hotspots"], [])
        self.assertFalse(summary["integration_ready"])

    def test_healthy_timeline(self):
        """Test GH summary for healthy timeline."""
        timeline = {
            "total_runs": 20,
            "eligible_run_count": 18,
            "runs": [
                {"run_id": f"r{i}", "eligibility": "AUDIT_ELIGIBLE" if i >= 2 else "NOT_ELIGIBLE"}
                for i in range(20)
            ],
            "failed_checks_frequency": {"PF-101": 2},
        }

        summary = summarize_bundle_for_global_health(timeline)

        self.assertTrue(summary["bundle_ok"])
        self.assertEqual(summary["historical_failure_rate"], 10.0)
        self.assertTrue(summary["integration_ready"])
        self.assertEqual(summary["health_indicators"]["recent_success_streak"], 18)

    def test_hotspots_severity(self):
        """Test hotspot severity calculation."""
        timeline = {
            "total_runs": 10,
            "eligible_run_count": 5,
            "runs": [
                {"run_id": f"r{i}", "eligibility": "AUDIT_ELIGIBLE" if i % 2 == 0 else "NOT_ELIGIBLE"}
                for i in range(10)
            ],
            # Use ranked_failed_checks format (list of tuples) as used by build_preflight_bundle_timeline
            "ranked_failed_checks": [
                ("PF-101", 3),  # 30% = HIGH
                ("PF-401", 1),  # 10% = MEDIUM
            ],
        }

        summary = summarize_bundle_for_global_health(timeline)

        # Hotspots are built from ranked_failed_checks
        self.assertGreaterEqual(len(summary["hotspots"]), 1)
        # PF-101 should be HIGH severity (30% > 20%)
        pf101_hotspot = next((h for h in summary["hotspots"] if h["check_id"] == "PF-101"), None)
        if pf101_hotspot:
            self.assertEqual(pf101_hotspot["severity"], "HIGH")

    def test_bundle_not_ok_with_critical_hotspot(self):
        """Test bundle_ok=False when critical hotspot exists."""
        timeline = {
            "total_runs": 10,
            "eligible_run_count": 9,
            "runs": [
                {"run_id": "r0", "eligibility": "NOT_ELIGIBLE"},
            ] + [
                {"run_id": f"r{i}", "eligibility": "AUDIT_ELIGIBLE"}
                for i in range(1, 10)
            ],
            "failed_checks_frequency": {"PF-101": 3},  # 30% = HIGH even with 1 failure
        }

        # Override the failed_checks_frequency to simulate high severity
        timeline["failed_checks_frequency"] = {"PF-101": 3}  # 3/10 = 30% = HIGH

        summary = summarize_bundle_for_global_health(timeline)

        # streak is 9, so bundle_ok should be True (streak >= 3)
        self.assertTrue(summary["bundle_ok"])

    def test_bundle_ok_with_success_streak(self):
        """Test bundle_ok=True with recent success streak."""
        # Many failures but last 5 all pass
        timeline = {
            "total_runs": 10,
            "eligible_run_count": 5,
            "runs": [
                {"run_id": f"r{i}", "eligibility": "NOT_ELIGIBLE" if i < 5 else "AUDIT_ELIGIBLE"}
                for i in range(10)
            ],
            "failed_checks_frequency": {"PF-101": 5},  # 50% = HIGH severity
        }

        summary = summarize_bundle_for_global_health(timeline)

        self.assertTrue(summary["bundle_ok"])  # streak = 5 >= 3
        self.assertEqual(summary["health_indicators"]["recent_success_streak"], 5)

    def test_integration_ready_requirements(self):
        """Test integration_ready requirements."""
        # Not ready: < 10 runs
        timeline_few_runs = {
            "total_runs": 5,
            "eligible_run_count": 5,
            "runs": [{"run_id": f"r{i}", "eligibility": "AUDIT_ELIGIBLE"} for i in range(5)],
            "failed_checks_frequency": {},
        }
        summary = summarize_bundle_for_global_health(timeline_few_runs)
        self.assertFalse(summary["integration_ready"])

        # Not ready: high failure rate
        timeline_high_failures = {
            "total_runs": 10,
            "eligible_run_count": 4,  # 60% failure rate
            "runs": [
                {"run_id": f"r{i}", "eligibility": "AUDIT_ELIGIBLE" if i < 4 else "NOT_ELIGIBLE"}
                for i in range(10)
            ],
            "failed_checks_frequency": {"PF-101": 6},
        }
        summary = summarize_bundle_for_global_health(timeline_high_failures)
        self.assertFalse(summary["integration_ready"])

        # Ready: 10+ runs and < 50% failure rate
        timeline_ready = {
            "total_runs": 10,
            "eligible_run_count": 6,  # 40% failure rate
            "runs": [
                {"run_id": f"r{i}", "eligibility": "AUDIT_ELIGIBLE" if i < 6 else "NOT_ELIGIBLE"}
                for i in range(10)
            ],
            "failed_checks_frequency": {"PF-101": 4},
        }
        summary = summarize_bundle_for_global_health(timeline_ready)
        self.assertTrue(summary["integration_ready"])

    def test_worst_period_failure_rate(self):
        """Test worst period failure rate calculation."""
        # 20 runs: runs 5-14 all fail (worst period)
        runs = []
        for i in range(20):
            if 5 <= i < 15:
                runs.append({"run_id": f"r{i}", "eligibility": "NOT_ELIGIBLE"})
            else:
                runs.append({"run_id": f"r{i}", "eligibility": "AUDIT_ELIGIBLE"})

        timeline = {
            "total_runs": 20,
            "eligible_run_count": 10,
            "runs": runs,
            "failed_checks_frequency": {"PF-101": 10},
        }

        summary = summarize_bundle_for_global_health(timeline)

        # Window of 10 from index 5-14 should be 100% failures
        self.assertEqual(summary["health_indicators"]["worst_period_failure_rate"], 100.0)


# =============================================================================
# PHASE IV TESTS: BUNDLE EVOLUTION GATE & DIRECTOR INTEGRATION TILE
# =============================================================================

from u2_preflight_bundle import (
    evaluate_bundle_for_integration,
    summarize_bundle_for_maas,
    build_bundle_director_panel,
    IntegrationStatus,
    MAASStatus,
    INTEGRATION_EVAL_SCHEMA_VERSION,
    MAAS_BUNDLE_SCHEMA_VERSION,
    DIRECTOR_PANEL_SCHEMA_VERSION,
)


class TestEvaluateBundleForIntegration(unittest.TestCase):
    """Tests for evaluate_bundle_for_integration (TASK 1)."""

    def _make_evolution_ledger(self, **overrides):
        """Helper to create evolution ledger."""
        defaults = {
            "stability_rating": "STABLE",
            "stability_score": 90.0,
            "window_analysis": {
                "recent_5_rate": 100.0,
                "recent_10_rate": 100.0,
                "recent_20_rate": 100.0,
            },
            "frequent_blockers": [],
            "eligibility_curve": [],
        }
        defaults.update(overrides)
        return defaults

    def _make_global_summary(self, **overrides):
        """Helper to create global summary."""
        defaults = {
            "bundle_ok": True,
            "integration_ready": True,
            "historical_failure_rate": 10.0,
            "hotspots": [],
            "health_indicators": {
                "total_runs_analyzed": 20,
                "recent_success_streak": 10,
            },
        }
        defaults.update(overrides)
        return defaults

    def test_integration_ok_all_good(self):
        """Test OK status when all conditions are met."""
        ledger = self._make_evolution_ledger()
        summary = self._make_global_summary()

        result = evaluate_bundle_for_integration(ledger, summary)

        self.assertEqual(result["schema_version"], INTEGRATION_EVAL_SCHEMA_VERSION)
        self.assertTrue(result["integration_ok"])
        self.assertEqual(result["status"], IntegrationStatus.OK.value)
        self.assertEqual(result["blocking_reasons"], [])
        self.assertEqual(result["warning_reasons"], [])

    def test_integration_blocked_critical_stability(self):
        """Test BLOCK status when stability is CRITICAL."""
        ledger = self._make_evolution_ledger(stability_rating="CRITICAL")
        summary = self._make_global_summary()

        result = evaluate_bundle_for_integration(ledger, summary)

        self.assertFalse(result["integration_ok"])
        self.assertEqual(result["status"], IntegrationStatus.BLOCK.value)
        self.assertIn("CRITICAL", result["blocking_reasons"][0])

    def test_integration_blocked_no_recent_eligible(self):
        """Test BLOCK status when no eligible runs in last 5."""
        ledger = self._make_evolution_ledger(
            window_analysis={"recent_5_rate": 0, "recent_10_rate": 50.0, "recent_20_rate": 75.0}
        )
        summary = self._make_global_summary()

        result = evaluate_bundle_for_integration(ledger, summary)

        self.assertFalse(result["integration_ok"])
        self.assertEqual(result["status"], IntegrationStatus.BLOCK.value)
        self.assertTrue(any("No eligible runs" in r for r in result["blocking_reasons"]))

    def test_integration_blocked_high_failure_rate(self):
        """Test BLOCK status when failure rate too high."""
        ledger = self._make_evolution_ledger()
        summary = self._make_global_summary(historical_failure_rate=55.0)

        result = evaluate_bundle_for_integration(ledger, summary)

        self.assertFalse(result["integration_ok"])
        self.assertEqual(result["status"], IntegrationStatus.BLOCK.value)
        self.assertTrue(any("failure rate" in r.lower() for r in result["blocking_reasons"]))

    def test_integration_blocked_not_integration_ready(self):
        """Test BLOCK status when not integration ready."""
        ledger = self._make_evolution_ledger()
        summary = self._make_global_summary(integration_ready=False)

        result = evaluate_bundle_for_integration(ledger, summary)

        self.assertFalse(result["integration_ok"])
        self.assertEqual(result["status"], IntegrationStatus.BLOCK.value)

    def test_integration_blocked_multiple_high_severity(self):
        """Test BLOCK status with multiple HIGH severity hotspots."""
        ledger = self._make_evolution_ledger()
        summary = self._make_global_summary(
            hotspots=[
                {"check_id": "PF-101", "severity": "HIGH"},
                {"check_id": "PF-401", "severity": "HIGH"},
            ]
        )

        result = evaluate_bundle_for_integration(ledger, summary)

        self.assertFalse(result["integration_ok"])
        self.assertEqual(result["status"], IntegrationStatus.BLOCK.value)
        self.assertTrue(any("HIGH severity" in r for r in result["blocking_reasons"]))

    def test_integration_warn_degrading(self):
        """Test WARN status when stability is DEGRADING."""
        ledger = self._make_evolution_ledger(stability_rating="DEGRADING")
        summary = self._make_global_summary()

        result = evaluate_bundle_for_integration(ledger, summary)

        self.assertTrue(result["integration_ok"])
        self.assertEqual(result["status"], IntegrationStatus.WARN.value)
        self.assertTrue(any("DEGRADING" in r for r in result["warning_reasons"]))

    def test_integration_warn_volatile(self):
        """Test WARN status when stability is VOLATILE."""
        ledger = self._make_evolution_ledger(stability_rating="VOLATILE")
        summary = self._make_global_summary()

        result = evaluate_bundle_for_integration(ledger, summary)

        self.assertTrue(result["integration_ok"])
        self.assertEqual(result["status"], IntegrationStatus.WARN.value)
        self.assertTrue(any("VOLATILE" in r for r in result["warning_reasons"]))

    def test_integration_warn_declining_eligibility(self):
        """Test WARN status when recent eligibility is declining."""
        ledger = self._make_evolution_ledger(
            window_analysis={"recent_5_rate": 70.0, "recent_10_rate": 90.0, "recent_20_rate": 90.0}
        )
        summary = self._make_global_summary()

        result = evaluate_bundle_for_integration(ledger, summary)

        self.assertTrue(result["integration_ok"])
        self.assertEqual(result["status"], IntegrationStatus.WARN.value)
        self.assertTrue(any("declining" in r.lower() for r in result["warning_reasons"]))

    def test_integration_warn_single_high_severity(self):
        """Test WARN status with single HIGH severity hotspot."""
        ledger = self._make_evolution_ledger()
        summary = self._make_global_summary(
            hotspots=[{"check_id": "PF-101", "severity": "HIGH"}]
        )

        result = evaluate_bundle_for_integration(ledger, summary)

        self.assertTrue(result["integration_ok"])
        self.assertEqual(result["status"], IntegrationStatus.WARN.value)
        self.assertTrue(any("PF-101" in r for r in result["warning_reasons"]))

    def test_metrics_included(self):
        """Test that metrics are included in result."""
        ledger = self._make_evolution_ledger()
        summary = self._make_global_summary()

        result = evaluate_bundle_for_integration(ledger, summary)

        self.assertIn("metrics", result)
        self.assertIn("stability_rating", result["metrics"])
        self.assertIn("stability_score", result["metrics"])
        self.assertIn("recent_5_rate", result["metrics"])


class TestSummarizeBundleForMaas(unittest.TestCase):
    """Tests for summarize_bundle_for_maas (TASK 2)."""

    def _make_evolution_ledger(self, **overrides):
        """Helper to create evolution ledger."""
        defaults = {
            "stability_rating": "STABLE",
            "stability_score": 90.0,
            "frequent_blockers": [],
        }
        defaults.update(overrides)
        return defaults

    def _make_global_summary(self, **overrides):
        """Helper to create global summary."""
        defaults = {
            "bundle_ok": True,
            "integration_ready": True,
            "historical_failure_rate": 10.0,
            "hotspots": [],
            "health_indicators": {
                "total_runs_analyzed": 20,
                "recent_success_streak": 10,
            },
        }
        defaults.update(overrides)
        return defaults

    def test_maas_ok_all_good(self):
        """Test OK status when all conditions are met."""
        summary = self._make_global_summary()
        ledger = self._make_evolution_ledger()

        result = summarize_bundle_for_maas(summary, ledger)

        self.assertEqual(result["schema_version"], MAAS_BUNDLE_SCHEMA_VERSION)
        self.assertTrue(result["bundle_admissible"])
        self.assertEqual(result["status"], MAASStatus.OK.value)
        self.assertEqual(result["stability_rating"], "STABLE")

    def test_maas_blocked_critical(self):
        """Test BLOCK status when stability is CRITICAL."""
        summary = self._make_global_summary()
        ledger = self._make_evolution_ledger(stability_rating="CRITICAL")

        result = summarize_bundle_for_maas(summary, ledger)

        self.assertFalse(result["bundle_admissible"])
        self.assertEqual(result["status"], MAASStatus.BLOCK.value)
        self.assertIn("CRITICAL", result["status_reason"])

    def test_maas_blocked_high_failure_rate(self):
        """Test BLOCK status when failure rate too high."""
        summary = self._make_global_summary(historical_failure_rate=55.0)
        ledger = self._make_evolution_ledger()

        result = summarize_bundle_for_maas(summary, ledger)

        self.assertFalse(result["bundle_admissible"])
        self.assertEqual(result["status"], MAASStatus.BLOCK.value)
        self.assertIn("failure rate", result["status_reason"].lower())

    def test_maas_blocked_multiple_high_severity(self):
        """Test BLOCK status with multiple HIGH severity hotspots."""
        summary = self._make_global_summary(
            hotspots=[
                {"check_id": "PF-101", "severity": "HIGH"},
                {"check_id": "PF-401", "severity": "HIGH"},
            ]
        )
        ledger = self._make_evolution_ledger()

        result = summarize_bundle_for_maas(summary, ledger)

        self.assertFalse(result["bundle_admissible"])
        self.assertEqual(result["status"], MAASStatus.BLOCK.value)

    def test_maas_attention_not_integration_ready(self):
        """Test ATTENTION status when not integration ready."""
        summary = self._make_global_summary(integration_ready=False)
        ledger = self._make_evolution_ledger()

        result = summarize_bundle_for_maas(summary, ledger)

        self.assertFalse(result["bundle_admissible"])
        self.assertEqual(result["status"], MAASStatus.ATTENTION.value)
        self.assertIn("Insufficient", result["status_reason"])

    def test_maas_attention_degrading(self):
        """Test ATTENTION status when stability is DEGRADING."""
        summary = self._make_global_summary()
        ledger = self._make_evolution_ledger(stability_rating="DEGRADING")

        result = summarize_bundle_for_maas(summary, ledger)

        self.assertTrue(result["bundle_admissible"])
        self.assertEqual(result["status"], MAASStatus.ATTENTION.value)

    def test_maas_attention_single_high_severity(self):
        """Test ATTENTION status with single HIGH severity hotspot."""
        summary = self._make_global_summary(
            hotspots=[{"check_id": "PF-101", "severity": "HIGH"}]
        )
        ledger = self._make_evolution_ledger()

        result = summarize_bundle_for_maas(summary, ledger)

        self.assertTrue(result["bundle_admissible"])
        self.assertEqual(result["status"], MAASStatus.ATTENTION.value)

    def test_failure_hotspots_extracted(self):
        """Test that failure hotspots are extracted."""
        summary = self._make_global_summary(
            hotspots=[
                {"check_id": "PF-101", "severity": "MEDIUM", "impact_score": 15.0},
                {"check_id": "PF-401", "severity": "LOW", "impact_score": 5.0},
            ]
        )
        ledger = self._make_evolution_ledger()

        result = summarize_bundle_for_maas(summary, ledger)

        self.assertEqual(len(result["failure_hotspots"]), 2)
        self.assertEqual(result["failure_hotspots"][0]["check_id"], "PF-101")

    def test_metrics_included(self):
        """Test that metrics are included in result."""
        summary = self._make_global_summary()
        ledger = self._make_evolution_ledger()

        result = summarize_bundle_for_maas(summary, ledger)

        self.assertIn("metrics", result)
        self.assertIn("stability_score", result["metrics"])
        self.assertIn("historical_failure_rate", result["metrics"])


class TestBuildBundleDirectorPanel(unittest.TestCase):
    """Tests for build_bundle_director_panel (TASK 3)."""

    def _make_evolution_ledger(self, **overrides):
        """Helper to create evolution ledger."""
        defaults = {
            "stability_rating": "STABLE",
            "stability_score": 90.0,
            "window_analysis": {
                "recent_5_rate": 100.0,
                "recent_10_rate": 100.0,
                "recent_20_rate": 100.0,
            },
            "frequent_blockers": [],
            "eligibility_curve": [{"run_id": f"r{i}"} for i in range(10)],
        }
        defaults.update(overrides)
        return defaults

    def _make_integration_eval(self, **overrides):
        """Helper to create integration eval."""
        defaults = {
            "status": "OK",
            "integration_ok": True,
            "blocking_reasons": [],
            "warning_reasons": [],
            "metrics": {
                "historical_failure_rate": 10.0,
                "stability_rating": "STABLE",
                "stability_score": 90.0,
                "recent_5_rate": 100.0,
            },
        }
        defaults.update(overrides)
        return defaults

    def test_panel_green_all_good(self):
        """Test GREEN status light when all is well."""
        ledger = self._make_evolution_ledger()
        integration_eval = self._make_integration_eval()

        result = build_bundle_director_panel(ledger, integration_eval)

        self.assertEqual(result["schema_version"], DIRECTOR_PANEL_SCHEMA_VERSION)
        self.assertEqual(result["status_light"], "GREEN")
        self.assertTrue(result["integration_ok"])
        self.assertIn("stable", result["headline"].lower())

    def test_panel_yellow_warnings(self):
        """Test YELLOW status light with warnings."""
        ledger = self._make_evolution_ledger()
        integration_eval = self._make_integration_eval(
            status="WARN",
            warning_reasons=["Stability declining"]
        )

        result = build_bundle_director_panel(ledger, integration_eval)

        self.assertEqual(result["status_light"], "YELLOW")
        self.assertTrue(result["integration_ok"])
        self.assertIn("attention", result["headline"].lower())

    def test_panel_red_blocked(self):
        """Test RED status light when blocked."""
        ledger = self._make_evolution_ledger()
        integration_eval = self._make_integration_eval(
            status="BLOCK",
            integration_ok=False,
            blocking_reasons=["Stability rating is CRITICAL"]
        )

        result = build_bundle_director_panel(ledger, integration_eval)

        self.assertEqual(result["status_light"], "RED")
        self.assertFalse(result["integration_ok"])
        self.assertIn("blocked", result["headline"].lower())

    def test_panel_includes_historical_failure_rate(self):
        """Test that historical failure rate is included."""
        ledger = self._make_evolution_ledger()
        integration_eval = self._make_integration_eval(
            metrics={"historical_failure_rate": 15.5}
        )

        result = build_bundle_director_panel(ledger, integration_eval)

        self.assertEqual(result["historical_failure_rate"], 15.5)

    def test_panel_includes_stability_rating(self):
        """Test that stability rating is included."""
        ledger = self._make_evolution_ledger(stability_rating="IMPROVING")
        integration_eval = self._make_integration_eval()

        result = build_bundle_director_panel(ledger, integration_eval)

        self.assertEqual(result["stability_rating"], "IMPROVING")

    def test_panel_details_included(self):
        """Test that details are included."""
        ledger = self._make_evolution_ledger(
            frequent_blockers=[{"check_id": "PF-101", "failure_count": 5}]
        )
        integration_eval = self._make_integration_eval()

        result = build_bundle_director_panel(ledger, integration_eval)

        self.assertIn("details", result)
        self.assertIn("recent_eligibility", result["details"])
        self.assertIn("stability_score", result["details"])
        self.assertEqual(result["details"]["top_blocker"], "PF-101")

    def test_panel_headline_includes_run_count(self):
        """Test that headline includes run count."""
        ledger = self._make_evolution_ledger(
            eligibility_curve=[{"run_id": f"r{i}"} for i in range(25)]
        )
        integration_eval = self._make_integration_eval()

        result = build_bundle_director_panel(ledger, integration_eval)

        self.assertIn("25 runs", result["headline"])

    def test_panel_no_blockers(self):
        """Test panel with no frequent blockers."""
        ledger = self._make_evolution_ledger(frequent_blockers=[])
        integration_eval = self._make_integration_eval()

        result = build_bundle_director_panel(ledger, integration_eval)

        self.assertIsNone(result["details"]["top_blocker"])


# =============================================================================
# PHASE V TESTS: BUNDLE AS INTEGRATION BACKBONE
# =============================================================================

# Import Phase V functions
from u2_preflight_bundle import (
    build_bundle_cross_layer_view,
    summarize_bundle_for_global_console,
    build_bundle_governance_signal,
    LayerStatus,
    GovernanceSignalType,
    CROSS_LAYER_VIEW_SCHEMA_VERSION,
    GLOBAL_CONSOLE_SCHEMA_VERSION,
    GOVERNANCE_SIGNAL_SCHEMA_VERSION,
)


class TestBuildBundleCrossLayerView(unittest.TestCase):
    """Tests for build_bundle_cross_layer_view function."""

    def _make_evolution_ledger(
        self,
        stability_rating="STABLE",
        stability_score=85,
        total_runs=20,
        eligible_runs=18,
        eligibility_curve=None,
        frequent_blockers=None,
    ):
        """Create a test evolution ledger."""
        return {
            "stability_rating": stability_rating,
            "stability_score": stability_score,
            "total_runs": total_runs,
            "eligible_runs": eligible_runs,
            "eligibility_curve": eligibility_curve or [90, 95, 100],
            "frequent_blockers": frequent_blockers or [],
        }

    def _make_layer_state(self, status="OK", reason=None, ok=None):
        """Create a test layer state."""
        state = {"status": status}
        if reason:
            state["reason"] = reason
        if ok is not None:
            state["ok"] = ok
        return state

    def test_cross_layer_ok_all_layers_healthy(self):
        """Test cross-layer view when all layers are OK."""
        ledger = self._make_evolution_ledger()
        preflight = self._make_layer_state("OK")
        topology = self._make_layer_state("GREEN")  # Test normalization
        security = self._make_layer_state("PASS")  # Test normalization

        result = build_bundle_cross_layer_view(ledger, preflight, topology, security)

        self.assertEqual(result["status"], "OK")
        self.assertTrue(result["integration_ready"])
        self.assertEqual(result["blocking_layers"], [])
        self.assertEqual(result["warning_layers"], [])
        self.assertIn("schema_version", result)

    def test_cross_layer_block_single_layer(self):
        """Test that any layer BLOCK causes overall BLOCK."""
        ledger = self._make_evolution_ledger()
        preflight = self._make_layer_state("OK")
        topology = self._make_layer_state("BLOCK")
        security = self._make_layer_state("OK")

        result = build_bundle_cross_layer_view(ledger, preflight, topology, security)

        self.assertEqual(result["status"], "BLOCK")
        self.assertFalse(result["integration_ready"])
        self.assertIn("topology", result["blocking_layers"])

    def test_cross_layer_block_multiple_layers(self):
        """Test multiple blocking layers."""
        ledger = self._make_evolution_ledger()
        preflight = self._make_layer_state("FAIL")
        topology = self._make_layer_state("CRITICAL")
        security = self._make_layer_state("OK")

        result = build_bundle_cross_layer_view(ledger, preflight, topology, security)

        self.assertEqual(result["status"], "BLOCK")
        self.assertFalse(result["integration_ready"])
        self.assertIn("preflight", result["blocking_layers"])
        self.assertIn("topology", result["blocking_layers"])

    def test_cross_layer_block_from_bundle_critical(self):
        """Test that CRITICAL bundle stability causes BLOCK."""
        ledger = self._make_evolution_ledger(stability_rating="CRITICAL")
        preflight = self._make_layer_state("OK")
        topology = self._make_layer_state("OK")
        security = self._make_layer_state("OK")

        result = build_bundle_cross_layer_view(ledger, preflight, topology, security)

        self.assertEqual(result["status"], "BLOCK")
        self.assertFalse(result["integration_ready"])
        self.assertIn("bundle", result["blocking_layers"])

    def test_cross_layer_warn_with_warning_layers(self):
        """Test WARN status when layers have warnings."""
        ledger = self._make_evolution_ledger()
        preflight = self._make_layer_state("WARN")
        topology = self._make_layer_state("YELLOW")  # Test normalization
        security = self._make_layer_state("OK")

        result = build_bundle_cross_layer_view(ledger, preflight, topology, security)

        self.assertEqual(result["status"], "WARN")
        self.assertTrue(result["integration_ready"])  # Warnings don't block
        self.assertIn("preflight", result["warning_layers"])
        self.assertIn("topology", result["warning_layers"])

    def test_cross_layer_warn_from_bundle_degrading(self):
        """Test that DEGRADING bundle stability causes WARN."""
        ledger = self._make_evolution_ledger(stability_rating="DEGRADING")
        preflight = self._make_layer_state("OK")
        topology = self._make_layer_state("OK")
        security = self._make_layer_state("OK")

        result = build_bundle_cross_layer_view(ledger, preflight, topology, security)

        self.assertEqual(result["status"], "WARN")
        self.assertTrue(result["integration_ready"])
        self.assertIn("bundle", result["warning_layers"])

    def test_cross_layer_missing_layers_unknown(self):
        """Test that missing layers are marked UNKNOWN."""
        ledger = self._make_evolution_ledger()

        result = build_bundle_cross_layer_view(ledger, None, None, None)

        self.assertEqual(result["layer_statuses"]["preflight"]["status"], "UNKNOWN")
        self.assertEqual(result["layer_statuses"]["topology"]["status"], "UNKNOWN")
        self.assertEqual(result["layer_statuses"]["security"]["status"], "UNKNOWN")
        # UNKNOWN layers don't block
        self.assertTrue(result["integration_ready"])

    def test_cross_layer_bundle_context_included(self):
        """Test that bundle context is included."""
        ledger = self._make_evolution_ledger(
            total_runs=30,
            eligible_runs=27,
            eligibility_curve=[80, 85, 90],
        )

        result = build_bundle_cross_layer_view(ledger, None, None, None)

        self.assertIn("bundle_context", result)
        self.assertEqual(result["bundle_context"]["total_runs"], 30)
        self.assertEqual(result["bundle_context"]["eligible_runs"], 27)
        self.assertEqual(result["bundle_context"]["recent_eligibility_pct"], 90)

    def test_cross_layer_boolean_status_detection(self):
        """Test detection of boolean ok/healthy fields."""
        ledger = self._make_evolution_ledger()
        preflight = {"ok": True}
        topology = {"healthy": False}  # Should be BLOCK

        result = build_bundle_cross_layer_view(ledger, preflight, topology, None)

        self.assertEqual(result["layer_statuses"]["preflight"]["status"], "OK")
        self.assertEqual(result["layer_statuses"]["topology"]["status"], "BLOCK")
        self.assertIn("topology", result["blocking_layers"])

    def test_cross_layer_reason_extraction(self):
        """Test extraction of reason from layer state."""
        ledger = self._make_evolution_ledger()
        preflight = {"status": "WARN", "reason": "Seed mismatch detected"}

        result = build_bundle_cross_layer_view(ledger, preflight, None, None)

        self.assertEqual(
            result["layer_statuses"]["preflight"]["reason"],
            "Seed mismatch detected"
        )


class TestSummarizeBundleForGlobalConsole(unittest.TestCase):
    """Tests for summarize_bundle_for_global_console function."""

    def _make_evolution_ledger(
        self,
        stability_rating="STABLE",
        stability_score=85,
        total_runs=20,
        eligible_runs=18,
        frequent_blockers=None,
    ):
        """Create a test evolution ledger."""
        return {
            "stability_rating": stability_rating,
            "stability_score": stability_score,
            "total_runs": total_runs,
            "eligible_runs": eligible_runs,
            "frequent_blockers": frequent_blockers or [],
        }

    def _make_integration_eval(
        self,
        status="OK",
        integration_ok=True,
        blocking_reasons=None,
        warning_reasons=None,
    ):
        """Create a test integration evaluation."""
        return {
            "status": status,
            "integration_ok": integration_ok,
            "blocking_reasons": blocking_reasons or [],
            "warning_reasons": warning_reasons or [],
        }

    def test_console_green_healthy(self):
        """Test GREEN status for healthy bundle."""
        ledger = self._make_evolution_ledger()
        integration_eval = self._make_integration_eval()

        result = summarize_bundle_for_global_console(ledger, integration_eval)

        self.assertEqual(result["status_light"], "GREEN")
        self.assertFalse(result["action_required"])
        self.assertIn("healthy", result["headline"].lower())
        self.assertIn("schema_version", result)

    def test_console_yellow_warnings(self):
        """Test YELLOW status for warnings."""
        ledger = self._make_evolution_ledger(stability_rating="DEGRADING")
        integration_eval = self._make_integration_eval(
            status="WARN",
            warning_reasons=["Stability degrading"]
        )

        result = summarize_bundle_for_global_console(ledger, integration_eval)

        self.assertEqual(result["status_light"], "YELLOW")
        self.assertTrue(result["action_required"])
        self.assertIn("attention", result["headline"].lower())

    def test_console_red_blocked(self):
        """Test RED status for blocked bundle."""
        ledger = self._make_evolution_ledger(stability_rating="CRITICAL")
        integration_eval = self._make_integration_eval(
            status="BLOCK",
            integration_ok=False,
            blocking_reasons=["Critical stability"]
        )

        result = summarize_bundle_for_global_console(ledger, integration_eval)

        self.assertEqual(result["status_light"], "RED")
        self.assertTrue(result["action_required"])
        self.assertIn("blocked", result["headline"].lower())

    def test_console_historical_failure_rate(self):
        """Test historical failure rate calculation."""
        ledger = self._make_evolution_ledger(total_runs=100, eligible_runs=75)
        integration_eval = self._make_integration_eval()

        result = summarize_bundle_for_global_console(ledger, integration_eval)

        self.assertEqual(result["historical_failure_rate"], 0.25)

    def test_console_quick_stats_included(self):
        """Test that quick stats are included."""
        ledger = self._make_evolution_ledger(
            total_runs=50,
            stability_score=92,
            frequent_blockers=[{"check_id": "PF-101"}],
        )
        integration_eval = self._make_integration_eval(
            warning_reasons=["Warning 1", "Warning 2"]
        )

        result = summarize_bundle_for_global_console(ledger, integration_eval)

        self.assertIn("quick_stats", result)
        self.assertEqual(result["quick_stats"]["total_runs"], 50)
        self.assertEqual(result["quick_stats"]["stability_score"], 92)
        self.assertEqual(result["quick_stats"]["active_blockers"], 1)
        self.assertEqual(result["quick_stats"]["pending_warnings"], 2)

    def test_console_success_rate_calculation(self):
        """Test success rate in quick stats."""
        ledger = self._make_evolution_ledger(total_runs=20, eligible_runs=18)
        integration_eval = self._make_integration_eval()

        result = summarize_bundle_for_global_console(ledger, integration_eval)

        self.assertEqual(result["quick_stats"]["success_rate"], 90.0)

    def test_console_empty_runs(self):
        """Test handling of zero runs."""
        ledger = self._make_evolution_ledger(total_runs=0, eligible_runs=0)
        integration_eval = self._make_integration_eval()

        result = summarize_bundle_for_global_console(ledger, integration_eval)

        self.assertEqual(result["historical_failure_rate"], 0.0)

    def test_console_integration_ok_included(self):
        """Test that integration_ok is included."""
        ledger = self._make_evolution_ledger()
        integration_eval = self._make_integration_eval(integration_ok=True)

        result = summarize_bundle_for_global_console(ledger, integration_eval)

        self.assertTrue(result["integration_ok"])


class TestBuildBundleGovernanceSignal(unittest.TestCase):
    """Tests for build_bundle_governance_signal function."""

    def _make_evolution_ledger(
        self,
        stability_rating="STABLE",
        stability_score=85,
        total_runs=20,
        eligible_runs=18,
    ):
        """Create a test evolution ledger."""
        return {
            "stability_rating": stability_rating,
            "stability_score": stability_score,
            "total_runs": total_runs,
            "eligible_runs": eligible_runs,
        }

    def _make_cross_layer_view(
        self,
        status="OK",
        blocking_layers=None,
        warning_layers=None,
        integration_ready=True,
    ):
        """Create a test cross-layer view."""
        return {
            "status": status,
            "blocking_layers": blocking_layers or [],
            "warning_layers": warning_layers or [],
            "integration_ready": integration_ready,
        }

    def test_governance_proceed_all_good(self):
        """Test PROCEED signal when everything is OK."""
        ledger = self._make_evolution_ledger()
        cross_layer = self._make_cross_layer_view()

        result = build_bundle_governance_signal(cross_layer, ledger)

        self.assertEqual(result["signal_type"], "PROCEED")
        self.assertEqual(result["blocking_factors"], [])
        self.assertEqual(result["risk_indicators"], [])
        self.assertIn("proceed", result["recommendation"].lower())
        self.assertIn("schema_version", result)

    def test_governance_halt_blocking_layer(self):
        """Test HALT signal when a layer is blocking."""
        ledger = self._make_evolution_ledger()
        cross_layer = self._make_cross_layer_view(
            status="BLOCK",
            blocking_layers=["security"],
            integration_ready=False,
        )

        result = build_bundle_governance_signal(cross_layer, ledger)

        self.assertEqual(result["signal_type"], "HALT")
        self.assertIn("Layer 'security' is blocking", result["blocking_factors"])
        self.assertIn("halt", result["recommendation"].lower())

    def test_governance_halt_critical_stability(self):
        """Test HALT signal for CRITICAL stability."""
        ledger = self._make_evolution_ledger(stability_rating="CRITICAL")
        cross_layer = self._make_cross_layer_view()

        result = build_bundle_governance_signal(cross_layer, ledger)

        self.assertEqual(result["signal_type"], "HALT")
        self.assertIn("Bundle stability is CRITICAL", result["blocking_factors"])

    def test_governance_halt_low_success_rate(self):
        """Test HALT signal for low success rate (<50%)."""
        ledger = self._make_evolution_ledger(total_runs=20, eligible_runs=8)
        cross_layer = self._make_cross_layer_view()

        result = build_bundle_governance_signal(cross_layer, ledger)

        self.assertEqual(result["signal_type"], "HALT")
        self.assertTrue(any("Success rate too low" in f for f in result["blocking_factors"]))

    def test_governance_review_warning_layers(self):
        """Test REVIEW signal for warning layers."""
        ledger = self._make_evolution_ledger()
        cross_layer = self._make_cross_layer_view(
            status="WARN",
            warning_layers=["preflight"],
        )

        result = build_bundle_governance_signal(cross_layer, ledger)

        self.assertEqual(result["signal_type"], "REVIEW")
        self.assertIn("Layer 'preflight' requires attention", result["risk_indicators"])

    def test_governance_review_degrading_stability(self):
        """Test REVIEW signal for DEGRADING stability."""
        ledger = self._make_evolution_ledger(stability_rating="DEGRADING")
        cross_layer = self._make_cross_layer_view()

        result = build_bundle_governance_signal(cross_layer, ledger)

        self.assertEqual(result["signal_type"], "REVIEW")
        self.assertIn("Bundle stability is degrading", result["risk_indicators"])

    def test_governance_review_volatile_stability(self):
        """Test REVIEW signal for VOLATILE stability."""
        ledger = self._make_evolution_ledger(stability_rating="VOLATILE")
        cross_layer = self._make_cross_layer_view()

        result = build_bundle_governance_signal(cross_layer, ledger)

        self.assertEqual(result["signal_type"], "REVIEW")
        self.assertIn("Bundle stability is volatile", result["risk_indicators"])

    def test_governance_review_low_ish_success_rate(self):
        """Test REVIEW signal for success rate between 50-80%."""
        ledger = self._make_evolution_ledger(total_runs=20, eligible_runs=14)
        cross_layer = self._make_cross_layer_view()

        result = build_bundle_governance_signal(cross_layer, ledger)

        self.assertEqual(result["signal_type"], "REVIEW")
        self.assertTrue(any("Success rate below threshold" in r for r in result["risk_indicators"]))

    def test_governance_confidence_no_data(self):
        """Test confidence is 0 with no data."""
        ledger = self._make_evolution_ledger(total_runs=0, eligible_runs=0)
        cross_layer = self._make_cross_layer_view()

        result = build_bundle_governance_signal(cross_layer, ledger)

        self.assertEqual(result["confidence"], 0.0)

    def test_governance_confidence_limited_data(self):
        """Test confidence is reduced with limited data."""
        ledger = self._make_evolution_ledger(total_runs=3, eligible_runs=3)
        cross_layer = self._make_cross_layer_view()

        result = build_bundle_governance_signal(cross_layer, ledger)

        self.assertEqual(result["confidence"], 0.5)

    def test_governance_confidence_moderate_data(self):
        """Test confidence with moderate data."""
        ledger = self._make_evolution_ledger(total_runs=7, eligible_runs=7)
        cross_layer = self._make_cross_layer_view()

        result = build_bundle_governance_signal(cross_layer, ledger)

        self.assertEqual(result["confidence"], 0.7)

    def test_governance_confidence_unstable_reduces(self):
        """Test confidence is reduced with volatile stability."""
        ledger = self._make_evolution_ledger(
            total_runs=20,
            eligible_runs=20,
            stability_rating="VOLATILE"
        )
        cross_layer = self._make_cross_layer_view()

        result = build_bundle_governance_signal(cross_layer, ledger)

        self.assertEqual(result["confidence"], 0.6)

    def test_governance_audit_trail_included(self):
        """Test that audit trail is included."""
        ledger = self._make_evolution_ledger(stability_score=88)
        cross_layer = self._make_cross_layer_view(blocking_layers=["topology"])
        context = {
            "experiment_id": "EXP-001",
            "phase": "U2",
            "operator_id": "OP-123",
        }

        result = build_bundle_governance_signal(cross_layer, ledger, context)

        self.assertIn("audit_trail", result)
        self.assertEqual(result["audit_trail"]["experiment_id"], "EXP-001")
        self.assertEqual(result["audit_trail"]["phase"], "U2")
        self.assertEqual(result["audit_trail"]["operator_id"], "OP-123")
        self.assertEqual(result["audit_trail"]["stability_score"], 88)
        self.assertEqual(result["audit_trail"]["blocking_layers"], ["topology"])

    def test_governance_integration_ready_propagated(self):
        """Test that integration_ready is propagated from cross-layer view."""
        ledger = self._make_evolution_ledger()
        cross_layer = self._make_cross_layer_view(integration_ready=False)

        result = build_bundle_governance_signal(cross_layer, ledger)

        self.assertFalse(result["integration_ready"])


# Import adapter for canonical schema tests
from u2_preflight_bundle import adapt_bundle_signal_for_governance


class TestAdaptBundleSignalForGovernance(unittest.TestCase):
    """Tests for adapt_bundle_signal_for_governance - CLAUDE I schema alignment."""

    def _make_governance_signal(
        self,
        signal_type="PROCEED",
        confidence=1.0,
        blocking_factors=None,
        risk_indicators=None,
        recommendation="All systems nominal.",
    ):
        """Create a test governance signal."""
        return {
            "signal_type": signal_type,
            "confidence": confidence,
            "blocking_factors": blocking_factors or [],
            "risk_indicators": risk_indicators or [],
            "recommendation": recommendation,
        }

    def test_adapter_schema_fields(self):
        """Test that adapter output has all canonical GovernanceSignal fields."""
        gov_signal = self._make_governance_signal()

        result = adapt_bundle_signal_for_governance(gov_signal)

        # Canonical schema requires these fields
        self.assertIn("layer_name", result)
        self.assertIn("status", result)
        self.assertIn("blocking_rules", result)
        self.assertIn("blocking_rate", result)
        self.assertIn("headline", result)
        self.assertEqual(result["layer_name"], "bundle")

    def test_adapter_proceed_maps_to_ok(self):
        """Test PROCEED signal maps to OK status."""
        gov_signal = self._make_governance_signal(signal_type="PROCEED")

        result = adapt_bundle_signal_for_governance(gov_signal)

        self.assertEqual(result["status"], "OK")
        self.assertEqual(result["blocking_rate"], 0.0)

    def test_adapter_review_maps_to_warn(self):
        """Test REVIEW signal maps to WARN status."""
        gov_signal = self._make_governance_signal(
            signal_type="REVIEW",
            risk_indicators=["Stability degrading"],
        )

        result = adapt_bundle_signal_for_governance(gov_signal)

        self.assertEqual(result["status"], "WARN")
        self.assertEqual(result["blocking_rate"], 0.3)
        self.assertIn("Stability degrading", result["blocking_rules"])

    def test_adapter_halt_maps_to_block(self):
        """Test HALT signal maps to BLOCK status."""
        gov_signal = self._make_governance_signal(
            signal_type="HALT",
            confidence=0.8,
            blocking_factors=["Critical failure"],
        )

        result = adapt_bundle_signal_for_governance(gov_signal)

        self.assertEqual(result["status"], "BLOCK")
        self.assertEqual(result["blocking_rate"], 0.2)  # 1.0 - 0.8 confidence
        self.assertIn("Critical failure", result["blocking_rules"])

    def test_adapter_combines_blocking_factors_and_risk_indicators(self):
        """Test that blocking_rules combines factors and indicators."""
        gov_signal = self._make_governance_signal(
            signal_type="HALT",
            blocking_factors=["Factor 1", "Factor 2"],
            risk_indicators=["Risk 1"],
        )

        result = adapt_bundle_signal_for_governance(gov_signal)

        self.assertEqual(len(result["blocking_rules"]), 3)
        self.assertIn("Factor 1", result["blocking_rules"])
        self.assertIn("Risk 1", result["blocking_rules"])

    def test_adapter_uses_recommendation_as_headline(self):
        """Test that recommendation becomes headline."""
        gov_signal = self._make_governance_signal(
            recommendation="Review recommended: stability issues"
        )

        result = adapt_bundle_signal_for_governance(gov_signal)

        self.assertEqual(result["headline"], "Review recommended: stability issues")

    def test_adapter_handles_missing_fields(self):
        """Test adapter handles minimal input gracefully."""
        # Minimal signal with only signal_type
        gov_signal = {"signal_type": "HALT"}

        result = adapt_bundle_signal_for_governance(gov_signal)

        self.assertEqual(result["status"], "BLOCK")
        self.assertEqual(result["blocking_rules"], [])
        self.assertEqual(result["blocking_rate"], 1.0)  # 1.0 - 0.0 confidence
        self.assertIn("Bundle: BLOCK", result["headline"])

    def test_adapter_blocking_rate_bounds(self):
        """Test blocking_rate stays in [0, 1] range."""
        # Test with various confidence levels
        for conf in [0.0, 0.5, 1.0]:
            gov_signal = self._make_governance_signal(
                signal_type="HALT",
                confidence=conf,
            )
            result = adapt_bundle_signal_for_governance(gov_signal)
            self.assertGreaterEqual(result["blocking_rate"], 0.0)
            self.assertLessEqual(result["blocking_rate"], 1.0)


if __name__ == "__main__":
    unittest.main(argv=['first-arg-is-ignored'], exit=False)
