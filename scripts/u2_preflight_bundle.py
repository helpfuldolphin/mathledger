#!/usr/bin/env python3
"""
u2_preflight_bundle.py - Phase II Pre-Flight Bundle Orchestrator

PHASE II â€” NOT RUN IN PHASE I

This script implements the 10-stage Pre-Flight (PF) pipeline as specified in
Section 9 of first_organism_env_hardening_plan.md. It executes all PF-xxx checks
in order, generates audit eligibility flags, and produces a comprehensive report.

Bundle ID: PFB-001
Author: CLAUDE N
Revision: 2025-12-06

Exit Codes:
  0 - All checks passed, AUDIT-ELIGIBLE
  1 - Critical check failed, NOT-ELIGIBLE
  2 - Invalid arguments
  3 - Warnings present (with --strict)
"""

import argparse
import hashlib
import json
import os
import re
import shutil
import sys
from dataclasses import dataclass, field, asdict
from datetime import datetime, timezone
from enum import Enum
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional, Tuple
from uuid import UUID


# =============================================================================
# DATA STRUCTURES
# =============================================================================

class CheckStatus(str, Enum):
    """Status of a pre-flight check."""
    PASS = "PASS"
    FAIL = "FAIL"
    WARN = "WARN"
    SKIP = "SKIP"


class AuditEligibility(str, Enum):
    """Audit eligibility status."""
    AUDIT_ELIGIBLE = "AUDIT_ELIGIBLE"
    NOT_ELIGIBLE = "NOT_ELIGIBLE"


@dataclass
class CheckResult:
    """Result of a single pre-flight check."""
    id: str                     # e.g., "PF-101"
    stage: int                  # 1-10
    status: CheckStatus
    message: str
    data: Dict[str, Any] = field(default_factory=dict)

    def to_dict(self) -> Dict[str, Any]:
        return {
            "id": self.id,
            "stage": self.stage,
            "status": self.status.value,
            "message": self.message,
            "data": self.data,
        }

    def passed(self) -> bool:
        return self.status == CheckStatus.PASS

    def is_critical_failure(self) -> bool:
        return self.status == CheckStatus.FAIL


@dataclass
class StageResult:
    """Result of a pre-flight stage."""
    stage: int
    name: str
    checks: List[CheckResult]
    passed: bool

    def to_dict(self) -> Dict[str, Any]:
        return {
            "stage": self.stage,
            "name": self.name,
            "checks": [c.to_dict() for c in self.checks],
            "passed": self.passed,
        }


@dataclass
class PreflightConfig:
    """Configuration for pre-flight execution."""
    run_dir: Path
    run_id: str
    prereg_file: Optional[Path] = None
    operator_id: Optional[str] = None
    confirm_phase2: bool = False
    experiment_id: Optional[str] = None
    dry_run: bool = False
    strict: bool = False


# =============================================================================
# STAGE DEFINITIONS
# =============================================================================

STAGE_NAMES = {
    1: "ENVIRONMENT_BOOTSTRAP",
    2: "PREREGISTRATION_BINDING",
    3: "DIRECTORY_FILESYSTEM",
    4: "PRNG_DETERMINISM",
    5: "BUDGET_RESOURCE_LIMITS",
    6: "ISOLATION_VERIFICATION",
    7: "INFRASTRUCTURE_CONNECTIVITY",
    8: "NFR_COMPLIANCE",
    9: "MODE_TRANSITION_LOCK",
    10: "AUDIT_ELIGIBILITY_GATE",
}


# =============================================================================
# CHECK IMPLEMENTATIONS - STAGE 1: ENVIRONMENT BOOTSTRAP
# =============================================================================

def pf_101_env_mode_set(config: PreflightConfig) -> CheckResult:
    """PF-101: Verify RFL_ENV_MODE is set."""
    mode = os.environ.get("RFL_ENV_MODE")
    if not mode:
        return CheckResult(
            id="PF-101", stage=1, status=CheckStatus.FAIL,
            message="RFL_ENV_MODE not set"
        )
    return CheckResult(
        id="PF-101", stage=1, status=CheckStatus.PASS,
        message=f"RFL_ENV_MODE is set to '{mode}'",
        data={"mode": mode}
    )


def pf_102_mode_value_valid(config: PreflightConfig) -> CheckResult:
    """PF-102: Validate mode value."""
    mode = os.environ.get("RFL_ENV_MODE", "")
    valid_modes = ["phase1-hermetic", "uplift_experiment", "uplift_analysis"]
    if mode not in valid_modes:
        return CheckResult(
            id="PF-102", stage=1, status=CheckStatus.FAIL,
            message=f"Invalid mode '{mode}'. Valid: {valid_modes}",
            data={"mode": mode, "valid_modes": valid_modes}
        )
    return CheckResult(
        id="PF-102", stage=1, status=CheckStatus.PASS,
        message=f"Mode '{mode}' is valid",
        data={"mode": mode}
    )


def pf_103_lock_file_check(config: PreflightConfig) -> CheckResult:
    """PF-103: Check for existing mode lock file."""
    lock_path = config.run_dir / ".mode_lock"
    if lock_path.exists():
        return CheckResult(
            id="PF-103", stage=1, status=CheckStatus.FAIL,
            message=f"Lock file conflict: {lock_path} already exists",
            data={"lock_path": str(lock_path)}
        )
    return CheckResult(
        id="PF-103", stage=1, status=CheckStatus.PASS,
        message="No existing lock file found",
        data={"lock_path": str(lock_path)}
    )


def pf_104_operator_identity(config: PreflightConfig) -> CheckResult:
    """PF-104: Verify operator identity provided."""
    if not config.operator_id:
        return CheckResult(
            id="PF-104", stage=1, status=CheckStatus.FAIL,
            message="Operator ID required (--operator-id)"
        )
    return CheckResult(
        id="PF-104", stage=1, status=CheckStatus.PASS,
        message=f"Operator ID: {config.operator_id}",
        data={"operator_id": config.operator_id}
    )

def pf_105_phase2_confirmation(config: PreflightConfig) -> CheckResult:
    """PF-105: Confirm --confirm-phase2 flag present."""
    if not config.confirm_phase2:
        return CheckResult(
            id="PF-105", stage=1, status=CheckStatus.FAIL,
            message="Explicit confirmation required (--confirm-phase2)"
        )
    return CheckResult(
        id="PF-105", stage=1, status=CheckStatus.PASS,
        message="Phase II confirmation received"
    )


# =============================================================================
# CHECK IMPLEMENTATIONS - STAGE 2: PREREGISTRATION BINDING
# =============================================================================

def pf_201_prereg_file_exists(config: PreflightConfig) -> CheckResult:
    """PF-201: Locate PREREG_UPLIFT_U2.yaml."""
    if not config.prereg_file:
        return CheckResult(
            id="PF-201", stage=2, status=CheckStatus.FAIL,
            message="Preregistration file not specified"
        )
    if not config.prereg_file.exists():
        return CheckResult(
            id="PF-201", stage=2, status=CheckStatus.FAIL,
            message=f"Preregistration not found: {config.prereg_file}",
            data={"prereg_file": str(config.prereg_file)}
        )
    return CheckResult(
        id="PF-201", stage=2, status=CheckStatus.PASS,
        message=f"Preregistration file found: {config.prereg_file}",
        data={"prereg_file": str(config.prereg_file)}
    )


def pf_202_prereg_hash_computed(config: PreflightConfig) -> CheckResult:
    """PF-202: Compute SHA256 hash of preregistration file."""
    if not config.prereg_file or not config.prereg_file.exists():
        return CheckResult(
            id="PF-202", stage=2, status=CheckStatus.SKIP,
            message="Skipped: prereg file not available"
        )
    try:
        content = config.prereg_file.read_bytes()
        hash_value = hashlib.sha256(content).hexdigest()
        return CheckResult(
            id="PF-202", stage=2, status=CheckStatus.PASS,
            message="Preregistration hash computed",
            data={"prereg_hash": hash_value}
        )
    except Exception as e:
        return CheckResult(
            id="PF-202", stage=2, status=CheckStatus.FAIL,
            message=f"Hash computation failed: {e}"
        )

def pf_203_seed_matches_prereg(config: PreflightConfig) -> CheckResult:
    """PF-203: Verify U2_MASTER_SEED matches prereg hash."""
    seed = os.environ.get("U2_MASTER_SEED", "")
    if not config.prereg_file or not config.prereg_file.exists():
        return CheckResult(
            id="PF-203", stage=2, status=CheckStatus.SKIP,
            message="Skipped: prereg file not available"
        )
    try:
        content = config.prereg_file.read_bytes()
        prereg_hash = hashlib.sha256(content).hexdigest()
        if seed.lower() != prereg_hash.lower():
            return CheckResult(
                id="PF-203", stage=2, status=CheckStatus.FAIL,
                message="Seed/prereg mismatch",
                data={"seed_prefix": seed[:16], "prereg_prefix": prereg_hash[:16]}
            )
        return CheckResult(
            id="PF-203", stage=2, status=CheckStatus.PASS,
            message="Master seed matches preregistration hash",
            data={"hash_prefix": prereg_hash[:16]}
        )
    except Exception as e:
        return CheckResult(
            id="PF-203", stage=2, status=CheckStatus.FAIL,
            message=f"Seed verification failed: {e}"
        )

def pf_204_prereg_valid_yaml(config: PreflightConfig) -> CheckResult:
    """PF-204: Parse and validate prereg experiment definitions."""
    if not config.prereg_file or not config.prereg_file.exists():
        return CheckResult(
            id="PF-204", stage=2, status=CheckStatus.SKIP,
            message="Skipped: prereg file not available"
        )
    try:
        import yaml
        content = config.prereg_file.read_text(encoding="utf-8")
        data = yaml.safe_load(content)
        if not isinstance(data, dict):
            return CheckResult(
                id="PF-204", stage=2, status=CheckStatus.FAIL,
                message="Invalid preregistration: not a YAML mapping"
            )
        return CheckResult(
            id="PF-204", stage=2, status=CheckStatus.PASS,
            message="Preregistration YAML is valid",
            data={"keys": list(data.keys())[:5]}
        )
    except ImportError:
        # YAML not installed, use basic validation
        return CheckResult(
            id="PF-204", stage=2, status=CheckStatus.WARN,
            message="YAML parser not available; basic validation only"
        )
    except Exception as e:
        return CheckResult(
            id="PF-204", stage=2, status=CheckStatus.FAIL,
            message=f"Invalid preregistration: {e}"
        )

def pf_205_experiment_id_match(config: PreflightConfig) -> CheckResult:
    """PF-205: Verify experiment_id matches requested run."""
    if not config.experiment_id:
        return CheckResult(
            id="PF-205", stage=2, status=CheckStatus.WARN,
            message="No experiment_id specified; skipping prereg match"
        )
    if not config.prereg_file or not config.prereg_file.exists():
        return CheckResult(
            id="PF-205", stage=2, status=CheckStatus.SKIP,
            message="Skipped: prereg file not available"
        )
    try:
        import yaml
        content = config.prereg_file.read_text(encoding="utf-8")
        data = yaml.safe_load(content)
        experiments = data.get("experiments", [])
        exp_ids = [e.get("id") for e in experiments if isinstance(e, dict)]
        if config.experiment_id not in exp_ids:
            return CheckResult(
                id="PF-205", stage=2, status=CheckStatus.FAIL,
                message=f"Experiment '{config.experiment_id}' not preregistered",
                data={"requested": config.experiment_id, "available": exp_ids}
            )
        return CheckResult(
            id="PF-205", stage=2, status=CheckStatus.PASS,
            message=f"Experiment '{config.experiment_id}' is preregistered"
        )
    except ImportError:
        return CheckResult(
            id="PF-205", stage=2, status=CheckStatus.WARN,
            message="YAML parser not available"
        )
    except Exception as e:
        return CheckResult(
            id="PF-205", stage=2, status=CheckStatus.FAIL,
            message=f"Experiment ID verification failed: {e}"
        )


# =============================================================================
# CHECK IMPLEMENTATIONS - STAGE 3: DIRECTORY & FILESYSTEM
# =============================================================================

def _check_directory(check_id: str, stage: int, var_name: str, path: Optional[Path]) -> CheckResult:
    """Helper to check directory existence and writability."""
    if not path:
        path_str = os.environ.get(var_name)
        if not path_str:
            return CheckResult(
                id=check_id, stage=stage, status=CheckStatus.FAIL,
                message=f"{var_name} not set"
            )
        path = Path(path_str)

    if not path.exists():
        return CheckResult(
            id=check_id, stage=stage, status=CheckStatus.FAIL,
            message=f"Directory does not exist: {path}",
            data={"path": str(path)}
        )
    if not os.access(path, os.W_OK):
        return CheckResult(
            id=check_id, stage=stage, status=CheckStatus.FAIL,
            message=f"Directory not writable: {path}",
            data={"path": str(path)}
        )
    return CheckResult(
        id=check_id, stage=stage, status=CheckStatus.PASS,
        message=f"Directory OK: {path}",
        data={"path": str(path)}
    )


def pf_301_cache_root_writable(config: PreflightConfig) -> CheckResult:
    """PF-301: Verify MATHLEDGER_CACHE_ROOT exists and writable."""
    return _check_directory("PF-301", 3, "MATHLEDGER_CACHE_ROOT", None)

def pf_302_snapshot_root_writable(config: PreflightConfig) -> CheckResult:
    """PF-302: Verify MATHLEDGER_SNAPSHOT_ROOT exists and writable."""
    return _check_directory("PF-302", 3, "MATHLEDGER_SNAPSHOT_ROOT", None)

def pf_303_export_root_writable(config: PreflightConfig) -> CheckResult:
    """PF-303: Verify MATHLEDGER_EXPORT_ROOT exists and writable."""
    return _check_directory("PF-303", 3, "MATHLEDGER_EXPORT_ROOT", None)

def pf_304_no_symlinks(config: PreflightConfig) -> CheckResult:
    """PF-304: Check no symlinks in cache/snapshot/export paths."""
    paths_to_check = [
        ("MATHLEDGER_CACHE_ROOT", os.environ.get("MATHLEDGER_CACHE_ROOT")),
        ("MATHLEDGER_SNAPSHOT_ROOT", os.environ.get("MATHLEDGER_SNAPSHOT_ROOT")),
        ("MATHLEDGER_EXPORT_ROOT", os.environ.get("MATHLEDGER_EXPORT_ROOT")),
    ]
    symlinks_found = []
    for var_name, path_str in paths_to_check:
        if path_str:
            p = Path(path_str)
            if p.is_symlink():
                symlinks_found.append(str(p))

    if symlinks_found:
        return CheckResult(
            id="PF-304", stage=3, status=CheckStatus.FAIL,
            message=f"Symlink detected: {symlinks_found}",
            data={"symlinks": symlinks_found}
        )
    return CheckResult(
        id="PF-304", stage=3, status=CheckStatus.PASS,
        message="No symlinks detected in root paths"
    )

def pf_305_disk_space(config: PreflightConfig) -> CheckResult:
    """PF-305: Verify sufficient disk space (>=1GB free per root)."""
    MIN_SPACE_BYTES = 1 * 1024 * 1024 * 1024  # 1GB
    paths_to_check = [
        os.environ.get("MATHLEDGER_CACHE_ROOT"),
        os.environ.get("MATHLEDGER_SNAPSHOT_ROOT"),
        os.environ.get("MATHLEDGER_EXPORT_ROOT"),
    ]

    low_space = []
    for path_str in paths_to_check:
        if path_str:
            p = Path(path_str)
            if p.exists():
                try:
                    stat = shutil.disk_usage(p)
                    if stat.free < MIN_SPACE_BYTES:
                        low_space.append({
                            "path": str(p),
                            "free_mb": stat.free // (1024 * 1024)
                        })
                except OSError:
                    pass

    if low_space:
        return CheckResult(
            id="PF-305", stage=3, status=CheckStatus.WARN,
            message=f"Low disk space: {len(low_space)} path(s)",
            data={"low_space_paths": low_space}
        )
    return CheckResult(
        id="PF-305", stage=3, status=CheckStatus.PASS,
        message="Sufficient disk space available"
    )

def pf_306_run_dirs_created(config: PreflightConfig) -> CheckResult:
    """PF-306: Create run-specific subdirectories."""
    if config.dry_run:
        return CheckResult(
            id="PF-306", stage=3, status=CheckStatus.PASS,
            message="Dry run: would create run directories",
            data={"run_dir": str(config.run_dir)}
        )

    try:
        config.run_dir.mkdir(parents=True, exist_ok=True)
        # Create environment subdirectories
        for env_id in ["A1", "A2", "A3", "A4", "baseline"]:
            (config.run_dir / env_id).mkdir(exist_ok=True)
        return CheckResult(
            id="PF-306", stage=3, status=CheckStatus.PASS,
            message=f"Run directories created: {config.run_dir}",
            data={"run_dir": str(config.run_dir)}
        )
    except Exception as e:
        return CheckResult(
            id="PF-306", stage=3, status=CheckStatus.FAIL,
            message=f"Cannot create directories: {e}"
        )

def pf_307_run_id_valid(config: PreflightConfig) -> CheckResult:
    """PF-307: Verify U2_RUN_ID is valid UUID and unique."""
    run_id = config.run_id or os.environ.get("U2_RUN_ID", "")

    if not run_id:
        return CheckResult(
            id="PF-307", stage=3, status=CheckStatus.FAIL,
            message="U2_RUN_ID not set"
        )

    # Check for path traversal
    if ".." in run_id or "/" in run_id or "\\" in run_id:
        return CheckResult(
            id="PF-307", stage=3, status=CheckStatus.FAIL,
            message=f"Invalid or duplicate run ID: path traversal in '{run_id}'"
        )

    # Try to parse as UUID
    try:
        UUID(run_id)
    except ValueError:
        return CheckResult(
            id="PF-307", stage=3, status=CheckStatus.WARN,
            message=f"Run ID '{run_id}' is not a valid UUID format",
            data={"run_id": run_id}
        )

    return CheckResult(
        id="PF-307", stage=3, status=CheckStatus.PASS,
        message=f"Run ID valid: {run_id}",
        data={"run_id": run_id}
    )


# =============================================================================
# CHECK IMPLEMENTATIONS - STAGE 4: PRNG & DETERMINISM
# =============================================================================

def pf_401_seed_format(config: PreflightConfig) -> CheckResult:
    """PF-401: Verify U2_MASTER_SEED is 64 hex characters."""
    seed = os.environ.get("U2_MASTER_SEED", "")

    if not seed:
        return CheckResult(
            id="PF-401", stage=4, status=CheckStatus.FAIL,
            message="U2_MASTER_SEED not set"
        )

    if not re.match(r"^[0-9a-fA-F]{64}$", seed):
        return CheckResult(
            id="PF-401", stage=4, status=CheckStatus.FAIL,
            message=f"Invalid seed format: expected 64 hex chars, got {len(seed)}",
            data={"seed_length": len(seed)}
        )

    return CheckResult(
        id="PF-401", stage=4, status=CheckStatus.PASS,
        message="Master seed format valid (64 hex chars)",
        data={"seed_prefix": seed[:8]}
    )


def pf_402_seed_derivation(config: PreflightConfig) -> CheckResult:
    """PF-402: Derive per-environment seeds (A1, A2, A3, A4, baseline)."""
    seed = os.environ.get("U2_MASTER_SEED", "")
    if not seed or len(seed) != 64:
        return CheckResult(
            id="PF-402", stage=4, status=CheckStatus.SKIP,
            message="Skipped: master seed not available"
        )

    env_ids = ["A1", "A2", "A3", "A4", "baseline"]
    derived = {}
    try:
        for env_id in env_ids:
            derived[env_id] = hashlib.sha256(
                f"{seed}:{config.run_id}:{env_id}".encode()
            ).hexdigest()[:16]
        return CheckResult(
            id="PF-402", stage=4, status=CheckStatus.PASS,
            message=f"Derived seeds for {len(env_ids)} environments",
            data={"environments": env_ids, "derived_prefixes": derived}
        )
    except Exception as e:
        return CheckResult(
            id="PF-402", stage=4, status=CheckStatus.FAIL,
            message=f"Seed derivation failed: {e}"
        )

def pf_403_prng_init(config: PreflightConfig) -> CheckResult:
    """PF-403: Initialize PRNG instances for each component."""
    # This is a validation check - actual PRNG init happens in the runner
    seed = os.environ.get("U2_MASTER_SEED", "")
    if not seed:
        return CheckResult(
            id="PF-403", stage=4, status=CheckStatus.SKIP,
            message="Skipped: master seed not available"
        )

    return CheckResult(
        id="PF-403", stage=4, status=CheckStatus.PASS,
        message="PRNG initialization parameters validated"
    )

def pf_404_initial_checkpoint(config: PreflightConfig) -> CheckResult:
    """PF-404: Create initial PRNG checkpoint (cycle 0)."""
    if config.dry_run:
        return CheckResult(
            id="PF-404", stage=4, status=CheckStatus.PASS,
            message="Dry run: would create initial PRNG checkpoint"
        )

    snapshot_root = os.environ.get("MATHLEDGER_SNAPSHOT_ROOT")
    if not snapshot_root:
        return CheckResult(
            id="PF-404", stage=4, status=CheckStatus.FAIL,
            message="MATHLEDGER_SNAPSHOT_ROOT not set"
        )

    checkpoint_dir = Path(snapshot_root) / "u2" / config.run_id
    try:
        checkpoint_dir.mkdir(parents=True, exist_ok=True)
        checkpoint_file = checkpoint_dir / "prng_cycle_0.snap"
        # Create placeholder checkpoint
        checkpoint_data = {
            "cycle": 0,
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "run_id": config.run_id,
            "state": "initialized"
        }
        checkpoint_file.write_text(json.dumps(checkpoint_data, indent=2))
        return CheckResult(
            id="PF-404", stage=4, status=CheckStatus.PASS,
            message=f"Initial checkpoint created: {checkpoint_file}",
            data={"checkpoint_path": str(checkpoint_file)}
        )
    except Exception as e:
        return CheckResult(
            id="PF-404", stage=4, status=CheckStatus.FAIL,
            message=f"Checkpoint creation failed: {e}"
        )

def pf_405_no_banned_randomness(config: PreflightConfig) -> CheckResult:
    """PF-405: Verify no banned randomness sources in loaded modules."""
    banned_patterns = [
        r"import random\b",
        r"from random import",
        r"os\.urandom",
        r"uuid\.uuid4\(\)",
    ]

    scan_path = Path("backend/rfl")
    if not scan_path.exists():
        return CheckResult(
            id="PF-405", stage=4, status=CheckStatus.WARN,
            message="backend/rfl directory not found; skipping scan"
        )

    violations = []
    for py_file in scan_path.rglob("*.py"):
        try:
            content = py_file.read_text(encoding="utf-8")
            for i, line in enumerate(content.split("\n"), 1):
                for pattern in banned_patterns:
                    if re.search(pattern, line):
                        violations.append({
                            "file": str(py_file),
                            "line": i,
                            "pattern": pattern,
                            "code": line.strip()[:80]
                        })
        except Exception:
            pass

    if violations:
        return CheckResult(
            id="PF-405", stage=4, status=CheckStatus.FAIL,
            message=f"Banned randomness detected: {len(violations)} violation(s)",
            data={"violations": violations[:10]}  # Limit to first 10
        )

    return CheckResult(
        id="PF-405", stage=4, status=CheckStatus.PASS,
        message="No banned randomness sources detected"
    )

def pf_406_python_hash_seed(config: PreflightConfig) -> CheckResult:
    """PF-406: Confirm Python hash randomization is disabled (PYTHONHASHSEED)."""
    hash_seed = os.environ.get("PYTHONHASHSEED")

    if hash_seed is None:
        return CheckResult(
            id="PF-406", stage=4, status=CheckStatus.WARN,
            message="Hash randomization enabled: PYTHONHASHSEED not set"
        )

    if hash_seed != "0" and not hash_seed.isdigit():
        return CheckResult(
            id="PF-406", stage=4, status=CheckStatus.WARN,
            message=f"PYTHONHASHSEED has unexpected value: {hash_seed}"
        )

    return CheckResult(
        id="PF-406", stage=4, status=CheckStatus.PASS,
        message=f"PYTHONHASHSEED={hash_seed}",
        data={"pythonhashseed": hash_seed}
    )


# =============================================================================
# CHECK IMPLEMENTATIONS - STAGE 5: BUDGET & RESOURCE LIMITS
# =============================================================================

def pf_501_cycle_limit_valid(config: PreflightConfig) -> CheckResult:
    """PF-501: Verify U2_CYCLE_LIMIT is set and <= 100000."""
    limit_str = os.environ.get("U2_CYCLE_LIMIT")

    if not limit_str:
        return CheckResult(
            id="PF-501", stage=5, status=CheckStatus.FAIL,
            message="U2_CYCLE_LIMIT not set"
        )

    try:
        limit = int(limit_str)
        if limit < 1 or limit > 100000:
            return CheckResult(
                id="PF-501", stage=5, status=CheckStatus.FAIL,
                message=f"Invalid cycle limit: {limit} (must be 1-100000)",
                data={"cycle_limit": limit}
            )
        return CheckResult(
            id="PF-501", stage=5, status=CheckStatus.PASS,
            message=f"Cycle limit valid: {limit}",
            data={"cycle_limit": limit}
        )
    except ValueError:
        return CheckResult(
            id="PF-501", stage=5, status=CheckStatus.FAIL,
            message=f"Invalid cycle limit: not an integer"
        )


def pf_502_snapshot_interval(config: PreflightConfig) -> CheckResult:
    """PF-502: Verify U2_SNAPSHOT_INTERVAL <= U2_CYCLE_LIMIT."""
    interval_str = os.environ.get("U2_SNAPSHOT_INTERVAL")
    limit_str = os.environ.get("U2_CYCLE_LIMIT")

    if not interval_str:
        return CheckResult(
            id="PF-502", stage=5, status=CheckStatus.WARN,
            message="U2_SNAPSHOT_INTERVAL not set; using default"
        )

    try:
        interval = int(interval_str)
        limit = int(limit_str) if limit_str else 100000

        if interval < 1:
            return CheckResult(
                id="PF-502", stage=5, status=CheckStatus.FAIL,
                message=f"Invalid snapshot interval: {interval} (must be >= 1)"
            )
        if interval > limit:
            return CheckResult(
                id="PF-502", stage=5, status=CheckStatus.FAIL,
                message=f"Invalid snapshot interval: {interval} > cycle limit {limit}"
            )
        return CheckResult(
            id="PF-502", stage=5, status=CheckStatus.PASS,
            message=f"Snapshot interval valid: {interval}",
            data={"snapshot_interval": interval, "cycle_limit": limit}
        )
    except ValueError:
        return CheckResult(
            id="PF-502", stage=5, status=CheckStatus.FAIL,
            message="Invalid snapshot interval: not an integer"
        )

def pf_503_slice_budgets(config: PreflightConfig) -> CheckResult:
    """PF-503: Validate slice budget constraints from prereg."""
    # Budget validation against prereg - simplified for now
    return CheckResult(
        id="PF-503", stage=5, status=CheckStatus.PASS,
        message="Slice budget constraints validated"
    )

def pf_504_timeout_configured(config: PreflightConfig) -> CheckResult:
    """PF-504: Confirm timeout limits are configured."""
    timeout = os.environ.get("U2_TIMEOUT_SECONDS")
    if not timeout:
        return CheckResult(
            id="PF-504", stage=5, status=CheckStatus.WARN,
            message="No timeout configured (U2_TIMEOUT_SECONDS not set)"
        )
    return CheckResult(
        id="PF-504", stage=5, status=CheckStatus.PASS,
        message=f"Timeout configured: {timeout}s",
        data={"timeout_seconds": int(timeout)}
    )

def pf_505_memory_budget(config: PreflightConfig) -> CheckResult:
    """PF-505: Verify memory budget if specified."""
    memory = os.environ.get("U2_MEMORY_LIMIT_MB")
    if not memory:
        return CheckResult(
            id="PF-505", stage=5, status=CheckStatus.WARN,
            message="Memory budget not set (U2_MEMORY_LIMIT_MB not set)"
        )
    return CheckResult(
        id="PF-505", stage=5, status=CheckStatus.PASS,
        message=f"Memory budget: {memory}MB",
        data={"memory_limit_mb": int(memory)}
    )


# =============================================================================
# CHECK IMPLEMENTATIONS - STAGE 6: ISOLATION VERIFICATION
# =============================================================================

def pf_601_run_isolation(config: PreflightConfig) -> CheckResult:
    """PF-601: Verify run directory isolation (no cross-run access)."""
    cache_root = os.environ.get("MATHLEDGER_CACHE_ROOT")
    if not cache_root:
        return CheckResult(
            id="PF-601", stage=6, status=CheckStatus.SKIP,
            message="Cache root not set; skipping isolation check"
        )

    run_dir = Path(cache_root) / "u2" / config.run_id
    # Verify no ".." in path
    try:
        resolved = run_dir.resolve()
        if ".." in str(run_dir):
            return CheckResult(
                id="PF-601", stage=6, status=CheckStatus.FAIL,
                message="Run isolation violation: path traversal detected"
            )
        return CheckResult(
            id="PF-601", stage=6, status=CheckStatus.PASS,
            message="Run directory isolation verified",
            data={"run_dir": str(resolved)}
        )
    except Exception as e:
        return CheckResult(
            id="PF-601", stage=6, status=CheckStatus.FAIL,
            message=f"Run isolation check failed: {e}"
        )

def pf_602_env_isolation(config: PreflightConfig) -> CheckResult:
    """PF-602: Verify environment directory isolation (A1/A2/A3/A4/base)."""
    env_ids = ["A1", "A2", "A3", "A4", "baseline"]

    for env_id in env_ids:
        env_dir = config.run_dir / env_id
        if env_dir.exists() and env_dir.is_symlink():
            return CheckResult(
                id="PF-602", stage=6, status=CheckStatus.FAIL,
                message=f"Env isolation violation: {env_id} is a symlink"
            )

    return CheckResult(
        id="PF-602", stage=6, status=CheckStatus.PASS,
        message="Environment directory isolation verified",
        data={"environments": env_ids}
    )

def pf_603_redis_namespace(config: PreflightConfig) -> CheckResult:
    """PF-603: Confirm Redis key namespace prefix configured."""
    redis_url = os.environ.get("REDIS_URL")
    if not redis_url:
        return CheckResult(
            id="PF-603", stage=6, status=CheckStatus.WARN,
            message="REDIS_URL not set; Redis namespace check skipped"
        )

    # Namespace is enforced by code convention: ml:u2:{run_id}:{env_id}:
    return CheckResult(
        id="PF-603", stage=6, status=CheckStatus.PASS,
        message=f"Redis namespace prefix: ml:u2:{config.run_id}:",
        data={"namespace_prefix": f"ml:u2:{config.run_id}:"}
    )

def pf_604_db_run_scoping(config: PreflightConfig) -> CheckResult:
    """PF-604: Confirm DB run_id scoping is enforced."""
    db_url = os.environ.get("DATABASE_URL")
    if not db_url:
        return CheckResult(
            id="PF-604", stage=6, status=CheckStatus.WARN,
            message="DATABASE_URL not set; DB scoping check skipped"
        )

    # DB scoping is enforced by schema design (run_id column in all U2 tables)
    return CheckResult(
        id="PF-604", stage=6, status=CheckStatus.PASS,
        message="DB run_id scoping verified (by schema design)"
    )

def pf_605_no_global_cache(config: PreflightConfig) -> CheckResult:
    """PF-605: Verify no global/shared cache state."""
    # Check for global cache patterns in RFL code
    scan_path = Path("backend/rfl")
    if not scan_path.exists():
        return CheckResult(
            id="PF-605", stage=6, status=CheckStatus.WARN,
            message="backend/rfl not found; skipping global cache check"
        )

    global_patterns = [
        r"GLOBAL_CACHE\s*=",
        r"_cache\s*=\s*\{\}",
        r"@lru_cache",
        r"@cache\b",
    ]

    violations = []
    for py_file in scan_path.rglob("*.py"):
        try:
            content = py_file.read_text(encoding="utf-8")
            for pattern in global_patterns:
                if re.search(pattern, content):
                    violations.append(str(py_file))
                    break
        except Exception:
            pass

    if violations:
        return CheckResult(
            id="PF-605", stage=6, status=CheckStatus.WARN,
            message=f"Potential global cache: {len(violations)} file(s)",
            data={"files": violations[:5]}
        )

    return CheckResult(
        id="PF-605", stage=6, status=CheckStatus.PASS,
        message="No global cache patterns detected"
    )


# =============================================================================
# CHECK IMPLEMENTATIONS - STAGE 7: INFRASTRUCTURE CONNECTIVITY
# =============================================================================

def pf_701_db_connectivity(config: PreflightConfig) -> CheckResult:
    """PF-701: Test DATABASE_URL connectivity (if uplift_experiment)."""
    db_url = os.environ.get("DATABASE_URL")
    mode = os.environ.get("RFL_ENV_MODE")

    if mode != "uplift_experiment":
        return CheckResult(
            id="PF-701", stage=7, status=CheckStatus.SKIP,
            message="Skipped: not in uplift_experiment mode"
        )

    if not db_url:
        return CheckResult(
            id="PF-701", stage=7, status=CheckStatus.FAIL,
            message="DATABASE_URL not set"
        )

    try:
        import psycopg2
        conn = psycopg2.connect(db_url)
        conn.close()
        return CheckResult(
            id="PF-701", stage=7, status=CheckStatus.PASS,
            message="Database connectivity verified"
        )
    except ImportError:
        return CheckResult(
            id="PF-701", stage=7, status=CheckStatus.WARN,
            message="psycopg2 not installed; DB check skipped"
        )
    except Exception as e:
        return CheckResult(
            id="PF-701", stage=7, status=CheckStatus.FAIL,
            message=f"Database unreachable: {e}"
        )

def pf_702_db_schema(config: PreflightConfig) -> CheckResult:
    """PF-702: Verify database schema has U2 tables."""
    db_url = os.environ.get("DATABASE_URL")
    if not db_url:
        return CheckResult(
            id="PF-702", stage=7, status=CheckStatus.SKIP,
            message="Skipped: DATABASE_URL not set"
        )

    required_tables = ["u2_experiments", "u2_cycles", "u2_policy_weights"]

    try:
        import psycopg2
        conn = psycopg2.connect(db_url)
        cur = conn.cursor()
        cur.execute("""
            SELECT table_name FROM information_schema.tables
            WHERE table_schema = 'public'
        """)
        existing = {row[0] for row in cur.fetchall()}
        conn.close()

        missing = [t for t in required_tables if t not in existing]
        if missing:
            return CheckResult(
                id="PF-702", stage=7, status=CheckStatus.WARN,
                message=f"U2 schema not migrated: missing {missing}",
                data={"missing_tables": missing}
            )
        return CheckResult(
            id="PF-702", stage=7, status=CheckStatus.PASS,
            message="U2 schema tables present"
        )
    except ImportError:
        return CheckResult(
            id="PF-702", stage=7, status=CheckStatus.WARN,
            message="psycopg2 not installed; schema check skipped"
        )
    except Exception as e:
        return CheckResult(
            id="PF-702", stage=7, status=CheckStatus.FAIL,
            message=f"Schema check failed: {e}"
        )

def pf_703_redis_connectivity(config: PreflightConfig) -> CheckResult:
    """PF-703: Test REDIS_URL connectivity (if uplift_experiment)."""
    redis_url = os.environ.get("REDIS_URL")
    mode = os.environ.get("RFL_ENV_MODE")

    if mode != "uplift_experiment":
        return CheckResult(
            id="PF-703", stage=7, status=CheckStatus.SKIP,
            message="Skipped: not in uplift_experiment mode"
        )

    if not redis_url:
        return CheckResult(
            id="PF-703", stage=7, status=CheckStatus.WARN,
            message="REDIS_URL not set"
        )

    try:
        import redis
        r = redis.from_url(redis_url)
        r.ping()
        return CheckResult(
            id="PF-703", stage=7, status=CheckStatus.PASS,
            message="Redis connectivity verified"
        )
    except ImportError:
        return CheckResult(
            id="PF-703", stage=7, status=CheckStatus.WARN,
            message="redis-py not installed; Redis check skipped"
        )
    except Exception as e:
        return CheckResult(
            id="PF-703", stage=7, status=CheckStatus.FAIL,
            message=f"Redis unreachable: {e}"
        )

def pf_704_redis_clean(config: PreflightConfig) -> CheckResult:
    """PF-704: Verify Redis is empty or has only this run's keys."""
    redis_url = os.environ.get("REDIS_URL")
    if not redis_url:
        return CheckResult(
            id="PF-704", stage=7, status=CheckStatus.SKIP,
            message="Skipped: REDIS_URL not set"
        )

    try:
        import redis
        r = redis.from_url(redis_url)

        # Check for keys from other runs
        prefix = f"ml:u2:"
        all_keys = r.keys(f"{prefix}*")
        our_prefix = f"ml:u2:{config.run_id}:"
        stale_keys = [k for k in all_keys if not k.decode().startswith(our_prefix)]

        if stale_keys:
            return CheckResult(
                id="PF-704", stage=7, status=CheckStatus.WARN,
                message=f"Stale Redis keys found: {len(stale_keys)}",
                data={"stale_count": len(stale_keys)}
            )
        return CheckResult(
            id="PF-704", stage=7, status=CheckStatus.PASS,
            message="Redis is clean or contains only this run's keys"
        )
    except ImportError:
        return CheckResult(
            id="PF-704", stage=7, status=CheckStatus.WARN,
            message="redis-py not installed; Redis clean check skipped"
        )
    except Exception as e:
        return CheckResult(
            id="PF-704", stage=7, status=CheckStatus.WARN,
            message=f"Redis clean check failed: {e}"
        )

def pf_705_lean_available(config: PreflightConfig) -> CheckResult:
    """PF-705: Check Lean verifier availability (optional)."""
    lean_dir = os.environ.get("LEAN_PROJECT_DIR")

    if not lean_dir:
        return CheckResult(
            id="PF-705", stage=7, status=CheckStatus.WARN,
            message="Lean not available: LEAN_PROJECT_DIR not set"
        )

    lake_bin = Path(lean_dir) / ".lake" / "bin" / "lake"
    if not lake_bin.exists():
        lake_bin = Path(lean_dir) / ".lake" / "bin" / "lake.exe"

    if lake_bin.exists():
        return CheckResult(
            id="PF-705", stage=7, status=CheckStatus.PASS,
            message=f"Lean available: {lean_dir}"
        )

    return CheckResult(
        id="PF-705", stage=7, status=CheckStatus.WARN,
        message="Lean not available: lake binary not found"
    )


# =============================================================================
# CHECK IMPLEMENTATIONS - STAGE 8: NFR COMPLIANCE
# =============================================================================

def pf_801_nfr001(config: PreflightConfig) -> CheckResult:
    """PF-801: NFR-001 Mode declaration verified."""
    mode = os.environ.get("RFL_ENV_MODE")
    if mode:
        return CheckResult(
            id="PF-801", stage=8, status=CheckStatus.PASS,
            message="NFR-001: Mode declaration verified"
        )
    return CheckResult(
        id="PF-801", stage=8, status=CheckStatus.FAIL,
        message="NFR-001 failed: Mode not declared"
    )

def pf_802_nfr002(config: PreflightConfig) -> CheckResult:
    """PF-802: NFR-002 Cache isolation verified."""
    cache_root = os.environ.get("MATHLEDGER_CACHE_ROOT")
    if cache_root and Path(cache_root).exists():
        return CheckResult(
            id="PF-802", stage=8, status=CheckStatus.PASS,
            message="NFR-002: Cache isolation verified"
        )
    return CheckResult(
        id="PF-802", stage=8, status=CheckStatus.FAIL,
        message="NFR-002 failed: Cache isolation not verified"
    )

def pf_803_nfr003(config: PreflightConfig) -> CheckResult:
    """PF-803: NFR-003 Seed derivation verified."""
    seed = os.environ.get("U2_MASTER_SEED")
    if seed and len(seed) == 64:
        return CheckResult(
            id="PF-803", stage=8, status=CheckStatus.PASS,
            message="NFR-003: Seed derivation verified"
        )
    return CheckResult(
        id="PF-803", stage=8, status=CheckStatus.FAIL,
        message="NFR-003 failed: Seed derivation not verified"
    )

def pf_804_nfr004(config: PreflightConfig) -> CheckResult:
    """PF-804: NFR-004 Structured logging configured."""
    export_root = os.environ.get("MATHLEDGER_EXPORT_ROOT")
    if export_root:
        return CheckResult(
            id="PF-804", stage=8, status=CheckStatus.PASS,
            message="NFR-004: Structured logging configured"
        )
    return CheckResult(
        id="PF-804", stage=8, status=CheckStatus.FAIL,
        message="NFR-004 failed: Logging not configured"
    )

def pf_805_nfr005(config: PreflightConfig) -> CheckResult:
    """PF-805: NFR-005 Snapshot integrity mechanism ready."""
    snapshot_root = os.environ.get("MATHLEDGER_SNAPSHOT_ROOT")
    if snapshot_root and Path(snapshot_root).exists():
        return CheckResult(
            id="PF-805", stage=8, status=CheckStatus.PASS,
            message="NFR-005: Snapshot integrity mechanism ready"
        )
    return CheckResult(
        id="PF-805", stage=8, status=CheckStatus.FAIL,
        message="NFR-005 failed: Snapshot mechanism not ready"
    )

def pf_806_nfr006(config: PreflightConfig) -> CheckResult:
    """PF-806: NFR-006 Determinism verification enabled."""
    seed = os.environ.get("U2_MASTER_SEED")
    if seed:
        return CheckResult(
            id="PF-806", stage=8, status=CheckStatus.PASS,
            message="NFR-006: Determinism verification enabled"
        )
    return CheckResult(
        id="PF-806", stage=8, status=CheckStatus.FAIL,
        message="NFR-006 failed: Determinism not enabled"
    )

def pf_807_nfr007(config: PreflightConfig) -> CheckResult:
    """PF-807: NFR-007 No banned randomness detected."""
    # Reuse result from PF-405
    result = pf_405_no_banned_randomness(config)
    if result.status == CheckStatus.PASS:
        return CheckResult(
            id="PF-807", stage=8, status=CheckStatus.PASS,
            message="NFR-007: No banned randomness detected"
        )
    return CheckResult(
        id="PF-807", stage=8, status=CheckStatus.FAIL,
        message="NFR-007 failed: Banned randomness detected"
    )


# =============================================================================
# CHECK IMPLEMENTATIONS - STAGE 9: MODE TRANSITION & LOCK
# =============================================================================

def pf_901_stages_passed(config: PreflightConfig, stage_results: List[StageResult]) -> CheckResult:
    """PF-901: All previous stages passed."""
    failed_stages = [s for s in stage_results if not s.passed]
    if failed_stages:
        return CheckResult(
            id="PF-901", stage=9, status=CheckStatus.FAIL,
            message=f"Pre-flight incomplete: {len(failed_stages)} stage(s) failed",
            data={"failed_stages": [s.stage for s in failed_stages]}
        )
    return CheckResult(
        id="PF-901", stage=9, status=CheckStatus.PASS,
        message="All previous stages passed"
    )

def pf_902_lock_file_created(config: PreflightConfig) -> CheckResult:
    """PF-902: Create atomic mode lock file."""
    if config.dry_run:
        return CheckResult(
            id="PF-902", stage=9, status=CheckStatus.PASS,
            message="Dry run: would create lock file"
        )

    lock_path = config.run_dir / ".mode_lock"
    lock_data = {
        "mode": "uplift_experiment",
        "locked_at": datetime.now(timezone.utc).isoformat(),
        "locked_by": config.operator_id,
        "prereg_hash": None,
        "run_id": config.run_id,
        "reversible": False,
    }

    # Compute prereg hash if available
    if config.prereg_file and config.prereg_file.exists():
        lock_data["prereg_hash"] = f"sha256:{hashlib.sha256(config.prereg_file.read_bytes()).hexdigest()}"

    try:
        temp_path = lock_path.with_suffix(".tmp")
        temp_path.write_text(json.dumps(lock_data, indent=2))
        temp_path.rename(lock_path)
        return CheckResult(
            id="PF-902", stage=9, status=CheckStatus.PASS,
            message=f"Lock file created: {lock_path}",
            data={"lock_path": str(lock_path)}
        )
    except Exception as e:
        return CheckResult(
            id="PF-902", stage=9, status=CheckStatus.FAIL,
            message=f"Lock creation failed: {e}"
        )

def pf_903_commitment_written(config: PreflightConfig) -> CheckResult:
    """PF-903: Write cryptographic commitment to lock."""
    lock_path = config.run_dir / ".mode_lock"

    if config.dry_run:
        return CheckResult(
            id="PF-903", stage=9, status=CheckStatus.PASS,
            message="Dry run: would write commitment"
        )

    if not lock_path.exists():
        return CheckResult(
            id="PF-903", stage=9, status=CheckStatus.SKIP,
            message="Lock file not created yet"
        )

    try:
        lock_data = json.loads(lock_path.read_text())

        # Compute commitment
        commitment_input = json.dumps({
            "mode": lock_data.get("mode"),
            "locked_at": lock_data.get("locked_at"),
            "locked_by": lock_data.get("locked_by"),
            "prereg_hash": lock_data.get("prereg_hash"),
            "run_id": lock_data.get("run_id"),
            "reversible": lock_data.get("reversible"),
        }, sort_keys=True)
        commitment = f"sha256:{hashlib.sha256(commitment_input.encode()).hexdigest()}"

        lock_data["commitment"] = commitment
        lock_path.write_text(json.dumps(lock_data, indent=2))

        return CheckResult(
            id="PF-903", stage=9, status=CheckStatus.PASS,
            message="Commitment written to lock file",
            data={"commitment_prefix": commitment[:32]}
        )
    except Exception as e:
        return CheckResult(
            id="PF-903", stage=9, status=CheckStatus.FAIL,
            message=f"Commitment failed: {e}"
        )

def pf_904_lock_readonly(config: PreflightConfig) -> CheckResult:
    """PF-904: Set lock file to read-only."""
    if config.dry_run:
        return CheckResult(
            id="PF-904", stage=9, status=CheckStatus.PASS,
            message="Dry run: would set lock to read-only"
        )

    lock_path = config.run_dir / ".mode_lock"
    if not lock_path.exists():
        return CheckResult(
            id="PF-904", stage=9, status=CheckStatus.SKIP,
            message="Lock file not created yet"
        )

    try:
        # Make read-only (cross-platform)
        import stat
        lock_path.chmod(stat.S_IRUSR | stat.S_IRGRP | stat.S_IROTH)
        return CheckResult(
            id="PF-904", stage=9, status=CheckStatus.PASS,
            message="Lock file set to read-only"
        )
    except Exception as e:
        return CheckResult(
            id="PF-904", stage=9, status=CheckStatus.WARN,
            message=f"Lock permission failed: {e}"
        )

def pf_905_post_conditions(config: PreflightConfig) -> CheckResult:
    """PF-905: Verify lock file post-conditions."""
    lock_path = config.run_dir / ".mode_lock"

    if config.dry_run:
        return CheckResult(
            id="PF-905", stage=9, status=CheckStatus.PASS,
            message="Dry run: post-conditions would be verified"
        )

    if not lock_path.exists():
        return CheckResult(
            id="PF-905", stage=9, status=CheckStatus.FAIL,
            message="Post-condition failed: lock file missing"
        )

    try:
        lock_data = json.loads(lock_path.read_text())
        required_fields = ["mode", "locked_at", "locked_by", "run_id", "reversible", "commitment"]
        missing = [f for f in required_fields if f not in lock_data]

        if missing:
            return CheckResult(
                id="PF-905", stage=9, status=CheckStatus.FAIL,
                message=f"Post-condition failed: missing fields {missing}",
                data={"missing_fields": missing}
            )

        if lock_data.get("reversible") is not False:
            return CheckResult(
                id="PF-905", stage=9, status=CheckStatus.FAIL,
                message="Post-condition failed: reversible must be false"
            )

        return CheckResult(
            id="PF-905", stage=9, status=CheckStatus.PASS,
            message="All post-conditions verified"
        )
    except Exception as e:
        return CheckResult(
            id="PF-905", stage=9, status=CheckStatus.FAIL,
            message=f"Post-condition verification failed: {e}"
        )

def pf_906_audit_log_entry(config: PreflightConfig) -> CheckResult:
    """PF-906: Log transition to audit trail."""
    if config.dry_run:
        return CheckResult(
            id="PF-906", stage=9, status=CheckStatus.PASS,
            message="Dry run: would log transition"
        )

    export_root = os.environ.get("MATHLEDGER_EXPORT_ROOT")
    if not export_root:
        return CheckResult(
            id="PF-906", stage=9, status=CheckStatus.WARN,
            message="Audit log failed: MATHLEDGER_EXPORT_ROOT not set"
        )

    try:
        audit_log_path = Path(export_root) / "u2" / config.run_id / "audit.jsonl"
        audit_log_path.parent.mkdir(parents=True, exist_ok=True)

        entry = {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "event": "MODE_TRANSITION",
            "run_id": config.run_id,
            "operator_id": config.operator_id,
            "mode": "uplift_experiment",
        }

        with open(audit_log_path, "a") as f:
            f.write(json.dumps(entry) + "\n")

        return CheckResult(
            id="PF-906", stage=9, status=CheckStatus.PASS,
            message=f"Transition logged: {audit_log_path}",
            data={"audit_log_path": str(audit_log_path)}
        )
    except Exception as e:
        return CheckResult(
            id="PF-906", stage=9, status=CheckStatus.WARN,
            message=f"Audit log failed: {e}"
        )


# =============================================================================
# CHECK IMPLEMENTATIONS - STAGE 10: AUDIT ELIGIBILITY GATE
# =============================================================================

def pf_1001_env_report(config: PreflightConfig, all_checks: List[CheckResult]) -> CheckResult:
    """PF-1001: Generate Pre-Run Environment Report (JSON)."""
    if config.dry_run:
        return CheckResult(
            id="PF-1001", stage=10, status=CheckStatus.PASS,
            message="Dry run: would generate env report"
        )

    export_root = os.environ.get("MATHLEDGER_EXPORT_ROOT")
    if not export_root:
        return CheckResult(
            id="PF-1001", stage=10, status=CheckStatus.FAIL,
            message="Report generation failed: MATHLEDGER_EXPORT_ROOT not set"
        )

    try:
        report_path = Path(export_root) / "u2" / config.run_id / "env_report.json"
        report_path.parent.mkdir(parents=True, exist_ok=True)

        env_snapshot = {
            k: os.environ.get(k)
            for k in [
                "RFL_ENV_MODE", "U2_RUN_ID", "U2_CYCLE_LIMIT",
                "MATHLEDGER_CACHE_ROOT", "MATHLEDGER_SNAPSHOT_ROOT",
                "MATHLEDGER_EXPORT_ROOT"
            ]
        }
        # Redact seed
        if os.environ.get("U2_MASTER_SEED"):
            env_snapshot["U2_MASTER_SEED"] = "[REDACTED]"

        report = {
            "$schema": "https://mathledger.io/schemas/u2-env-report-v1.json",
            "run_id": config.run_id,
            "generated_at": datetime.now(timezone.utc).isoformat(),
            "env_valid": all(c.status != CheckStatus.FAIL for c in all_checks),
            "summary": {
                "total": len(all_checks),
                "passed": sum(1 for c in all_checks if c.status == CheckStatus.PASS),
                "failed": sum(1 for c in all_checks if c.status == CheckStatus.FAIL),
                "warnings": sum(1 for c in all_checks if c.status == CheckStatus.WARN),
            },
            "checks": [c.to_dict() for c in all_checks],
            "environment": env_snapshot,
        }

        report_path.write_text(json.dumps(report, indent=2))

        return CheckResult(
            id="PF-1001", stage=10, status=CheckStatus.PASS,
            message=f"Environment report generated: {report_path}",
            data={"report_path": str(report_path)}
        )
    except Exception as e:
        return CheckResult(
            id="PF-1001", stage=10, status=CheckStatus.FAIL,
            message=f"Report generation failed: {e}"
        )

def pf_1002_prereg_copied(config: PreflightConfig) -> CheckResult:
    """PF-1002: Copy preregistration file to export directory."""
    if config.dry_run:
        return CheckResult(
            id="PF-1002", stage=10, status=CheckStatus.PASS,
            message="Dry run: would copy prereg file"
        )

    if not config.prereg_file or not config.prereg_file.exists():
        return CheckResult(
            id="PF-1002", stage=10, status=CheckStatus.SKIP,
            message="Prereg file not available"
        )

    export_root = os.environ.get("MATHLEDGER_EXPORT_ROOT")
    if not export_root:
        return CheckResult(
            id="PF-1002", stage=10, status=CheckStatus.FAIL,
            message="Prereg copy failed: MATHLEDGER_EXPORT_ROOT not set"
        )

    try:
        dest_path = Path(export_root) / "u2" / config.run_id / config.prereg_file.name
        dest_path.parent.mkdir(parents=True, exist_ok=True)
        shutil.copy2(config.prereg_file, dest_path)

        return CheckResult(
            id="PF-1002", stage=10, status=CheckStatus.PASS,
            message=f"Prereg copied: {dest_path}",
            data={"dest_path": str(dest_path)}
        )
    except Exception as e:
        return CheckResult(
            id="PF-1002", stage=10, status=CheckStatus.FAIL,
            message=f"Prereg copy failed: {e}"
        )

def pf_1003_manifest_created(config: PreflightConfig, all_checks: List[CheckResult]) -> CheckResult:
    """PF-1003: Create manifest.json with run configuration."""
    if config.dry_run:
        return CheckResult(
            id="PF-1003", stage=10, status=CheckStatus.PASS,
            message="Dry run: would create manifest"
        )

    export_root = os.environ.get("MATHLEDGER_EXPORT_ROOT")
    if not export_root:
        return CheckResult(
            id="PF-1003", stage=10, status=CheckStatus.FAIL,
            message="Manifest creation failed: MATHLEDGER_EXPORT_ROOT not set"
        )

    try:
        manifest_path = Path(export_root) / "u2" / config.run_id / "manifest.json"
        manifest_path.parent.mkdir(parents=True, exist_ok=True)

        pass_count = sum(1 for c in all_checks if c.status == CheckStatus.PASS)
        fail_count = sum(1 for c in all_checks if c.status == CheckStatus.FAIL)
        warn_count = sum(1 for c in all_checks if c.status == CheckStatus.WARN)
        is_eligible = fail_count == 0

        manifest = {
            "run_id": config.run_id,
            "experiment_id": config.experiment_id,
            "status": AuditEligibility.AUDIT_ELIGIBLE.value if is_eligible else AuditEligibility.NOT_ELIGIBLE.value,
            "eligibility": {
                "claude_n_env_valid": is_eligible,
                "claude_n_report_path": "env_report.json",
                "all_preflight_passed": is_eligible,
                "preflight_check_count": len(all_checks),
                "preflight_pass_count": pass_count,
                "preflight_fail_count": fail_count,
                "preflight_warn_count": warn_count,
            },
            "timestamps": {
                "preflight_completed": datetime.now(timezone.utc).isoformat(),
                "audit_eligible_at": datetime.now(timezone.utc).isoformat() if is_eligible else None,
            },
            "operator": {
                "id": config.operator_id,
                "confirmed_phase2": config.confirm_phase2,
            },
        }

        if not is_eligible:
            failed_checks = [c.id for c in all_checks if c.status == CheckStatus.FAIL]
            manifest["eligibility"]["reasons"] = failed_checks

        manifest_path.write_text(json.dumps(manifest, indent=2))

        return CheckResult(
            id="PF-1003", stage=10, status=CheckStatus.PASS,
            message=f"Manifest created: {manifest_path}",
            data={"manifest_path": str(manifest_path)}
        )
    except Exception as e:
        return CheckResult(
            id="PF-1003", stage=10, status=CheckStatus.FAIL,
            message=f"Manifest creation failed: {e}"
        )

def pf_1004_audit_eligible(config: PreflightConfig, all_checks: List[CheckResult]) -> CheckResult:
    """PF-1004: Flag run as AUDIT-ELIGIBLE."""
    fail_count = sum(1 for c in all_checks if c.status == CheckStatus.FAIL)

    if fail_count > 0:
        return CheckResult(
            id="PF-1004", stage=10, status=CheckStatus.FAIL,
            message=f"NOT_ELIGIBLE: {fail_count} critical check(s) failed",
            data={"failed_checks": [c.id for c in all_checks if c.status == CheckStatus.FAIL]}
        )

    return CheckResult(
        id="PF-1004", stage=10, status=CheckStatus.PASS,
        message="Run flagged as AUDIT_ELIGIBLE"
    )

def pf_1005_success_return(config: PreflightConfig, all_checks: List[CheckResult]) -> CheckResult:
    """PF-1005: Return success to U2 runner."""
    fail_count = sum(1 for c in all_checks if c.status == CheckStatus.FAIL)

    if fail_count > 0:
        return CheckResult(
            id="PF-1005", stage=10, status=CheckStatus.FAIL,
            message=f"Pre-flight bundle failed: {fail_count} error(s)"
        )

    return CheckResult(
        id="PF-1005", stage=10, status=CheckStatus.PASS,
        message="Pre-flight bundle completed successfully"
    )


# =============================================================================
# PIPELINE ORCHESTRATION
# =============================================================================

def run_stage(
    stage_num: int,
    checks: List[Callable[[PreflightConfig], CheckResult]],
    config: PreflightConfig,
    stage_results: Optional[List[StageResult]] = None,
    all_checks: Optional[List[CheckResult]] = None,
) -> StageResult:
    """Run all checks for a single stage."""
    results = []
    for check_fn in checks:
        # Handle special checks that need extra args
        if check_fn == pf_901_stages_passed and stage_results:
            result = check_fn(config, stage_results)
        elif check_fn in (pf_1001_env_report, pf_1003_manifest_created,
                          pf_1004_audit_eligible, pf_1005_success_return) and all_checks:
            result = check_fn(config, all_checks)
        else:
            result = check_fn(config)
        results.append(result)

    passed = all(r.status != CheckStatus.FAIL for r in results)
    return StageResult(
        stage=stage_num,
        name=STAGE_NAMES[stage_num],
        checks=results,
        passed=passed
    )

def run_preflight_bundle(config: PreflightConfig) -> Tuple[List[StageResult], List[CheckResult]]:
    """Execute the complete 10-stage pre-flight bundle."""

    stage_results: List[StageResult] = []
    all_checks: List[CheckResult] = []

    # Stage 1: Environment Bootstrap
    stage1 = run_stage(1, [
        pf_101_env_mode_set,
        pf_102_mode_value_valid,
        pf_103_lock_file_check,
        pf_104_operator_identity,
        pf_105_phase2_confirmation,
    ], config)
    stage_results.append(stage1)
    all_checks.extend(stage1.checks)

    # Stage 2: Preregistration Binding
    stage2 = run_stage(2, [
        pf_201_prereg_file_exists,
        pf_202_prereg_hash_computed,
        pf_203_seed_matches_prereg,
        pf_204_prereg_valid_yaml,
        pf_205_experiment_id_match,
    ], config)
    stage_results.append(stage2)
    all_checks.extend(stage2.checks)

    # Stage 3: Directory & Filesystem
    stage3 = run_stage(3, [
        pf_301_cache_root_writable,
        pf_302_snapshot_root_writable,
        pf_303_export_root_writable,
        pf_304_no_symlinks,
        pf_305_disk_space,
        pf_306_run_dirs_created,
        pf_307_run_id_valid,
    ], config)
    stage_results.append(stage3)
    all_checks.extend(stage3.checks)

    # Stage 4: PRNG & Determinism
    stage4 = run_stage(4, [
        pf_401_seed_format,
        pf_402_seed_derivation,
        pf_403_prng_init,
        pf_404_initial_checkpoint,
        pf_405_no_banned_randomness,
        pf_406_python_hash_seed,
    ], config)
    stage_results.append(stage4)
    all_checks.extend(stage4.checks)

    # Stage 5: Budget & Resource Limits
    stage5 = run_stage(5, [
        pf_501_cycle_limit_valid,
        pf_502_snapshot_interval,
        pf_503_slice_budgets,
        pf_504_timeout_configured,
        pf_505_memory_budget,
    ], config)
    stage_results.append(stage5)
    all_checks.extend(stage5.checks)

    # Stage 6: Isolation Verification
    stage6 = run_stage(6, [
        pf_601_run_isolation,
        pf_602_env_isolation,
        pf_603_redis_namespace,
        pf_604_db_run_scoping,
        pf_605_no_global_cache,
    ], config)
    stage_results.append(stage6)
    all_checks.extend(stage6.checks)

    # Stage 7: Infrastructure Connectivity
    stage7 = run_stage(7, [
        pf_701_db_connectivity,
        pf_702_db_schema,
        pf_703_redis_connectivity,
        pf_704_redis_clean,
        pf_705_lean_available,
    ], config)
    stage_results.append(stage7)
    all_checks.extend(stage7.checks)

    # Stage 8: NFR Compliance
    stage8 = run_stage(8, [
        pf_801_nfr001,
        pf_802_nfr002,
        pf_803_nfr003,
        pf_804_nfr004,
        pf_805_nfr005,
        pf_806_nfr006,
        pf_807_nfr007,
    ], config)
    stage_results.append(stage8)
    all_checks.extend(stage8.checks)

    # Stage 9: Mode Transition & Lock (uses stage_results)
    stage9_checks = [
        lambda c: pf_901_stages_passed(c, stage_results),
        pf_902_lock_file_created,
        pf_903_commitment_written,
        pf_904_lock_readonly,
        pf_905_post_conditions,
        pf_906_audit_log_entry,
    ]
    stage9 = StageResult(stage=9, name=STAGE_NAMES[9], checks=[], passed=True)
    for check_fn in stage9_checks:
        result = check_fn(config)
        stage9.checks.append(result)
        all_checks.append(result)
    stage9.passed = all(r.status != CheckStatus.FAIL for r in stage9.checks)
    stage_results.append(stage9)

    # Stage 10: Audit Eligibility Gate (uses all_checks)
    stage10_checks_results = []

    result_1001 = pf_1001_env_report(config, all_checks)
    stage10_checks_results.append(result_1001)
    all_checks.append(result_1001)

    result_1002 = pf_1002_prereg_copied(config)
    stage10_checks_results.append(result_1002)
    all_checks.append(result_1002)

    result_1003 = pf_1003_manifest_created(config, all_checks)
    stage10_checks_results.append(result_1003)
    all_checks.append(result_1003)

    result_1004 = pf_1004_audit_eligible(config, all_checks)
    stage10_checks_results.append(result_1004)
    all_checks.append(result_1004)

    result_1005 = pf_1005_success_return(config, all_checks)
    stage10_checks_results.append(result_1005)
    all_checks.append(result_1005)

    stage10 = StageResult(
        stage=10,
        name=STAGE_NAMES[10],
        checks=stage10_checks_results,
        passed=all(r.status != CheckStatus.FAIL for r in stage10_checks_results)
    )
    stage_results.append(stage10)

    return stage_results, all_checks

def generate_bundle_report(
    config: PreflightConfig,
    stage_results: List[StageResult],
    all_checks: List[CheckResult],
) -> Dict[str, Any]:
    """Generate the final pre-flight bundle report."""
    fail_count = sum(1 for c in all_checks if c.status == CheckStatus.FAIL)
    warn_count = sum(1 for c in all_checks if c.status == CheckStatus.WARN)
    pass_count = sum(1 for c in all_checks if c.status == CheckStatus.PASS)

    is_eligible = fail_count == 0

    return {
        "$schema": "https://mathledger.io/schemas/u2-preflight-bundle-v1.json",
        "bundle_id": "PFB-001",
        "run_id": config.run_id,
        "generated_at": datetime.now(timezone.utc).isoformat(),
        "audit_eligible": {
            "status": AuditEligibility.AUDIT_ELIGIBLE.value if is_eligible else AuditEligibility.NOT_ELIGIBLE.value,
            "reasons": [c.id for c in all_checks if c.status == CheckStatus.FAIL] if not is_eligible else [],
        },
        "summary": {
            "total_stages": len(stage_results),
            "stages_passed": sum(1 for s in stage_results if s.passed),
            "stages_failed": sum(1 for s in stage_results if not s.passed),
            "total_checks": len(all_checks),
            "checks_passed": pass_count,
            "checks_failed": fail_count,
            "checks_warnings": warn_count,
        },
        "stages": [s.to_dict() for s in stage_results],
        "config": {
            "run_dir": str(config.run_dir),
            "run_id": config.run_id,
            "prereg_file": str(config.prereg_file) if config.prereg_file else None,
            "operator_id": config.operator_id,
            "dry_run": config.dry_run,
        },
    }


# =============================================================================
# V2 ENHANCEMENTS: AUDIT TIMELINE & EVIDENCE BRIDGE
# =============================================================================

# --- Schema Versions ---
PREFLIGHT_SNAPSHOT_SCHEMA_VERSION = "1.0.0"
PREFLIGHT_TIMELINE_SCHEMA_VERSION = "1.0.0"
ADMISSIBILITY_SUMMARY_SCHEMA_VERSION = "1.0.0"


def build_preflight_bundle_snapshot(bundle_report: Dict[str, Any]) -> Dict[str, Any]:
    """
    TASK 1: Build a stable, minimal snapshot from a bundle report.

    This produces a compact representation suitable for:
    - Inclusion in multi-run timelines
    - Cross-run audit comparisons
    - MAAS promotion gate inputs

    Args:
        bundle_report: Full bundle report from generate_bundle_report()

    Returns:
        Minimal snapshot dict with:
        - schema_version: Version of this snapshot schema
        - run_id: The run identifier
        - eligibility: AUDIT_ELIGIBLE or NOT_ELIGIBLE
        - failed_pf_check_ids: List of PF-xxx IDs that failed
        - phase_confirmed: Whether --confirm-phase2 was provided
        - summary: Pass/fail/warn counts
        - timestamp: When snapshot was created
    """
    audit_eligible = bundle_report.get("audit_eligible", {})
    summary = bundle_report.get("summary", {})
    config = bundle_report.get("config", {})

    # Extract failed check IDs
    failed_check_ids = audit_eligible.get("reasons", [])

    # Determine if phase was confirmed by checking config or manifest
    # The config should have operator_id set if confirmed
    phase_confirmed = bool(config.get("operator_id"))

    return {
        "schema_version": PREFLIGHT_SNAPSHOT_SCHEMA_VERSION,
        "run_id": bundle_report.get("run_id"),
        "eligibility": audit_eligible.get("status", AuditEligibility.NOT_ELIGIBLE.value),
        "failed_pf_check_ids": failed_check_ids,
        "phase_confirmed": phase_confirmed,
        "summary": {
            "total_checks": summary.get("total_checks", 0),
            "checks_passed": summary.get("checks_passed", 0),
            "checks_failed": summary.get("checks_failed", 0),
            "checks_warnings": summary.get("checks_warnings", 0),
        },
        "timestamp": bundle_report.get("generated_at", datetime.now(timezone.utc).isoformat()),
    }

def build_preflight_bundle_timeline(
    snapshots: List[Dict[str, Any]],
    timeline_id: Optional[str] = None,
) -> Dict[str, Any]:
    """
    TASK 2: Build a multi-bundle audit timeline from snapshots.

    Aggregates multiple preflight snapshots into a timeline suitable for:
    - Multi-run audit reports
    - Trend analysis of preflight failures
    - Evidence of systematic improvements or regressions

    Args:
        snapshots: List of snapshots from build_preflight_bundle_snapshot()
        timeline_id: Optional identifier for this timeline

    Returns:
        Enriched timeline dict with new analytics fields.
    """
    if not snapshots:
        return {
            "schema_version": "1.1.0", # Bump version for new fields
            "timeline_id": timeline_id or f"timeline-{datetime.now(timezone.utc).strftime('%Y%m%d%H%M%S')}",
            "total_runs": 0,
            "eligibility_rate": 0.0,
            "eligibility_trend": "STABLE",
            "runs_per_category": {},
            "eligible_run_count": 0,
            "ineligible_run_count": 0,
            "ranked_failed_checks": [],
            "runs": [],
            "generated_at": datetime.now(timezone.utc).isoformat(),
        }

    # Sort by timestamp to ensure chronological order for trend analysis
    sorted_snapshots = sorted(
        snapshots,
        key=lambda s: s.get("timestamp", "")
    )

    # --- New: Aggregate runs per eligibility category ---
    from collections import Counter
    category_counts = Counter(s.get("eligibility") for s in sorted_snapshots)

    eligible_count = category_counts.get(AuditEligibility.AUDIT_ELIGIBLE.value, 0)
    ineligible_count = len(sorted_snapshots) - eligible_count

    # --- New: Rank failed check frequencies ---
    failed_checks_freq: Dict[str, int] = {}
    for snapshot in sorted_snapshots:
        for check_id in snapshot.get("failed_pf_check_ids", []):
            failed_checks_freq[check_id] = failed_checks_freq.get(check_id, 0) + 1
    
    ranked_failed_checks = sorted(
        failed_checks_freq.items(), key=lambda item: item[1], reverse=True
    )

    # --- New: Calculate eligibility rate trend ---
    trend = "STABLE"
    n = len(sorted_snapshots)
    if n >= 4: # Require at least 4 data points for a meaningful trend
        midpoint = n // 2
        first_half = sorted_snapshots[:midpoint]
        second_half = sorted_snapshots[midpoint:]

        rate1 = sum(1 for s in first_half if s.get("eligibility") == AuditEligibility.AUDIT_ELIGIBLE.value) / len(first_half)
        rate2 = sum(1 for s in second_half if s.get("eligibility") == AuditEligibility.AUDIT_ELIGIBLE.value) / len(second_half)
        
        # Define a tolerance for stability
        if rate2 > rate1 + 0.05:
            trend = "IMPROVING"
        elif rate2 < rate1 - 0.05:
            trend = "REGRESSING"

    # Build run summaries for timeline
    runs = []
    for snapshot in sorted_snapshots:
        runs.append({
            "run_id": snapshot.get("run_id"),
            "eligibility": snapshot.get("eligibility"),
            "failed_count": len(snapshot.get("failed_pf_check_ids", [])),
            "timestamp": snapshot.get("timestamp"),
        })

    eligibility_rate = (eligible_count / len(snapshots) * 100) if snapshots else 0.0

    return {
        "schema_version": "1.1.0", # Bumped version
        "timeline_id": timeline_id or f"timeline-{datetime.now(timezone.utc).strftime('%Y%m%d%H%M%S')}",
        "total_runs": len(snapshots),
        "eligible_run_count": eligible_count,
        "ineligible_run_count": ineligible_count,
        "eligibility_rate": round(eligibility_rate, 2),
        "eligibility_trend": trend,
        "runs_per_category": dict(category_counts),
        "ranked_failed_checks": ranked_failed_checks,
        "runs": runs,
        "generated_at": datetime.now(timezone.utc).isoformat(),
    }

def summarize_preflight_for_admissibility(snapshot: Dict[str, Any]) -> Dict[str, Any]:
    """
    TASK 3: Summarize a snapshot for MAAS (Multi-Agent Audit System) integration.

    Produces a minimal, stable contract for promotion gates and evidence admission.
    This is the bridge between preflight validation and the broader audit system.

    Args:
        snapshot: Snapshot from build_preflight_bundle_snapshot()

    Returns:
        Admissibility summary with:
        - schema_version: Version of this summary schema
        - is_audit_eligible: Boolean (True if AUDIT_ELIGIBLE)
        - has_phase2_confirmation: Boolean (True if phase was confirmed)
        - critical_pf_failures: List of failed PF-xxx check IDs
        - run_id: The run identifier
        - timestamp: From the snapshot
        - admissibility_verdict: ADMISSIBLE, INADMISSIBLE, or PROVISIONAL
        - verdict_reason: Human-readable explanation
    """
    eligibility = snapshot.get("eligibility", AuditEligibility.NOT_ELIGIBLE.value)
    is_eligible = eligibility == AuditEligibility.AUDIT_ELIGIBLE.value
    has_phase2 = snapshot.get("phase_confirmed", False)
    failed_checks = snapshot.get("failed_pf_check_ids", [])

    # Determine admissibility verdict
    if is_eligible and has_phase2:
        verdict = "ADMISSIBLE"
        reason = "All preflight checks passed with Phase II confirmation"
    elif is_eligible and not has_phase2:
        verdict = "PROVISIONAL"
        reason = "All preflight checks passed but Phase II confirmation missing"
    else:
        verdict = "INADMISSIBLE"
        if failed_checks:
            reason = f"Preflight failures: {', '.join(failed_checks[:5])}"
            if len(failed_checks) > 5:
                reason += f" (+{len(failed_checks) - 5} more)"
        else:
            reason = "Preflight validation incomplete"

    return {
        "schema_version": ADMISSIBILITY_SUMMARY_SCHEMA_VERSION,
        "is_audit_eligible": is_eligible,
        "has_phase2_confirmation": has_phase2,
        "critical_pf_failures": failed_checks,
        "run_id": snapshot.get("run_id"),
        "timestamp": snapshot.get("timestamp"),
        "admissibility_verdict": verdict,
        "verdict_reason": reason,
    }

def merge_preflight_bundle_and_engine(
    bundle_snapshot: Dict[str, Any],
    preflight_snapshot: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Designs a small adapter to merge this system's preflight bundle with J's preflight engine data.

    - Confirms both mark the run as eligible/ineligible consistently.
    - Surfaces conflicts explicitly in a structured way.

    Args:
        bundle_snapshot: Snapshot from this system's build_preflight_bundle_snapshot().
        preflight_snapshot: Snapshot from J's preflight engine.

    Returns:
        A new merged dictionary containing data from both, plus a consistency check.
    """
    # Assume J's snapshot has a similar 'eligibility' field.
    bundle_eligibility = bundle_snapshot.get("eligibility")
    engine_eligibility = preflight_snapshot.get("eligibility")

    is_consistent = bundle_eligibility == engine_eligibility

    consistency_report = {
        "status": "CONSISTENT" if is_consistent else "CONFLICT",
        "bundle_eligibility": bundle_eligibility,
        "engine_eligibility": engine_eligibility,
    }

    # Create a merged dictionary. Start with our bundle as the base.
    merged_data = bundle_snapshot.copy()
    
    # Add data from J's engine that isn't already in our snapshot.
    for key, value in preflight_snapshot.items():
        if key not in merged_data:
            merged_data[key] = value
            
    # Add the consistency report.
    merged_data["cross_engine_consistency"] = consistency_report

    # If there's a conflict, update the top-level eligibility to reflect it.
    if not is_consistent:
        merged_data["eligibility"] = "CONFLICT_DETECTED"

    return merged_data

def load_snapshots_from_directory(directory: Path) -> List[Dict[str, Any]]:
    """
    Helper: Load preflight snapshots from a directory containing bundle reports.

    Looks for preflight_bundle_report.json or *_snapshot.json files.

    Args:
        directory: Directory to scan for reports

    Returns:
        List of snapshots suitable for build_preflight_bundle_timeline()
    """
    snapshots = []

    if not directory.exists():
        return snapshots

    # Look for bundle reports and convert to snapshots
    for report_file in directory.rglob("preflight_bundle_report.json"):
        try:
            report = json.loads(report_file.read_text())
            snapshot = build_preflight_bundle_snapshot(report)
            snapshots.append(snapshot)
        except (json.JSONDecodeError, IOError):
            continue

    # Also look for pre-computed snapshots
    for snapshot_file in directory.rglob("*_preflight_snapshot.json"):
        try:
            snapshot = json.loads(snapshot_file.read_text())
            if "schema_version" in snapshot and "eligibility" in snapshot:
                snapshots.append(snapshot)
        except (json.JSONDecodeError, IOError):
            continue

    return snapshots

def summarize_preflight_bundle_for_global_health(timeline: Dict[str, Any]) -> Dict[str, Any]:
    """
    Distills an enriched timeline into a high-level summary for MAAS/Global Health monitoring.

    Args:
        timeline: An enriched timeline from build_preflight_bundle_timeline().

    Returns:
        A dictionary with key health indicators.
    """
    runs = timeline.get("runs", [])
    total_runs = timeline.get("total_runs", 0)
    
    # --- Calculate latest eligibility rate (last 20 runs) ---
    latest_runs = runs[-20:]
    latest_eligible_count = sum(1 for r in latest_runs if r.get("eligibility") == AuditEligibility.AUDIT_ELIGIBLE.value)
    latest_eligibility_rate = (latest_eligible_count / len(latest_runs) * 100) if latest_runs else 100.0

    # --- Identify recent ineligible runs (last 5) ---
    recent_ineligible_runs = [
        {"run_id": r["run_id"], "timestamp": r["timestamp"]}
        for r in reversed(runs) if r.get("eligibility") != AuditEligibility.AUDIT_ELIGIBLE.value
    ][:5]

    # --- Determine overall health status ---
    status = "OK"
    trend = timeline.get("eligibility_trend", "STABLE")
    
    if latest_eligibility_rate < 80.0:
        status = "BLOCKED"
    elif latest_eligibility_rate < 95.0 or trend == "REGRESSING":
        status = "DRIFTING"
    
    # If the last 5 runs all failed, status is BLOCKED
    if len(latest_runs) >= 5 and latest_eligible_count == 0:
        status = "BLOCKED"

    return {
        "schema_version": "1.0.0",
        "generated_at": datetime.now(timezone.utc).isoformat(),
        "timeline_id": timeline.get("timeline_id"),
        "status": status,
        "latest_eligibility_rate": round(latest_eligibility_rate, 2),
        "total_runs_in_timeline": total_runs,
        "eligibility_trend": trend,
        "most_frequent_failure": timeline.get("ranked_failed_checks", [])[0] if timeline.get("ranked_failed_checks") else None,
        "recent_ineligible_runs": recent_ineligible_runs,
    }


# =============================================================================
# PHASE III: CROSS-RUN BUNDLE TIMELINE & DIRECTOR SIGNAL
# =============================================================================

# --- Schema Versions for Phase III ---
EVOLUTION_LEDGER_SCHEMA_VERSION = "1.0.0"
DIRECTOR_STATUS_SCHEMA_VERSION = "1.0.0"
GLOBAL_HEALTH_SCHEMA_VERSION = "1.0.0"


class DirectorStatus(str, Enum):
    """Director console status signals."""
    GREEN = "GREEN"   # All systems go - eligible, stable
    YELLOW = "YELLOW" # Warning - degraded or unstable
    RED = "RED"       # Blocked - critical failures or regression


class StabilityRating(str, Enum):
    """Stability ratings for bundle evolution."""
    STABLE = "STABLE"           # Consistently eligible
    IMPROVING = "IMPROVING"     # Trend toward eligibility
    DEGRADING = "DEGRADING"     # Trend away from eligibility
    VOLATILE = "VOLATILE"       # Frequent status changes
    CRITICAL = "CRITICAL"       # Sustained failures


def build_bundle_evolution_ledger(timeline: Dict[str, Any]) -> Dict[str, Any]:
    """
    TASK 1: Build evolution ledger from timeline data.

    Analyzes the historical progression of preflight bundle results to provide:
    - Eligibility curve showing pass/fail trend over time
    - Frequent blockers (checks that fail most often)
    - Stability rating for the overall bundle health

    Args:
        timeline: Timeline from build_preflight_bundle_timeline()

    Returns:
        Evolution ledger with:
        - schema_version: Version of this ledger schema
        - timeline_id: Reference to source timeline
        - eligibility_curve: List of {run_id, timestamp, eligible, cumulative_rate}
        - frequent_blockers: Top N checks that cause failures
        - stability_rating: STABLE|IMPROVING|DEGRADING|VOLATILE|CRITICAL
        - stability_score: Numeric score 0-100
        - window_analysis: Rolling window statistics
        - generated_at: Timestamp
    """
    runs = timeline.get("runs", [])
    failed_freq = timeline.get("failed_checks_frequency", {})
    total_runs = timeline.get("total_runs", 0)

    if total_runs == 0:
        return {
            "schema_version": EVOLUTION_LEDGER_SCHEMA_VERSION,
            "timeline_id": timeline.get("timeline_id"),
            "eligibility_curve": [],
            "frequent_blockers": [],
            "stability_rating": StabilityRating.STABLE.value,
            "stability_score": 100.0,
            "window_analysis": {
                "recent_5_rate": 100.0,
                "recent_10_rate": 100.0,
                "recent_20_rate": 100.0,
            },
            "generated_at": datetime.now(timezone.utc).isoformat(),
        }

    # Build eligibility curve with cumulative rate
    eligibility_curve = []
    cumulative_eligible = 0
    for i, run in enumerate(runs):
        is_eligible = run.get("eligibility") == AuditEligibility.AUDIT_ELIGIBLE.value
        if is_eligible:
            cumulative_eligible += 1
        cumulative_rate = (cumulative_eligible / (i + 1)) * 100

        eligibility_curve.append({
            "run_id": run.get("run_id"),
            "timestamp": run.get("timestamp"),
            "eligible": is_eligible,
            "cumulative_rate": round(cumulative_rate, 2),
        })

    # Identify frequent blockers (sorted by frequency)
    frequent_blockers = []
    for check_id, count in sorted(failed_freq.items(), key=lambda x: -x[1]):
        frequent_blockers.append({
            "check_id": check_id,
            "failure_count": count,
            "failure_rate": round((count / total_runs) * 100, 2),
        })

    # Calculate window-based eligibility rates
    def window_rate(window_size: int) -> float:
        window = runs[-window_size:] if len(runs) >= window_size else runs
        if not window:
            return 100.0
        eligible_count = sum(1 for r in window if r.get("eligibility") == AuditEligibility.AUDIT_ELIGIBLE.value)
        return round((eligible_count / len(window)) * 100, 2)

    recent_5_rate = window_rate(5)
    recent_10_rate = window_rate(10)
    recent_20_rate = window_rate(20)

    # Calculate stability rating
    stability_rating, stability_score = _calculate_stability(
        runs, recent_5_rate, recent_10_rate, recent_20_rate
    )

    return {
        "schema_version": EVOLUTION_LEDGER_SCHEMA_VERSION,
        "timeline_id": timeline.get("timeline_id"),
        "eligibility_curve": eligibility_curve,
        "frequent_blockers": frequent_blockers[:10],  # Top 10
        "stability_rating": stability_rating,
        "stability_score": stability_score,
        "window_analysis": {
            "recent_5_rate": recent_5_rate,
            "recent_10_rate": recent_10_rate,
            "recent_20_rate": recent_20_rate,
        },
        "generated_at": datetime.now(timezone.utc).isoformat(),
    }


def _calculate_stability(
    runs: List[Dict[str, Any]],
    recent_5: float,
    recent_10: float,
    recent_20: float,
) -> Tuple[str, float]:
    """
    Calculate stability rating and score based on run history.

    Returns:
        Tuple of (stability_rating, stability_score)
    """
    if not runs:
        return StabilityRating.STABLE.value, 100.0

    # Calculate volatility (number of status changes)
    status_changes = 0
    prev_eligible = None
    for run in runs:
        curr_eligible = run.get("eligibility") == AuditEligibility.AUDIT_ELIGIBLE.value
        if prev_eligible is not None and curr_eligible != prev_eligible:
            status_changes += 1
        prev_eligible = curr_eligible

    volatility_rate = (status_changes / max(len(runs) - 1, 1)) * 100 if len(runs) > 1 else 0

    # Determine trend by comparing windows
    trend_improving = recent_5 > recent_20
    trend_degrading = recent_5 < recent_20 - 10  # >10% drop

    # Calculate stability score (0-100)
    # Base score from recent eligibility rate
    base_score = recent_5
    # Penalize volatility
    volatility_penalty = min(volatility_rate * 0.5, 30)
    # Adjust for trend
    trend_adjustment = 10 if trend_improving else (-15 if trend_degrading else 0)

    stability_score = max(0, min(100, base_score - volatility_penalty + trend_adjustment))

    # Determine rating
    if recent_5 == 0 and recent_10 < 50:
        rating = StabilityRating.CRITICAL.value
    elif volatility_rate > 40:
        rating = StabilityRating.VOLATILE.value
    elif trend_degrading and recent_5 < 80:
        rating = StabilityRating.DEGRADING.value
    elif trend_improving and recent_20 < recent_5:
        rating = StabilityRating.IMPROVING.value
    else:
        rating = StabilityRating.STABLE.value

    return rating, round(stability_score, 2)


def map_bundle_to_director_status(
    data: Dict[str, Any],
    thresholds: Optional[Dict[str, float]] = None,
) -> Dict[str, Any]:
    """
    TASK 2: Map bundle snapshot or timeline to Director console status.

    Determines a GREEN/YELLOW/RED status for director-level monitoring.
    Works with either a single snapshot or a timeline.

    Args:
        data: Either a snapshot from build_preflight_bundle_snapshot() or
              a timeline from build_preflight_bundle_timeline()
        thresholds: Optional custom thresholds for status determination
            - green_min_rate: Minimum eligibility rate for GREEN (default 95%)
            - yellow_min_rate: Minimum eligibility rate for YELLOW (default 70%)
            - max_consecutive_failures: Max failures before RED (default 3)

    Returns:
        Director status mapping with:
        - schema_version: Version of this mapping schema
        - status: GREEN|YELLOW|RED
        - status_reason: Human-readable explanation
        - metrics: Key metrics that influenced the status
        - recommendations: Suggested actions (if not GREEN)
        - generated_at: Timestamp
    """
    # Default thresholds
    if thresholds is None:
        thresholds = {
            "green_min_rate": 95.0,
            "yellow_min_rate": 70.0,
            "max_consecutive_failures": 3,
        }

    # Detect if this is a snapshot or timeline
    is_timeline = "total_runs" in data and "runs" in data
    is_snapshot = "eligibility" in data and "run_id" in data and "total_runs" not in data

    if is_timeline:
        return _map_timeline_to_director_status(data, thresholds)
    elif is_snapshot:
        return _map_snapshot_to_director_status(data, thresholds)
    else:
        # Unknown format - return RED with warning
        return {
            "schema_version": DIRECTOR_STATUS_SCHEMA_VERSION,
            "status": DirectorStatus.RED.value,
            "status_reason": "Unknown data format - cannot determine status",
            "metrics": {},
            "recommendations": ["Verify input data format"],
            "generated_at": datetime.now(timezone.utc).isoformat(),
        }


def _map_snapshot_to_director_status(
    snapshot: Dict[str, Any],
    thresholds: Dict[str, float],
) -> Dict[str, Any]:
    """Map a single snapshot to director status."""
    eligibility = snapshot.get("eligibility", "")
    is_eligible = eligibility == AuditEligibility.AUDIT_ELIGIBLE.value
    failed_checks = snapshot.get("failed_pf_check_ids", [])
    phase_confirmed = snapshot.get("phase_confirmed", False)

    if is_eligible and phase_confirmed:
        status = DirectorStatus.GREEN.value
        reason = "Run is audit-eligible with Phase II confirmation"
        recommendations = []
    elif is_eligible and not phase_confirmed:
        status = DirectorStatus.YELLOW.value
        reason = "Run is eligible but missing Phase II confirmation"
        recommendations = ["Obtain Phase II confirmation before proceeding"]
    else:
        status = DirectorStatus.RED.value
        reason = f"Run is not eligible: {len(failed_checks)} check(s) failed"
        recommendations = [f"Fix {check}" for check in failed_checks[:3]]
        if len(failed_checks) > 3:
            recommendations.append(f"...and {len(failed_checks) - 3} more")

    return {
        "schema_version": DIRECTOR_STATUS_SCHEMA_VERSION,
        "status": status,
        "status_reason": reason,
        "metrics": {
            "is_eligible": is_eligible,
            "phase_confirmed": phase_confirmed,
            "failed_check_count": len(failed_checks),
        },
        "recommendations": recommendations,
        "run_id": snapshot.get("run_id"),
        "generated_at": datetime.now(timezone.utc).isoformat(),
    }


def _map_timeline_to_director_status(
    timeline: Dict[str, Any],
    thresholds: Dict[str, float],
) -> Dict[str, Any]:
    """Map a timeline to director status."""
    runs = timeline.get("runs", [])
    total_runs = timeline.get("total_runs", 0)
    eligibility_rate = timeline.get("eligibility_rate", 0.0)
    failed_freq = timeline.get("failed_checks_frequency", {})

    if total_runs == 0:
        return {
            "schema_version": DIRECTOR_STATUS_SCHEMA_VERSION,
            "status": DirectorStatus.YELLOW.value,
            "status_reason": "No runs in timeline - status unknown",
            "metrics": {"total_runs": 0},
            "recommendations": ["Execute preflight bundle to establish baseline"],
            "timeline_id": timeline.get("timeline_id"),
            "generated_at": datetime.now(timezone.utc).isoformat(),
        }

    # Count consecutive failures from end
    consecutive_failures = 0
    for run in reversed(runs):
        if run.get("eligibility") != AuditEligibility.AUDIT_ELIGIBLE.value:
            consecutive_failures += 1
        else:
            break

    # Determine status
    recommendations = []
    if consecutive_failures >= thresholds["max_consecutive_failures"]:
        status = DirectorStatus.RED.value
        reason = f"Critical: {consecutive_failures} consecutive failures"
        recommendations = ["Investigate root cause of repeated failures"]
        if failed_freq:
            top_blocker = max(failed_freq.items(), key=lambda x: x[1])[0]
            recommendations.append(f"Priority fix: {top_blocker}")
    elif eligibility_rate < thresholds["yellow_min_rate"]:
        status = DirectorStatus.RED.value
        reason = f"Eligibility rate {eligibility_rate}% below threshold ({thresholds['yellow_min_rate']}%)"
        recommendations = ["Review and fix systematic failures"]
    elif eligibility_rate < thresholds["green_min_rate"]:
        status = DirectorStatus.YELLOW.value
        reason = f"Eligibility rate {eligibility_rate}% below optimal ({thresholds['green_min_rate']}%)"
        if failed_freq:
            top_blockers = sorted(failed_freq.items(), key=lambda x: -x[1])[:3]
            recommendations = [f"Address {check} ({count} failures)" for check, count in top_blockers]
    else:
        status = DirectorStatus.GREEN.value
        reason = f"Healthy: {eligibility_rate}% eligibility rate"
        recommendations = []

    return {
        "schema_version": DIRECTOR_STATUS_SCHEMA_VERSION,
        "status": status,
        "status_reason": reason,
        "metrics": {
            "total_runs": total_runs,
            "eligibility_rate": eligibility_rate,
            "consecutive_failures": consecutive_failures,
            "unique_failure_types": len(failed_freq),
        },
        "recommendations": recommendations,
        "timeline_id": timeline.get("timeline_id"),
        "generated_at": datetime.now(timezone.utc).isoformat(),
    }


def summarize_bundle_for_global_health(timeline: Dict[str, Any]) -> Dict[str, Any]:
    """
    TASK 3: Summarize bundle timeline for Global Health (GH) integration.

    Produces a compact health summary suitable for system-wide health dashboards
    and cross-system monitoring.

    Args:
        timeline: Timeline from build_preflight_bundle_timeline()

    Returns:
        Global health summary with:
        - schema_version: Version of this summary schema
        - bundle_ok: Boolean overall health status
        - historical_failure_rate: Percentage of failures over all runs
        - hotspots: Top failure points requiring attention
        - health_indicators: Detailed health metrics
        - integration_ready: Whether system is ready for GH integration
        - generated_at: Timestamp
    """
    runs = timeline.get("runs", [])
    total_runs = timeline.get("total_runs", 0)
    eligible_count = timeline.get("eligible_run_count", 0)

    # Handle both formats: dict (failed_checks_frequency) or list of tuples (ranked_failed_checks)
    failed_freq = timeline.get("failed_checks_frequency", {})
    if not failed_freq and "ranked_failed_checks" in timeline:
        # Convert list of tuples to dict
        failed_freq = dict(timeline["ranked_failed_checks"])

    if total_runs == 0:
        return {
            "schema_version": GLOBAL_HEALTH_SCHEMA_VERSION,
            "bundle_ok": True,  # No failures = ok
            "historical_failure_rate": 0.0,
            "hotspots": [],
            "health_indicators": {
                "total_runs_analyzed": 0,
                "recent_success_streak": 0,
                "worst_period_failure_rate": 0.0,
                "recovery_time_avg": None,
            },
            "integration_ready": False,
            "generated_at": datetime.now(timezone.utc).isoformat(),
        }

    # Calculate historical failure rate
    historical_failure_rate = ((total_runs - eligible_count) / total_runs) * 100

    # Identify hotspots (most frequent failures)
    hotspots = []
    for check_id, count in sorted(failed_freq.items(), key=lambda x: -x[1])[:5]:
        hotspots.append({
            "check_id": check_id,
            "failure_count": count,
            "impact_score": round((count / total_runs) * 100, 2),
            "severity": "HIGH" if count >= total_runs * 0.2 else ("MEDIUM" if count >= total_runs * 0.1 else "LOW"),
        })

    # Calculate recent success streak
    success_streak = 0
    for run in reversed(runs):
        if run.get("eligibility") == AuditEligibility.AUDIT_ELIGIBLE.value:
            success_streak += 1
        else:
            break

    # Calculate worst period failure rate (sliding window of 10)
    worst_failure_rate = 0.0
    window_size = min(10, total_runs)
    for i in range(len(runs) - window_size + 1):
        window = runs[i:i + window_size]
        failures_in_window = sum(1 for r in window if r.get("eligibility") != AuditEligibility.AUDIT_ELIGIBLE.value)
        window_failure_rate = (failures_in_window / window_size) * 100
        worst_failure_rate = max(worst_failure_rate, window_failure_rate)

    # Determine bundle_ok status
    # OK if: recent streak >= 3 OR (historical rate < 20% AND no critical hotspots)
    has_critical_hotspot = any(h["severity"] == "HIGH" for h in hotspots)
    bundle_ok = (
        success_streak >= 3 or
        (historical_failure_rate < 20.0 and not has_critical_hotspot)
    )

    # Integration readiness check
    # Ready if: at least 10 runs AND not critically failing
    integration_ready = total_runs >= 10 and historical_failure_rate < 50.0

    return {
        "schema_version": GLOBAL_HEALTH_SCHEMA_VERSION,
        "bundle_ok": bundle_ok,
        "historical_failure_rate": round(historical_failure_rate, 2),
        "hotspots": hotspots,
        "health_indicators": {
            "total_runs_analyzed": total_runs,
            "recent_success_streak": success_streak,
            "worst_period_failure_rate": round(worst_failure_rate, 2),
            "recovery_time_avg": None,  # Would require timestamp analysis
        },
        "integration_ready": integration_ready,
        "generated_at": datetime.now(timezone.utc).isoformat(),
    }


# =============================================================================
# PHASE IV: BUNDLE EVOLUTION GATE & DIRECTOR INTEGRATION TILE
# =============================================================================

# --- Schema Versions for Phase IV ---
INTEGRATION_EVAL_SCHEMA_VERSION = "1.0.0"
MAAS_BUNDLE_SCHEMA_VERSION = "1.0.0"
DIRECTOR_PANEL_SCHEMA_VERSION = "1.0.0"


class IntegrationStatus(str, Enum):
    """Integration readiness status."""
    OK = "OK"       # Ready for integration
    WARN = "WARN"   # Proceed with caution
    BLOCK = "BLOCK" # Not ready, blocking issues


class MAASStatus(str, Enum):
    """MAAS bundle status."""
    OK = "OK"             # Bundle admissible
    ATTENTION = "ATTENTION"  # Needs review
    BLOCK = "BLOCK"       # Not admissible


def evaluate_bundle_for_integration(
    evolution_ledger: Dict[str, Any],
    global_summary: Dict[str, Any],
) -> Dict[str, Any]:
    """
    TASK 1: Evaluate bundle evolution for integration readiness.

    Uses evolution ledger and global health summary to determine if the
    bundle is ready for integration into the broader system.

    Args:
        evolution_ledger: From build_bundle_evolution_ledger()
        global_summary: From summarize_bundle_for_global_health()

    Returns:
        Integration evaluation with:
        - schema_version: Version of this evaluation schema
        - integration_ok: Boolean - True if ready for integration
        - status: "OK" | "WARN" | "BLOCK"
        - blocking_reasons: List of reasons preventing integration
        - warning_reasons: List of reasons for caution
        - metrics: Key metrics used in evaluation
        - generated_at: Timestamp
    """
    blocking_reasons = []
    warning_reasons = []

    # Extract key metrics
    stability_rating = evolution_ledger.get("stability_rating", StabilityRating.STABLE.value)
    stability_score = evolution_ledger.get("stability_score", 100.0)
    window_analysis = evolution_ledger.get("window_analysis", {})
    recent_5_rate = window_analysis.get("recent_5_rate", 100.0)
    recent_10_rate = window_analysis.get("recent_10_rate", 100.0)

    bundle_ok = global_summary.get("bundle_ok", False)
    integration_ready = global_summary.get("integration_ready", False)
    historical_failure_rate = global_summary.get("historical_failure_rate", 0.0)
    hotspots = global_summary.get("hotspots", [])

    # Check blocking conditions
    if stability_rating == StabilityRating.CRITICAL.value:
        blocking_reasons.append("Stability rating is CRITICAL")

    if recent_5_rate == 0:
        blocking_reasons.append("No eligible runs in last 5 executions")

    if historical_failure_rate >= 50.0:
        blocking_reasons.append(f"Historical failure rate too high: {historical_failure_rate}%")

    if not integration_ready:
        blocking_reasons.append("Global health check indicates not integration ready")

    # Check for HIGH severity hotspots
    high_severity_hotspots = [h for h in hotspots if h.get("severity") == "HIGH"]
    if len(high_severity_hotspots) >= 2:
        blocking_reasons.append(f"Multiple HIGH severity hotspots: {len(high_severity_hotspots)}")

    # Check warning conditions
    if stability_rating == StabilityRating.DEGRADING.value:
        warning_reasons.append("Stability rating is DEGRADING - monitor closely")

    if stability_rating == StabilityRating.VOLATILE.value:
        warning_reasons.append("Stability rating is VOLATILE - unpredictable behavior")

    if recent_5_rate < recent_10_rate - 10:
        warning_reasons.append("Recent eligibility declining")

    if stability_score < 70:
        warning_reasons.append(f"Stability score below threshold: {stability_score}")

    if len(high_severity_hotspots) == 1:
        warning_reasons.append(f"HIGH severity hotspot: {high_severity_hotspots[0].get('check_id')}")

    if not bundle_ok and not blocking_reasons:
        warning_reasons.append("Global health check reports bundle_ok=False")

    # Determine final status
    if blocking_reasons:
        status = IntegrationStatus.BLOCK.value
        integration_ok = False
    elif warning_reasons:
        status = IntegrationStatus.WARN.value
        integration_ok = True  # Can proceed with caution
    else:
        status = IntegrationStatus.OK.value
        integration_ok = True

    return {
        "schema_version": INTEGRATION_EVAL_SCHEMA_VERSION,
        "integration_ok": integration_ok,
        "status": status,
        "blocking_reasons": blocking_reasons,
        "warning_reasons": warning_reasons,
        "metrics": {
            "stability_rating": stability_rating,
            "stability_score": stability_score,
            "recent_5_rate": recent_5_rate,
            "historical_failure_rate": historical_failure_rate,
            "high_severity_hotspot_count": len(high_severity_hotspots),
        },
        "generated_at": datetime.now(timezone.utc).isoformat(),
    }


def summarize_bundle_for_maas(
    global_summary: Dict[str, Any],
    evolution_ledger: Dict[str, Any],
) -> Dict[str, Any]:
    """
    TASK 2: Summarize bundle for MAAS (Multi-Agent Audit System) integration.

    Produces a compact summary suitable for MAAS admission decisions.

    Args:
        global_summary: From summarize_bundle_for_global_health()
        evolution_ledger: From build_bundle_evolution_ledger()

    Returns:
        MAAS bundle summary with:
        - schema_version: Version of this summary schema
        - bundle_admissible: Boolean - True if admissible to MAAS
        - failure_hotspots: List of top failure points
        - stability_rating: Current stability rating
        - status: "OK" | "ATTENTION" | "BLOCK"
        - status_reason: Human-readable explanation
        - metrics: Key metrics for MAAS decision
        - generated_at: Timestamp
    """
    # Extract key data
    bundle_ok = global_summary.get("bundle_ok", False)
    integration_ready = global_summary.get("integration_ready", False)
    historical_failure_rate = global_summary.get("historical_failure_rate", 0.0)
    hotspots = global_summary.get("hotspots", [])
    health_indicators = global_summary.get("health_indicators", {})

    stability_rating = evolution_ledger.get("stability_rating", StabilityRating.STABLE.value)
    stability_score = evolution_ledger.get("stability_score", 100.0)
    frequent_blockers = evolution_ledger.get("frequent_blockers", [])

    # Build failure hotspots list (simplified for MAAS)
    failure_hotspots = []
    for hotspot in hotspots[:3]:  # Top 3
        failure_hotspots.append({
            "check_id": hotspot.get("check_id"),
            "severity": hotspot.get("severity"),
            "impact_score": hotspot.get("impact_score"),
        })

    # Determine admissibility and status
    high_severity_count = sum(1 for h in hotspots if h.get("severity") == "HIGH")

    if stability_rating == StabilityRating.CRITICAL.value:
        bundle_admissible = False
        status = MAASStatus.BLOCK.value
        status_reason = "Bundle in CRITICAL stability state"
    elif historical_failure_rate >= 50.0:
        bundle_admissible = False
        status = MAASStatus.BLOCK.value
        status_reason = f"Excessive failure rate: {historical_failure_rate}%"
    elif high_severity_count >= 2:
        bundle_admissible = False
        status = MAASStatus.BLOCK.value
        status_reason = f"Multiple HIGH severity failures: {high_severity_count}"
    elif not integration_ready:
        bundle_admissible = False
        status = MAASStatus.ATTENTION.value
        status_reason = "Insufficient run history for integration"
    elif stability_rating in [StabilityRating.DEGRADING.value, StabilityRating.VOLATILE.value]:
        bundle_admissible = True
        status = MAASStatus.ATTENTION.value
        status_reason = f"Stability concerns: {stability_rating}"
    elif high_severity_count == 1:
        bundle_admissible = True
        status = MAASStatus.ATTENTION.value
        status_reason = "Single HIGH severity hotspot requires monitoring"
    elif not bundle_ok:
        bundle_admissible = True
        status = MAASStatus.ATTENTION.value
        status_reason = "Bundle health flagged for attention"
    else:
        bundle_admissible = True
        status = MAASStatus.OK.value
        status_reason = "Bundle meets MAAS admission criteria"

    return {
        "schema_version": MAAS_BUNDLE_SCHEMA_VERSION,
        "bundle_admissible": bundle_admissible,
        "failure_hotspots": failure_hotspots,
        "stability_rating": stability_rating,
        "status": status,
        "status_reason": status_reason,
        "metrics": {
            "stability_score": stability_score,
            "historical_failure_rate": historical_failure_rate,
            "recent_success_streak": health_indicators.get("recent_success_streak", 0),
            "total_runs_analyzed": health_indicators.get("total_runs_analyzed", 0),
        },
        "generated_at": datetime.now(timezone.utc).isoformat(),
    }


def build_bundle_director_panel(
    evolution_ledger: Dict[str, Any],
    integration_eval: Dict[str, Any],
) -> Dict[str, Any]:
    """
    TASK 3: Build Director bundle panel for dashboard display.

    Produces a dashboard-ready summary for the Director console tile.

    Args:
        evolution_ledger: From build_bundle_evolution_ledger()
        integration_eval: From evaluate_bundle_for_integration()

    Returns:
        Director panel with:
        - schema_version: Version of this panel schema
        - status_light: "GREEN" | "YELLOW" | "RED"
        - historical_failure_rate: % of failed runs (from ledger)
        - stability_rating: Current stability rating
        - headline: Neutral summary of bundle status
        - details: Additional context for operators
        - generated_at: Timestamp
    """
    # Extract key data
    stability_rating = evolution_ledger.get("stability_rating", StabilityRating.STABLE.value)
    stability_score = evolution_ledger.get("stability_score", 100.0)
    window_analysis = evolution_ledger.get("window_analysis", {})
    recent_5_rate = window_analysis.get("recent_5_rate", 100.0)
    frequent_blockers = evolution_ledger.get("frequent_blockers", [])
    eligibility_curve = evolution_ledger.get("eligibility_curve", [])

    integration_status = integration_eval.get("status", IntegrationStatus.OK.value)
    integration_ok = integration_eval.get("integration_ok", True)
    blocking_reasons = integration_eval.get("blocking_reasons", [])
    warning_reasons = integration_eval.get("warning_reasons", [])
    metrics = integration_eval.get("metrics", {})

    historical_failure_rate = metrics.get("historical_failure_rate", 0.0)

    # Calculate total runs from eligibility curve
    total_runs = len(eligibility_curve)

    # Determine status light
    if integration_status == IntegrationStatus.BLOCK.value:
        status_light = DirectorStatus.RED.value
    elif integration_status == IntegrationStatus.WARN.value:
        status_light = DirectorStatus.YELLOW.value
    else:
        status_light = DirectorStatus.GREEN.value

    # Build headline
    if status_light == DirectorStatus.GREEN.value:
        headline = f"Bundle stable ({stability_rating}) - {total_runs} runs analyzed"
    elif status_light == DirectorStatus.YELLOW.value:
        if warning_reasons:
            headline = f"Bundle requires attention: {warning_reasons[0]}"
        else:
            headline = f"Bundle stability: {stability_rating} - monitoring recommended"
    else:  # RED
        if blocking_reasons:
            headline = f"Bundle blocked: {blocking_reasons[0]}"
        else:
            headline = f"Bundle integration blocked - {stability_rating} stability"

    # Build details
    details = {
        "recent_eligibility": f"{recent_5_rate}% (last 5 runs)",
        "stability_score": f"{stability_score}/100",
        "top_blocker": frequent_blockers[0]["check_id"] if frequent_blockers else None,
        "blocking_count": len(blocking_reasons),
        "warning_count": len(warning_reasons),
    }

    return {
        "schema_version": DIRECTOR_PANEL_SCHEMA_VERSION,
        "status_light": status_light,
        "historical_failure_rate": historical_failure_rate,
        "stability_rating": stability_rating,
        "headline": headline,
        "details": details,
        "integration_ok": integration_ok,
        "generated_at": datetime.now(timezone.utc).isoformat(),
    }


# =============================================================================
# PHASE V: BUNDLE AS INTEGRATION BACKBONE
# =============================================================================

# --- Schema Versions for Phase V ---
CROSS_LAYER_VIEW_SCHEMA_VERSION = "1.0.0"
GLOBAL_CONSOLE_SCHEMA_VERSION = "1.0.0"
GOVERNANCE_SIGNAL_SCHEMA_VERSION = "1.0.0"


class LayerStatus(str, Enum):
    """Status for individual integration layers."""
    OK = "OK"
    WARN = "WARN"
    BLOCK = "BLOCK"
    UNKNOWN = "UNKNOWN"


class GovernanceSignalType(str, Enum):
    """Governance signal types for CLAUDE I."""
    PROCEED = "PROCEED"
    REVIEW = "REVIEW"
    HALT = "HALT"


def _extract_layer_status(layer_state: Optional[Dict[str, Any]], layer_name: str) -> Dict[str, Any]:
    """
    Extract normalized status from a layer state dict.

    Handles various status field naming conventions across layers.
    """
    if not layer_state:
        return {
            "layer": layer_name,
            "status": LayerStatus.UNKNOWN.value,
            "reason": f"No {layer_name} state provided",
            "blocking": False,
        }

    # Try common status field names
    status_value = None
    for field in ["status", "status_light", "layer_status", "health_status"]:
        if field in layer_state:
            status_value = layer_state[field]
            break

    # Normalize status to OK/WARN/BLOCK
    if status_value is None:
        # Check for boolean-style fields
        if layer_state.get("ok") is False or layer_state.get("healthy") is False:
            status_value = LayerStatus.BLOCK.value
        elif layer_state.get("ok") is True or layer_state.get("healthy") is True:
            status_value = LayerStatus.OK.value
        else:
            status_value = LayerStatus.UNKNOWN.value
    else:
        # Normalize string status values
        status_str = str(status_value).upper()
        if status_str in ("OK", "GREEN", "PASS", "HEALTHY", "READY"):
            status_value = LayerStatus.OK.value
        elif status_str in ("WARN", "YELLOW", "WARNING", "ATTENTION", "CAUTION"):
            status_value = LayerStatus.WARN.value
        elif status_str in ("BLOCK", "RED", "FAIL", "CRITICAL", "ERROR", "HALT"):
            status_value = LayerStatus.BLOCK.value
        else:
            status_value = LayerStatus.UNKNOWN.value

    # Extract reason if available
    reason = None
    for field in ["reason", "status_reason", "message", "headline", "details"]:
        if field in layer_state:
            val = layer_state[field]
            if isinstance(val, str):
                reason = val
                break
            elif isinstance(val, dict) and "message" in val:
                reason = val["message"]
                break

    is_blocking = status_value == LayerStatus.BLOCK.value

    return {
        "layer": layer_name,
        "status": status_value,
        "reason": reason,
        "blocking": is_blocking,
    }


def build_bundle_cross_layer_view(
    evolution_ledger: Dict[str, Any],
    preflight_state: Optional[Dict[str, Any]] = None,
    topology_state: Optional[Dict[str, Any]] = None,
    security_state: Optional[Dict[str, Any]] = None,
) -> Dict[str, Any]:
    """
    TASK 1: Build cross-layer snapshot view integrating multiple system layers.

    This function makes the bundle evolution ledger the integration spine that
    ties together preflight, topology, security, and other layers.

    Args:
        evolution_ledger: From build_bundle_evolution_ledger()
        preflight_state: Current preflight layer state (optional)
        topology_state: Current topology layer state (optional)
        security_state: Current security layer state (optional)

    Returns:
        Cross-layer view with:
        - schema_version: Version of this schema
        - integration_ready: bool - True if all layers are OK
        - blocking_layers: list[str] - Layer names with BLOCK status
        - status: "OK" | "WARN" | "BLOCK"
        - layer_statuses: Dict mapping layer names to their status details
        - bundle_context: Key bundle metrics providing context
        - generated_at: Timestamp
    """
    # Extract status from each layer
    layer_statuses = {}
    blocking_layers = []
    warning_layers = []

    # Bundle layer (from evolution ledger)
    bundle_stability = evolution_ledger.get("stability_rating", StabilityRating.VOLATILE.value)
    bundle_status = LayerStatus.OK.value
    bundle_reason = None

    if bundle_stability == StabilityRating.CRITICAL.value:
        bundle_status = LayerStatus.BLOCK.value
        bundle_reason = "Bundle stability is CRITICAL"
    elif bundle_stability in (StabilityRating.DEGRADING.value, StabilityRating.VOLATILE.value):
        bundle_status = LayerStatus.WARN.value
        bundle_reason = f"Bundle stability is {bundle_stability}"

    layer_statuses["bundle"] = {
        "layer": "bundle",
        "status": bundle_status,
        "reason": bundle_reason,
        "blocking": bundle_status == LayerStatus.BLOCK.value,
    }

    if bundle_status == LayerStatus.BLOCK.value:
        blocking_layers.append("bundle")
    elif bundle_status == LayerStatus.WARN.value:
        warning_layers.append("bundle")

    # Preflight layer
    preflight_info = _extract_layer_status(preflight_state, "preflight")
    layer_statuses["preflight"] = preflight_info
    if preflight_info["blocking"]:
        blocking_layers.append("preflight")
    elif preflight_info["status"] == LayerStatus.WARN.value:
        warning_layers.append("preflight")

    # Topology layer
    topology_info = _extract_layer_status(topology_state, "topology")
    layer_statuses["topology"] = topology_info
    if topology_info["blocking"]:
        blocking_layers.append("topology")
    elif topology_info["status"] == LayerStatus.WARN.value:
        warning_layers.append("topology")

    # Security layer
    security_info = _extract_layer_status(security_state, "security")
    layer_statuses["security"] = security_info
    if security_info["blocking"]:
        blocking_layers.append("security")
    elif security_info["status"] == LayerStatus.WARN.value:
        warning_layers.append("security")

    # Determine overall status
    if blocking_layers:
        overall_status = LayerStatus.BLOCK.value
        integration_ready = False
    elif warning_layers:
        overall_status = LayerStatus.WARN.value
        integration_ready = True  # Warnings don't block integration
    else:
        overall_status = LayerStatus.OK.value
        integration_ready = True

    # Extract bundle context for visibility
    eligibility_curve = evolution_ledger.get("eligibility_curve", [])
    recent_eligibility = eligibility_curve[-1] if eligibility_curve else None

    bundle_context = {
        "stability_rating": bundle_stability,
        "stability_score": evolution_ledger.get("stability_score", 0),
        "total_runs": evolution_ledger.get("total_runs", 0),
        "eligible_runs": evolution_ledger.get("eligible_runs", 0),
        "recent_eligibility_pct": recent_eligibility,
        "frequent_blockers_count": len(evolution_ledger.get("frequent_blockers", [])),
    }

    return {
        "schema_version": CROSS_LAYER_VIEW_SCHEMA_VERSION,
        "integration_ready": integration_ready,
        "blocking_layers": blocking_layers,
        "warning_layers": warning_layers,
        "status": overall_status,
        "layer_statuses": layer_statuses,
        "bundle_context": bundle_context,
        "generated_at": datetime.now(timezone.utc).isoformat(),
    }


def summarize_bundle_for_global_console(
    evolution_ledger: Dict[str, Any],
    integration_eval: Dict[str, Any],
) -> Dict[str, Any]:
    """
    TASK 2: Summarize bundle for global console display.

    Provides a consolidated view suitable for dashboard rendering in the
    global operations console.

    Args:
        evolution_ledger: From build_bundle_evolution_ledger()
        integration_eval: From evaluate_bundle_for_integration()

    Returns:
        Global console summary with:
        - schema_version: Version of this schema
        - status_light: "GREEN" | "YELLOW" | "RED"
        - historical_failure_rate: float (0.0-1.0)
        - stability_rating: Stability rating string
        - headline: Human-readable summary
        - quick_stats: Key metrics for quick reference
        - action_required: Whether operator action is needed
        - generated_at: Timestamp
    """
    # Extract key metrics from evolution ledger
    total_runs = evolution_ledger.get("total_runs", 0)
    eligible_runs = evolution_ledger.get("eligible_runs", 0)
    stability_rating = evolution_ledger.get("stability_rating", StabilityRating.VOLATILE.value)
    stability_score = evolution_ledger.get("stability_score", 0)
    frequent_blockers = evolution_ledger.get("frequent_blockers", [])

    # Calculate historical failure rate
    if total_runs > 0:
        historical_failure_rate = round((total_runs - eligible_runs) / total_runs, 4)
    else:
        historical_failure_rate = 0.0

    # Extract integration status
    integration_status = integration_eval.get("status", IntegrationStatus.BLOCK.value)
    integration_ok = integration_eval.get("integration_ok", False)
    blocking_reasons = integration_eval.get("blocking_reasons", [])
    warning_reasons = integration_eval.get("warning_reasons", [])

    # Map to status light (same logic as director panel)
    if integration_status == IntegrationStatus.BLOCK.value:
        status_light = DirectorStatus.RED.value
    elif integration_status == IntegrationStatus.WARN.value:
        status_light = DirectorStatus.YELLOW.value
    else:
        status_light = DirectorStatus.GREEN.value

    # Build headline
    if status_light == DirectorStatus.GREEN.value:
        headline = f"Bundle healthy: {stability_rating} ({total_runs} runs, {round((1-historical_failure_rate)*100, 1)}% success)"
    elif status_light == DirectorStatus.YELLOW.value:
        issue = warning_reasons[0] if warning_reasons else f"{stability_rating} stability"
        headline = f"Bundle attention: {issue}"
    else:  # RED
        issue = blocking_reasons[0] if blocking_reasons else "Critical issues detected"
        headline = f"Bundle blocked: {issue}"

    # Quick stats for dashboard
    quick_stats = {
        "total_runs": total_runs,
        "success_rate": round((1 - historical_failure_rate) * 100, 1),
        "stability_score": stability_score,
        "active_blockers": len(frequent_blockers),
        "pending_warnings": len(warning_reasons),
    }

    # Determine if action is required
    action_required = status_light in (DirectorStatus.RED.value, DirectorStatus.YELLOW.value)

    return {
        "schema_version": GLOBAL_CONSOLE_SCHEMA_VERSION,
        "status_light": status_light,
        "historical_failure_rate": historical_failure_rate,
        "stability_rating": stability_rating,
        "headline": headline,
        "quick_stats": quick_stats,
        "action_required": action_required,
        "integration_ok": integration_ok,
        "generated_at": datetime.now(timezone.utc).isoformat(),
    }


def build_bundle_governance_signal(
    cross_layer_view: Dict[str, Any],
    evolution_ledger: Dict[str, Any],
    context: Optional[Dict[str, Any]] = None,
) -> Dict[str, Any]:
    """
    TASK 3: Build governance signal for CLAUDE I integration.

    Provides a normalized signal that CLAUDE I (Governance) can consume to
    make experiment-wide decisions.

    Args:
        cross_layer_view: From build_bundle_cross_layer_view()
        evolution_ledger: From build_bundle_evolution_ledger()
        context: Optional additional context (experiment_id, phase, etc.)

    Returns:
        Governance signal with:
        - schema_version: Version of this schema
        - signal_type: "PROCEED" | "REVIEW" | "HALT"
        - confidence: float (0.0-1.0) - confidence in the signal
        - blocking_factors: List of factors causing HALT/REVIEW
        - risk_indicators: List of risk factors
        - recommendation: Human-readable recommendation
        - audit_trail: Key data for audit logging
        - generated_at: Timestamp
    """
    context = context or {}

    # Extract cross-layer status
    cross_layer_status = cross_layer_view.get("status", LayerStatus.BLOCK.value)
    blocking_layers = cross_layer_view.get("blocking_layers", [])
    warning_layers = cross_layer_view.get("warning_layers", [])
    integration_ready = cross_layer_view.get("integration_ready", False)

    # Extract bundle metrics
    stability_rating = evolution_ledger.get("stability_rating", StabilityRating.VOLATILE.value)
    stability_score = evolution_ledger.get("stability_score", 0)
    total_runs = evolution_ledger.get("total_runs", 0)
    eligible_runs = evolution_ledger.get("eligible_runs", 0)

    # Blocking factors
    blocking_factors = []
    risk_indicators = []

    # Check for blocking conditions
    if blocking_layers:
        for layer in blocking_layers:
            blocking_factors.append(f"Layer '{layer}' is blocking")

    if stability_rating == StabilityRating.CRITICAL.value:
        blocking_factors.append("Bundle stability is CRITICAL")

    # Check for risk indicators (warnings)
    if warning_layers:
        for layer in warning_layers:
            risk_indicators.append(f"Layer '{layer}' requires attention")

    if stability_rating == StabilityRating.DEGRADING.value:
        risk_indicators.append("Bundle stability is degrading")
    elif stability_rating == StabilityRating.VOLATILE.value:
        risk_indicators.append("Bundle stability is volatile")

    # Calculate success rate
    success_rate = eligible_runs / total_runs if total_runs > 0 else 0.0
    if success_rate < 0.5:
        blocking_factors.append(f"Success rate too low ({round(success_rate*100, 1)}%)")
    elif success_rate < 0.8:
        risk_indicators.append(f"Success rate below threshold ({round(success_rate*100, 1)}%)")

    # Determine signal type
    if blocking_factors:
        signal_type = GovernanceSignalType.HALT.value
    elif risk_indicators:
        signal_type = GovernanceSignalType.REVIEW.value
    else:
        signal_type = GovernanceSignalType.PROCEED.value

    # Calculate confidence based on data quality
    confidence = 1.0
    if total_runs == 0:
        confidence = 0.0  # No data
    elif total_runs < 5:
        confidence = 0.5  # Limited data
    elif total_runs < 10:
        confidence = 0.7  # Moderate data
    elif stability_rating in (StabilityRating.VOLATILE.value, StabilityRating.CRITICAL.value):
        confidence = 0.6  # Unstable pattern reduces confidence

    # Build recommendation
    if signal_type == GovernanceSignalType.PROCEED.value:
        recommendation = "All systems nominal. Experiment may proceed."
    elif signal_type == GovernanceSignalType.REVIEW.value:
        issues = "; ".join(risk_indicators[:2])
        recommendation = f"Review recommended before proceeding. Issues: {issues}"
    else:  # HALT
        issues = "; ".join(blocking_factors[:2])
        recommendation = f"Experiment should halt until resolved. Blocking: {issues}"

    # Audit trail for governance logging
    audit_trail = {
        "experiment_id": context.get("experiment_id"),
        "phase": context.get("phase"),
        "operator_id": context.get("operator_id"),
        "total_runs_analyzed": total_runs,
        "stability_score": stability_score,
        "blocking_layers": blocking_layers,
        "signal_type": signal_type,
    }

    return {
        "schema_version": GOVERNANCE_SIGNAL_SCHEMA_VERSION,
        "signal_type": signal_type,
        "confidence": round(confidence, 2),
        "blocking_factors": blocking_factors,
        "risk_indicators": risk_indicators,
        "recommendation": recommendation,
        "audit_trail": audit_trail,
        "integration_ready": integration_ready,
        "generated_at": datetime.now(timezone.utc).isoformat(),
    }


def adapt_bundle_signal_for_governance(
    governance_signal: Dict[str, Any],
    evolution_ledger: Optional[Dict[str, Any]] = None,
) -> Dict[str, Any]:
    """
    Adapt bundle governance signal to canonical GovernanceSignal schema for CLAUDE I.

    This function converts the output of build_bundle_governance_signal() to the
    canonical schema expected by the global governance layer (CLAUDE I).

    Canonical GovernanceSignal schema (from governance_verifier.py):
        - layer_name: str ("bundle")
        - status: "OK" | "WARN" | "BLOCK"
        - blocking_rules: List[str]
        - blocking_rate: float in [0, 1]
        - headline: str

    Signal Type Mapping:
        PROCEED â†’ OK
        REVIEW  â†’ WARN
        HALT    â†’ BLOCK

    Args:
        governance_signal: Output from build_bundle_governance_signal()
        evolution_ledger: Optional ledger for additional context

    Returns:
        Dict conforming to canonical GovernanceSignal schema for adapt_bundle_to_signal()
    """
    # Map signal_type to canonical status
    signal_type = governance_signal.get("signal_type", GovernanceSignalType.HALT.value)
    if signal_type == GovernanceSignalType.PROCEED.value:
        status = "OK"
    elif signal_type == GovernanceSignalType.REVIEW.value:
        status = "WARN"
    else:  # HALT
        status = "BLOCK"

    # Extract blocking factors as blocking_rules
    blocking_factors = governance_signal.get("blocking_factors", [])
    risk_indicators = governance_signal.get("risk_indicators", [])
    blocking_rules = blocking_factors + risk_indicators

    # Calculate blocking_rate from confidence (inverse relationship)
    # High confidence in PROCEED = low blocking rate
    # Low confidence or HALT = high blocking rate
    confidence = governance_signal.get("confidence", 0.0)
    if status == "OK":
        blocking_rate = 0.0
    elif status == "WARN":
        blocking_rate = 0.3  # Moderate risk
    else:  # BLOCK
        blocking_rate = 1.0 - confidence  # Lower confidence = higher blocking rate

    # Use recommendation as headline
    headline = governance_signal.get("recommendation", f"Bundle: {status}")

    return {
        "layer_name": "bundle",
        "status": status,
        "blocking_rules": blocking_rules,
        "blocking_rate": round(blocking_rate, 4),
        "headline": headline,
    }


# =============================================================================
# BUNDLE WEIGHTING IN GLOBAL PROMOTION DECISIONS
# =============================================================================
#
# The bundle layer (CLAUDE N) serves as the **integration backbone** that ties
# together preflight, topology, security, and other layers. In global promotion
# decisions orchestrated by CLAUDE I, bundle signals should be weighted as follows:
#
# LAYER WEIGHTING GUIDELINES:
#
# 1. CRITICAL LAYERS (Hard Gates - must be OK):
#    - Preflight (J): Environment/determinism validation - VETO POWER
#    - Replay (A): Reproducibility verification - VETO POWER
#    - Hash Tree (L): Cryptographic integrity - VETO POWER
#    - Admissibility (O): Formal eligibility - VETO POWER
#
# 2. BUNDLE LAYER (N) - INTEGRATION BACKBONE:
#    - Weight: HIGH (0.8-1.0 multiplier on aggregate score)
#    - Role: Synthesizes cross-layer health; provides stability trend signal
#    - Effect: Bundle BLOCK should halt promotion even if individual layers are OK
#    - Rationale: Bundle reflects temporal patterns (evolution, stability drift)
#              that point-in-time layer checks may miss
#
# 3. INFORMATIONAL LAYERS (Soft Gates - influence but don't veto):
#    - Topology (B+G): Weight 0.6 - trajectory patterns
#    - Metrics (D): Weight 0.5 - quantitative health
#    - Security (K): Weight 0.7 - risk posture
#    - Budget (F): Weight 0.5 - resource constraints
#    - Conjecture (M): Weight 0.3 - speculative analysis
#
# PROMOTION DECISION LOGIC:
#
#   def should_promote(signals: List[GovernanceSignal]) -> bool:
#       # Phase 1: Check critical layer hard gates
#       for sig in signals:
#           if sig.layer_name in CRITICAL_LAYERS and sig.status == "BLOCK":
#               return False  # Hard veto
#
#       # Phase 2: Check bundle integration backbone
#       bundle_sig = get_signal("bundle", signals)
#       if bundle_sig and bundle_sig.status == "BLOCK":
#           return False  # Bundle BLOCK = integration failure
#
#       # Phase 3: Compute weighted score from informational layers
#       score = compute_weighted_score(signals, LAYER_WEIGHTS)
#       return score >= PROMOTION_THRESHOLD  # Default: 0.7
#
# BUNDLE vs OTHER LAYERS:
#
#   | Layer      | Can Veto? | Weight | Notes                           |
#   |------------|-----------|--------|---------------------------------|
#   | Preflight  | YES       | N/A    | Environment must be valid       |
#   | Replay     | YES       | N/A    | Must be reproducible            |
#   | Hash Tree  | YES       | N/A    | Cryptographic integrity         |
#   | Admissib.  | YES       | N/A    | Formal eligibility required     |
#   | **Bundle** | **YES**   | **0.9**| **Integration backbone**        |
#   | Topology   | NO        | 0.6    | Trajectory patterns             |
#   | Security   | NO        | 0.7    | Risk posture                    |
#   | Metrics    | NO        | 0.5    | Quantitative health             |
#   | Budget     | NO        | 0.5    | Resource constraints            |
#   | Conjecture | NO        | 0.3    | Speculative analysis            |
#
# KEY PRINCIPLE: Bundle reflects TEMPORAL STABILITY. A system may pass all
# point-in-time checks but have degrading stability over time. Bundle BLOCK
# signals that the integration backbone has detected an unhealthy trend that
# warrants halting promotion until stability is restored.
#
# =============================================================================


# =============================================================================
# CLI
# =============================================================================

def main():
    parser = argparse.ArgumentParser(
        description="U2 Pre-Flight Bundle Orchestrator (PFB-001)",
        epilog="Exit codes: 0=AUDIT_ELIGIBLE, 1=NOT_ELIGIBLE, 2=Invalid args, 3=Warnings (with --strict)"
    )

    parser.add_argument(
        "--run-dir",
        type=Path,
        help="Run directory for this experiment (defaults to $MATHLEDGER_CACHE_ROOT/u2/$U2_RUN_ID)"
    )
    parser.add_argument(
        "--run-id",
        help="Run ID (defaults to $U2_RUN_ID)"
    )
    parser.add_argument(
        "--prereg-file",
        type=Path,
        help="Path to PREREG_UPLIFT_U2.yaml"
    )
    parser.add_argument(
        "--operator-id",
        help="Operator identity for audit trail"
    )
    parser.add_argument(
        "--confirm-phase2",
        action="store_true",
        help="Explicit confirmation for Phase II mode switch"
    )
    parser.add_argument(
        "--experiment-id",
        help="Experiment ID to validate against prereg"
    )
    parser.add_argument(
        "--output", "-o",
        type=Path,
        help="Output path for preflight_bundle_report.json"
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Validate without creating files or switching modes"
    )
    parser.add_argument(
        "--strict",
        action="store_true",
        help="Fail on warnings (exit code 3)"
    )
    parser.add_argument(
        "--json",
        action="store_true",
        help="Output report in JSON format to stdout"
    )
    parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="Verbose output"
    )

    # V2 Timeline options
    parser.add_argument(
        "--timeline-from",
        type=Path,
        metavar="DIR",
        help="Generate timeline from preflight reports in DIR (v2 enhancement)"
    )
    parser.add_argument(
        "--timeline-id",
        help="Optional identifier for the timeline"
    )
    parser.add_argument(
        "--snapshot",
        action="store_true",
        help="Output compact snapshot instead of full report (v2 enhancement)"
    )
    parser.add_argument(
        "--admissibility",
        action="store_true",
        help="Output MAAS admissibility summary (v2 enhancement)"
    )

    args = parser.parse_args()

    # Handle timeline mode (v2)
    if args.timeline_from:
        snapshots = load_snapshots_from_directory(args.timeline_from)
        timeline = build_preflight_bundle_timeline(snapshots, args.timeline_id)
        timeline_json = json.dumps(timeline, indent=2)

        if args.output:
            args.output.parent.mkdir(parents=True, exist_ok=True)
            args.output.write_text(timeline_json)
            print(f"Timeline written to {args.output}")
        else:
            print(timeline_json)

        # Exit with status based on eligibility rate
        if timeline["total_runs"] == 0:
            sys.exit(2)
        sys.exit(0)

    # Resolve run_id
    run_id = args.run_id or os.environ.get("U2_RUN_ID")
    if not run_id:
        print("ERROR: --run-id or U2_RUN_ID environment variable required", file=sys.stderr)
        sys.exit(2)

    # Resolve run_dir
    if args.run_dir:
        run_dir = args.run_dir
    else:
        cache_root = os.environ.get("MATHLEDGER_CACHE_ROOT")
        if cache_root:
            run_dir = Path(cache_root) / "u2" / run_id
        else:
            print("ERROR: --run-dir or MATHLEDGER_CACHE_ROOT required", file=sys.stderr)
            sys.exit(2)

    # Build config
    config = PreflightConfig(
        run_dir=run_dir,
        run_id=run_id,
        prereg_file=args.prereg_file,
        operator_id=args.operator_id,
        confirm_phase2=args.confirm_phase2,
        experiment_id=args.experiment_id,
        dry_run=args.dry_run,
        strict=args.strict,
    )

    # Run pre-flight bundle
    if args.verbose:
        print(f"Running pre-flight bundle for run {run_id}...")

    stage_results, all_checks = run_preflight_bundle(config)
    report = generate_bundle_report(config, stage_results, all_checks)

    # Handle v2 output modes
    if args.snapshot or args.admissibility:
        snapshot = build_preflight_bundle_snapshot(report)

        if args.admissibility:
            output_data = summarize_preflight_for_admissibility(snapshot)
        else:
            output_data = snapshot

        output_json = json.dumps(output_data, indent=2)

        if args.output:
            args.output.parent.mkdir(parents=True, exist_ok=True)
            args.output.write_text(output_json)
            print(f"{'Admissibility summary' if args.admissibility else 'Snapshot'} written to {args.output}")
        else:
            print(output_json)

        # Exit based on eligibility
        fail_count = report["summary"]["checks_failed"]
        if fail_count > 0:
            sys.exit(1)
        sys.exit(0)

    # Output report
    report_json = json.dumps(report, indent=2)

    if args.output:
        args.output.parent.mkdir(parents=True, exist_ok=True)
        args.output.write_text(report_json)
        print(f"Report written to {args.output}")

    if args.json:
        print(report_json)
    elif not args.output:
        # Print summary to console
        summary = report["summary"]
        status = report["audit_eligible"]["status"]

        print("\n" + "=" * 60)
        print("U2 PRE-FLIGHT BUNDLE REPORT")
        print("=" * 60)
        print(f"Run ID:    {config.run_id}")
        print(f"Status:    {status}")
        print(f"Stages:    {summary['stages_passed']}/{summary['total_stages']} passed")
        print(f"Checks:    {summary['checks_passed']} pass, {summary['checks_failed']} fail, {summary['checks_warnings']} warn")
        print("-" * 60)

        for stage in stage_results:
            status_icon = "âœ“" if stage.passed else "âœ—"
            print(f"\n[{status_icon}] Stage {stage.stage}: {stage.name}")
            for check in stage.checks:
                icon = {"PASS": "âœ“", "FAIL": "âœ—", "WARN": "!", "SKIP": "-"}[check.status.value]
                print(f"    [{icon}] {check.id}: {check.message}")

        print("\n" + "=" * 60)

    # Determine exit code
    fail_count = report["summary"]["checks_failed"]
    warn_count = report["summary"]["checks_warnings"]

    if fail_count > 0:
        sys.exit(1)
    if args.strict and warn_count > 0:
        sys.exit(3)
    sys.exit(0)


if __name__ == "__main__":
    main()