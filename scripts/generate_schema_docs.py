#!/usr/bin/env python3
"""
Schema Documentation Generator — Auto-Generated Documentation Fabricator

Agent: doc-ops-2 — Schema Weaver
Mission: Generate documentation AUTOMATICALLY from YAML/JSON schemas and Python definitions.

This tool reads schema files from:
    - config/curriculum_uplift_phase2.yaml
    - config/verifier_budget_phase2.yaml
    - docs/json_schemas/* (if present)
    - docs/evidence/*.json
    - config/**/*.json
    - experiments/manifest.py schema definitions

And emits:
    - docs/SCHEMA_REFERENCE.md
    - docs/SCHEMA_DIFF_REPORT.md

Features:
    - Auto-generate tables for each field
    - Auto-generate type summaries
    - Auto-detect schema drift across files
    - Deterministic formatting (sorted keys, stable output)

Usage:
    python scripts/generate_schema_docs.py           # Generate docs
    python scripts/generate_schema_docs.py --check   # CI mode: fail if docs differ
"""

from __future__ import annotations

import argparse
import ast
import hashlib
import json
import re
import sys
from collections import defaultdict
from dataclasses import dataclass, field
from datetime import datetime, timezone
from pathlib import Path
from typing import Any

try:
    import yaml
except ImportError:
    yaml = None  # type: ignore


# =============================================================================
# CONSTANTS
# =============================================================================

HEADER_BANNER = """\
<!-- AUTO-GENERATED — DO NOT EDIT BY HAND -->
<!-- Generated by scripts/generate_schema_docs.py -->
<!-- Regenerate via: uv run python scripts/generate_schema_docs.py -->
"""

PROJECT_ROOT = Path(__file__).resolve().parent.parent

SCHEMA_SOURCES = {
    "yaml": [
        "config/curriculum_uplift_phase2.yaml",
        "config/verifier_budget_phase2.yaml",
        "config/curriculum.yaml",
    ],
    "json_config": [
        "config/allblue_lanes.json",
        "config/causal/default.json",
        "config/rfl/production.json",
        "config/rfl/quick.json",
        "config/rfl/quick_test.json",
    ],
    "json_schema": [
        "docs/evidence/experiment_manifest_schema.json",
    ],
    "python_definitions": [
        "experiments/manifest.py",
    ],
}

OUTPUT_REFERENCE = PROJECT_ROOT / "docs" / "SCHEMA_REFERENCE.md"
OUTPUT_DRIFT = PROJECT_ROOT / "docs" / "SCHEMA_DIFF_REPORT.md"


# =============================================================================
# DATA CLASSES
# =============================================================================


@dataclass
class SchemaField:
    """Represents a field extracted from a schema."""

    name: str
    path: str
    field_type: str
    required: bool = True
    description: str = ""
    default: Any = None
    example: Any = None
    constraints: list[str] = field(default_factory=list)


@dataclass
class SchemaDefinition:
    """Represents a parsed schema definition."""

    source_file: str
    schema_name: str
    schema_type: str  # yaml, json, python
    version: str = ""
    description: str = ""
    fields: list[SchemaField] = field(default_factory=list)
    raw_hash: str = ""
    nested_schemas: dict[str, list[SchemaField]] = field(default_factory=dict)


# =============================================================================
# TYPE INFERENCE
# =============================================================================


def infer_type(value: Any) -> str:
    """Infer the type string for a given value."""
    if value is None:
        return "null"
    if isinstance(value, bool):
        return "boolean"
    if isinstance(value, int):
        return "integer"
    if isinstance(value, float):
        return "number"
    if isinstance(value, str):
        return "string"
    if isinstance(value, list):
        if not value:
            return "array"
        inner_types = {infer_type(v) for v in value[:5]}  # Sample first 5
        if len(inner_types) == 1:
            return f"array<{inner_types.pop()}>"
        return f"array<{' | '.join(sorted(inner_types))}>"
    if isinstance(value, dict):
        return "object"
    return type(value).__name__


def extract_constraints(key: str, value: Any) -> list[str]:
    """Extract constraints from key naming conventions and value patterns."""
    constraints = []

    # Naming convention hints
    if "_min" in key or key.startswith("min_"):
        constraints.append("minimum bound")
    if "_max" in key or key.startswith("max_"):
        constraints.append("maximum bound")
    if "_pct" in key or "percent" in key.lower():
        constraints.append("0-100 percentage")
    if "_s" in key and isinstance(value, (int, float)):
        constraints.append("seconds")
    if "_ms" in key and isinstance(value, (int, float)):
        constraints.append("milliseconds")
    if "timeout" in key.lower():
        constraints.append("timeout value")
    if "hash" in key.lower() and isinstance(value, str):
        if len(value) == 64:
            constraints.append("SHA-256 hex string")
        elif len(value) == 32:
            constraints.append("MD5 hex string")
    if "url" in key.lower():
        constraints.append("URL string")
    if "path" in key.lower():
        constraints.append("file path")

    # Value-based hints
    if isinstance(value, str):
        if re.match(r"^\d{4}-\d{2}-\d{2}", value):
            constraints.append("ISO date format")
        if re.match(r"^[a-f0-9]{64}$", value):
            constraints.append("SHA-256 hash")

    return constraints


# =============================================================================
# SCHEMA PARSERS
# =============================================================================


def compute_file_hash(filepath: Path) -> str:
    """Compute SHA-256 hash of file contents for drift detection."""
    if not filepath.exists():
        return "FILE_NOT_FOUND"
    content = filepath.read_bytes()
    return hashlib.sha256(content).hexdigest()[:16]


def parse_yaml_schema(filepath: Path) -> SchemaDefinition | None:
    """Parse a YAML configuration file into a schema definition."""
    if yaml is None:
        print(f"Warning: PyYAML not installed, skipping {filepath}", file=sys.stderr)
        return None

    if not filepath.exists():
        return None

    try:
        content = filepath.read_text(encoding="utf-8")
        data = yaml.safe_load(content)
    except Exception as e:
        print(f"Warning: Failed to parse {filepath}: {e}", file=sys.stderr)
        return None

    if not isinstance(data, dict):
        return None

    schema = SchemaDefinition(
        source_file=str(filepath.relative_to(PROJECT_ROOT)),
        schema_name=filepath.stem,
        schema_type="yaml",
        version=str(data.get("version", "")),
        description=str(data.get("description", "")),
        raw_hash=compute_file_hash(filepath),
    )

    # Extract fields recursively
    _extract_fields_recursive(data, "", schema.fields, schema.nested_schemas)

    return schema


def parse_json_schema(filepath: Path) -> SchemaDefinition | None:
    """Parse a JSON configuration/schema file into a schema definition."""
    if not filepath.exists():
        return None

    try:
        content = filepath.read_text(encoding="utf-8")
        data = json.loads(content)
    except Exception as e:
        print(f"Warning: Failed to parse {filepath}: {e}", file=sys.stderr)
        return None

    if not isinstance(data, dict):
        return None

    schema = SchemaDefinition(
        source_file=str(filepath.relative_to(PROJECT_ROOT)),
        schema_name=filepath.stem,
        schema_type="json",
        version=str(data.get("version", data.get("manifest_version", ""))),
        description=str(data.get("description", "")),
        raw_hash=compute_file_hash(filepath),
    )

    # Extract fields recursively
    _extract_fields_recursive(data, "", schema.fields, schema.nested_schemas)

    return schema


def _extract_fields_recursive(
    data: dict[str, Any],
    prefix: str,
    fields: list[SchemaField],
    nested: dict[str, list[SchemaField]],
    max_depth: int = 5,
) -> None:
    """Recursively extract fields from nested dictionaries."""
    if max_depth <= 0:
        return

    for key, value in sorted(data.items()):
        path = f"{prefix}.{key}" if prefix else key

        sf = SchemaField(
            name=key,
            path=path,
            field_type=infer_type(value),
            constraints=extract_constraints(key, value),
        )

        # Capture example for primitive types
        if not isinstance(value, (dict, list)):
            sf.example = value

        fields.append(sf)

        # Recurse into nested objects
        if isinstance(value, dict) and value:
            nested_fields: list[SchemaField] = []
            _extract_fields_recursive(value, path, nested_fields, nested, max_depth - 1)
            if nested_fields:
                nested[path] = nested_fields

        # Handle arrays of objects
        elif isinstance(value, list) and value and isinstance(value[0], dict):
            nested_fields = []
            _extract_fields_recursive(
                value[0], f"{path}[]", nested_fields, nested, max_depth - 1
            )
            if nested_fields:
                nested[f"{path}[]"] = nested_fields


def parse_python_schema(filepath: Path) -> SchemaDefinition | None:
    """Parse Python file for class-based schema definitions."""
    if not filepath.exists():
        return None

    try:
        content = filepath.read_text(encoding="utf-8")
        tree = ast.parse(content)
    except Exception as e:
        print(f"Warning: Failed to parse {filepath}: {e}", file=sys.stderr)
        return None

    schema = SchemaDefinition(
        source_file=str(filepath.relative_to(PROJECT_ROOT)),
        schema_name=filepath.stem,
        schema_type="python",
        raw_hash=compute_file_hash(filepath),
    )

    # Find class definitions
    for node in ast.walk(tree):
        if isinstance(node, ast.ClassDef):
            # Extract docstring
            docstring = ast.get_docstring(node) or ""
            if docstring and not schema.description:
                schema.description = docstring.split("\n")[0]

            # Extract __init__ parameters as fields
            for item in node.body:
                if isinstance(item, ast.FunctionDef) and item.name == "__init__":
                    _extract_init_params(item, schema.fields, node.name)

            # Extract class-level type annotations
            for item in node.body:
                if isinstance(item, ast.AnnAssign) and isinstance(
                    item.target, ast.Name
                ):
                    field_name = item.target.id
                    type_str = _ast_type_to_string(item.annotation)
                    schema.fields.append(
                        SchemaField(
                            name=field_name,
                            path=f"{node.name}.{field_name}",
                            field_type=type_str,
                        )
                    )

    return schema


def _extract_init_params(
    func: ast.FunctionDef, fields: list[SchemaField], class_name: str
) -> None:
    """Extract parameters from __init__ method."""
    for arg in func.args.args:
        if arg.arg == "self":
            continue

        type_str = "Any"
        if arg.annotation:
            type_str = _ast_type_to_string(arg.annotation)

        fields.append(
            SchemaField(
                name=arg.arg,
                path=f"{class_name}.__init__.{arg.arg}",
                field_type=type_str,
            )
        )


def _ast_type_to_string(node: ast.expr) -> str:
    """Convert an AST type annotation to a string."""
    if isinstance(node, ast.Name):
        return node.id
    if isinstance(node, ast.Constant):
        return str(node.value)
    if isinstance(node, ast.Subscript):
        base = _ast_type_to_string(node.value)
        if isinstance(node.slice, ast.Tuple):
            args = ", ".join(_ast_type_to_string(e) for e in node.slice.elts)
        else:
            args = _ast_type_to_string(node.slice)
        return f"{base}[{args}]"
    if isinstance(node, ast.Attribute):
        return f"{_ast_type_to_string(node.value)}.{node.attr}"
    if isinstance(node, ast.BinOp) and isinstance(node.op, ast.BitOr):
        return f"{_ast_type_to_string(node.left)} | {_ast_type_to_string(node.right)}"
    return "Any"


# =============================================================================
# DOCUMENT GENERATION
# =============================================================================


def generate_reference_doc(schemas: list[SchemaDefinition]) -> str:
    """Generate the SCHEMA_REFERENCE.md content."""
    lines = [
        HEADER_BANNER,
        f"<!-- Generation timestamp: {datetime.now(timezone.utc).isoformat()} -->",
        "",
        "# Schema Reference",
        "",
        "This document provides auto-generated documentation for all schema definitions",
        "in the MathLedger project. It serves as the **single source of truth** for",
        "human readers understanding configuration structures.",
        "",
        "## Table of Contents",
        "",
    ]

    # Generate TOC
    for schema in schemas:
        anchor = schema.schema_name.lower().replace("_", "-").replace(".", "-")
        lines.append(f"- [{schema.schema_name}](#{anchor})")

    lines.extend(["", "---", ""])

    # Generate schema sections
    for schema in schemas:
        lines.extend(_generate_schema_section(schema))

    # Type summary
    lines.extend(_generate_type_summary(schemas))

    return "\n".join(lines)


def _generate_schema_section(schema: SchemaDefinition) -> list[str]:
    """Generate documentation section for a single schema."""
    lines = [
        f"## {schema.schema_name}",
        "",
        f"**Source:** `{schema.source_file}`  ",
        f"**Type:** {schema.schema_type.upper()}  ",
    ]

    if schema.version:
        lines.append(f"**Version:** {schema.version}  ")
    if schema.raw_hash:
        lines.append(f"**Hash:** `{schema.raw_hash}`  ")

    lines.append("")

    if schema.description:
        lines.extend([schema.description, ""])

    # Top-level fields table
    if schema.fields:
        lines.extend(["### Fields", ""])
        lines.extend(_generate_fields_table(schema.fields))

    # Nested schema tables
    for nested_path, nested_fields in sorted(schema.nested_schemas.items()):
        safe_header = nested_path.replace("<", "&lt;").replace(">", "&gt;")
        lines.extend([f"### Nested: `{safe_header}`", ""])
        lines.extend(_generate_fields_table(nested_fields))

    lines.extend(["---", ""])
    return lines


def _generate_fields_table(fields: list[SchemaField]) -> list[str]:
    """Generate a markdown table for a list of fields."""
    if not fields:
        return ["*No fields defined.*", ""]

    lines = [
        "| Field | Type | Constraints | Example |",
        "|-------|------|-------------|---------|",
    ]

    for f in fields:
        constraints = ", ".join(f.constraints) if f.constraints else "—"
        example = _format_example(f.example) if f.example is not None else "—"
        # Escape pipes in values
        name_safe = f.name.replace("|", "\\|")
        type_safe = f.field_type.replace("|", "\\|").replace("<", "&lt;").replace(">", "&gt;")
        constraints_safe = constraints.replace("|", "\\|")
        example_safe = example.replace("|", "\\|")
        lines.append(f"| `{name_safe}` | {type_safe} | {constraints_safe} | {example_safe} |")

    lines.append("")
    return lines


def _format_example(value: Any, max_len: int = 40) -> str:
    """Format an example value for display."""
    if isinstance(value, str):
        if len(value) > max_len:
            return f'`"{value[:max_len-3]}..."`'
        return f'`"{value}"`'
    if isinstance(value, bool):
        return f"`{str(value).lower()}`"
    if isinstance(value, (int, float)):
        return f"`{value}`"
    return f"`{value}`"


def _generate_type_summary(schemas: list[SchemaDefinition]) -> list[str]:
    """Generate a type distribution summary."""
    type_counts: dict[str, int] = defaultdict(int)
    for schema in schemas:
        for f in schema.fields:
            base_type = f.field_type.split("<")[0].split("[")[0]
            type_counts[base_type] += 1
        for nested_fields in schema.nested_schemas.values():
            for f in nested_fields:
                base_type = f.field_type.split("<")[0].split("[")[0]
                type_counts[base_type] += 1

    lines = [
        "## Type Distribution Summary",
        "",
        "| Type | Count |",
        "|------|-------|",
    ]

    for t, count in sorted(type_counts.items(), key=lambda x: -x[1]):
        lines.append(f"| {t} | {count} |")

    lines.append("")
    return lines


def generate_drift_report(schemas: list[SchemaDefinition]) -> str:
    """Generate the SCHEMA_DIFF_REPORT.md content for drift detection."""
    lines = [
        HEADER_BANNER,
        f"<!-- Generation timestamp: {datetime.now(timezone.utc).isoformat()} -->",
        "",
        "# Schema Drift Report",
        "",
        "This report tracks schema file hashes for drift detection.",
        "If hashes change between documentation regenerations, it indicates",
        "schema modifications that should be reviewed.",
        "",
        "## Current Schema Hashes",
        "",
        "| Schema | File | Hash | Type |",
        "|--------|------|------|------|",
    ]

    for schema in schemas:
        lines.append(
            f"| {schema.schema_name} | `{schema.source_file}` | `{schema.raw_hash}` | {schema.schema_type} |"
        )

    lines.extend(
        [
            "",
            "## Schema Field Counts",
            "",
            "| Schema | Top-Level Fields | Nested Structures | Total Fields |",
            "|--------|------------------|-------------------|--------------|",
        ]
    )

    for schema in schemas:
        nested_count = sum(len(f) for f in schema.nested_schemas.values())
        total = len(schema.fields) + nested_count
        lines.append(
            f"| {schema.schema_name} | {len(schema.fields)} | {len(schema.nested_schemas)} | {total} |"
        )

    lines.extend(
        [
            "",
            "## Cross-Schema Field Analysis",
            "",
            "Common field names appearing across multiple schemas:",
            "",
        ]
    )

    # Find common fields
    field_occurrences: dict[str, list[str]] = defaultdict(list)
    for schema in schemas:
        seen_in_schema: set[str] = set()
        for f in schema.fields:
            if f.name not in seen_in_schema:
                field_occurrences[f.name].append(schema.schema_name)
                seen_in_schema.add(f.name)
        for nested_fields in schema.nested_schemas.values():
            for f in nested_fields:
                if f.name not in seen_in_schema:
                    field_occurrences[f.name].append(schema.schema_name)
                    seen_in_schema.add(f.name)

    common_fields = {k: v for k, v in field_occurrences.items() if len(v) > 1}
    if common_fields:
        lines.extend(["| Field Name | Schemas |", "|------------|---------|"])
        for field_name, schema_list in sorted(
            common_fields.items(), key=lambda x: -len(x[1])
        )[:20]:
            schemas_str = ", ".join(schema_list[:5])
            if len(schema_list) > 5:
                schemas_str += f" (+{len(schema_list) - 5} more)"
            lines.append(f"| `{field_name}` | {schemas_str} |")
    else:
        lines.append("*No common fields found across schemas.*")

    lines.append("")
    return "\n".join(lines)


# =============================================================================
# MAIN ENTRY POINT
# =============================================================================


def collect_schemas() -> list[SchemaDefinition]:
    """Collect all schema definitions from configured sources."""
    schemas: list[SchemaDefinition] = []

    # Parse YAML schemas
    for rel_path in SCHEMA_SOURCES["yaml"]:
        filepath = PROJECT_ROOT / rel_path
        schema = parse_yaml_schema(filepath)
        if schema:
            schemas.append(schema)

    # Parse JSON config files
    for rel_path in SCHEMA_SOURCES["json_config"]:
        filepath = PROJECT_ROOT / rel_path
        schema = parse_json_schema(filepath)
        if schema:
            schemas.append(schema)

    # Parse JSON schema files
    for rel_path in SCHEMA_SOURCES["json_schema"]:
        filepath = PROJECT_ROOT / rel_path
        schema = parse_json_schema(filepath)
        if schema:
            schemas.append(schema)

    # Also check for json_schemas directory
    json_schemas_dir = PROJECT_ROOT / "docs" / "json_schemas"
    if json_schemas_dir.exists():
        for json_file in sorted(json_schemas_dir.glob("*.json")):
            schema = parse_json_schema(json_file)
            if schema:
                schemas.append(schema)

    # Parse Python schema definitions
    for rel_path in SCHEMA_SOURCES["python_definitions"]:
        filepath = PROJECT_ROOT / rel_path
        schema = parse_python_schema(filepath)
        if schema:
            schemas.append(schema)

    return schemas


def main() -> int:
    """Main entry point."""
    parser = argparse.ArgumentParser(
        description="Generate schema documentation from YAML/JSON/Python sources."
    )
    parser.add_argument(
        "--check",
        action="store_true",
        help="CI mode: fail if generated docs differ from existing",
    )
    parser.add_argument(
        "--verbose", "-v", action="store_true", help="Verbose output"
    )
    args = parser.parse_args()

    if yaml is None:
        print("Error: PyYAML is required. Install with: pip install pyyaml", file=sys.stderr)
        return 1

    print("Collecting schemas...")
    schemas = collect_schemas()
    print(f"Found {len(schemas)} schema definitions")

    if args.verbose:
        for s in schemas:
            print(f"  - {s.schema_name} ({s.schema_type}): {len(s.fields)} fields")

    print("Generating SCHEMA_REFERENCE.md...")
    reference_content = generate_reference_doc(schemas)

    print("Generating SCHEMA_DIFF_REPORT.md...")
    drift_content = generate_drift_report(schemas)

    if args.check:
        # CI mode: compare with existing
        exit_code = 0

        if OUTPUT_REFERENCE.exists():
            existing = OUTPUT_REFERENCE.read_text(encoding="utf-8")
            # Strip timestamp lines for comparison
            existing_stripped = _strip_timestamps(existing)
            new_stripped = _strip_timestamps(reference_content)
            if existing_stripped != new_stripped:
                print(f"ERROR: {OUTPUT_REFERENCE} differs from generated content!")
                print("Run 'python scripts/generate_schema_docs.py' to regenerate.")
                exit_code = 1
            else:
                print(f"OK: {OUTPUT_REFERENCE} is up to date")
        else:
            print(f"ERROR: {OUTPUT_REFERENCE} does not exist!")
            exit_code = 1

        if OUTPUT_DRIFT.exists():
            existing = OUTPUT_DRIFT.read_text(encoding="utf-8")
            existing_stripped = _strip_timestamps(existing)
            new_stripped = _strip_timestamps(drift_content)
            if existing_stripped != new_stripped:
                print(f"ERROR: {OUTPUT_DRIFT} differs from generated content!")
                print("Run 'python scripts/generate_schema_docs.py' to regenerate.")
                exit_code = 1
            else:
                print(f"OK: {OUTPUT_DRIFT} is up to date")
        else:
            print(f"ERROR: {OUTPUT_DRIFT} does not exist!")
            exit_code = 1

        return exit_code

    # Write mode
    OUTPUT_REFERENCE.parent.mkdir(parents=True, exist_ok=True)
    OUTPUT_REFERENCE.write_text(reference_content, encoding="utf-8")
    print(f"Wrote {OUTPUT_REFERENCE}")

    OUTPUT_DRIFT.write_text(drift_content, encoding="utf-8")
    print(f"Wrote {OUTPUT_DRIFT}")

    print("Done!")
    return 0


def _strip_timestamps(content: str) -> str:
    """Strip timestamp lines for comparison."""
    lines = []
    for line in content.split("\n"):
        if "Generation timestamp:" in line:
            continue
        lines.append(line)
    return "\n".join(lines)


if __name__ == "__main__":
    sys.exit(main())

