# scripts/archive_dag_posture.py
"""
PHASE III - Script to generate and archive a new DAG posture snapshot.

This script is intended to be run in CI after a merge to the 'main' branch.
It generates the new canonical posture for the mainline DAG and archives it
in the posture history database (a directory of versioned JSON files).
"""
import json
import sys
import subprocess
from collections import Counter, defaultdict
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict

# Add project root for local imports
project_root = str(Path(__file__).resolve().parents[1])
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from backend.dag.invariant_guard import (
    ProofDag,
    SliceProfile,
    evaluate_dag_invariants,
    load_invariant_rules,
    summarize_dag_invariants_for_global_health,
)
from backend.dag.preflight_check import (
    PreflightAuditor,
    load_dag_from_jsonl,
    build_dag_posture_from_snapshot,
)

HISTORY_DIR = Path("artifacts/dag/posture_history")
INVARIANT_RULES_PATH = Path("config/dag_invariants.yaml")

def get_git_short_sha() -> str:
    """Gets the short commit SHA of the current HEAD."""
    try:
        return subprocess.check_output(
            ["git", "rev-parse", "--short", "HEAD"],
            encoding='utf-8'
        ).strip()
    except (subprocess.CalledProcessError, FileNotFoundError):
        return "unknown_sha"

def generate_dummy_log(path: Path):
    """Generates a dummy log file if one doesn't exist."""
    print(f"[INFO] Generating dummy log file at: {path}", file=sys.stderr)
    path.parent.mkdir(parents=True, exist_ok=True)
    with open(path, "w") as f:
        f.write('{"conclusion": "A", "premises": []}\n')
        f.write('{"conclusion": "B", "premises": ["A"]}\n')

def main():
    """Main function to generate and archive the posture."""
    print("--- Starting DAG Posture Archival ---", file=sys.stderr)

    # In a real CI environment, this log path would point to the canonical
    # log file generated by the build for the 'main' branch.
    mainline_log_path = Path("results/main_branch_run.jsonl")
    if not mainline_log_path.exists():
        generate_dummy_log(mainline_log_path)

    # 1. Run the preflight audit to get a report
    print(f"Running preflight audit on '{mainline_log_path}'...", file=sys.stderr)
    auditor = PreflightAuditor()
    dag_snapshot = load_dag_from_jsonl(mainline_log_path)
    health_checks = auditor.run_health_checks(snapshot=dag_snapshot)

    # 2. Build the posture snapshot from the report
    # The function expects a PreflightReport object, not a dict. We need to adapt.
    # A simpler way is to use the `build_dag_posture_from_snapshot` function.
    print("Building posture snapshot...", file=sys.stderr)
    posture = build_dag_posture_from_snapshot(dag_snapshot)
    posture["health_checks"] = [
        {"name": c.name, "status": c.status, "details": c.details}
        for c in health_checks
    ]

    # 3. Add timestamp for historical tracking
    timestamp = datetime.now(timezone.utc)
    posture["timestamp"] = timestamp.isoformat()

    # 3b. Evaluate invariants if config is present
    attach_invariant_tile(dag_snapshot, posture)

    # 4. Determine filename and save the artifact
    short_sha = get_git_short_sha()
    filename = f"{timestamp.strftime('%Y%m%dT%H%M%SZ')}_{short_sha}.json"
    output_path = HISTORY_DIR / filename
    
    print(f"Saving new posture snapshot to: {output_path}", file=sys.stderr)
    HISTORY_DIR.mkdir(parents=True, exist_ok=True)
    with open(output_path, 'w') as f:
        json.dump(posture, f, indent=2)

    # Also, update the 'latest' or 'main' posture file for CI checks
    latest_path = Path("artifacts/dag/main_posture.json")
    print(f"Updating latest mainline posture at: {latest_path}", file=sys.stderr)
    with open(latest_path, 'w') as f:
        json.dump(posture, f, indent=2)

    print("\n--- DAG Posture Archival Complete ---", file=sys.stderr)


def attach_invariant_tile(snapshot, posture: Dict[str, Any]) -> None:
    """
    Load invariant config, evaluate, and attach global health tile.
    """
    if not INVARIANT_RULES_PATH.exists():
        return
    try:
        rules = load_invariant_rules(INVARIANT_RULES_PATH)
    except Exception as exc:  # pragma: no cover - logging only
        print(f"[WARN] Failed to load invariant rules: {exc}", file=sys.stderr)
        return

    proof_dag = snapshot_to_proof_dag(snapshot)
    report = evaluate_dag_invariants(proof_dag, rules)
    summary = summarize_dag_invariants_for_global_health(report)
    summary["schema_version"] = "1.0"
    posture.setdefault("global_health", {})["dag_invariants"] = summary
    posture["dag_invariant_report"] = {
        "schema_version": "1.0",
        **report,
    }


def snapshot_to_proof_dag(snapshot) -> ProofDag:
    """
    Convert DagSnapshot to ProofDag for invariant guard evaluation.
    """
    depth_map = _compute_depths(snapshot)
    slices: Dict[str, Dict[str, Any]] = defaultdict(
        lambda: {"max_depth": 0, "edges": 0, "nodes": 0, "kinds": Counter()}
    )
    total_nodes = 0
    total_edges = 0

    for node in snapshot.nodes.values():
        profile = slices[node.slice_id]
        node_depth = depth_map.get(node.conclusion, 1)
        profile["max_depth"] = max(profile["max_depth"], node_depth)
        profile["edges"] += len(node.premises)
        profile["nodes"] += 1
        profile["kinds"][node.kind] += 1
        total_nodes += 1
        total_edges += len(node.premises)

    slice_profiles = {
        slice_id: SliceProfile(
            slice_id=slice_id,
            max_depth=payload["max_depth"],
            max_branching_factor=(
                payload["edges"] / payload["nodes"]
                if payload["nodes"] > 0
                else 0.0
            ),
            node_kind_counts=dict(payload["kinds"]),
        )
        for slice_id, payload in slices.items()
    }

    ledger_entry = {
        "cycle": 0,
        "MaxDepth(t)": max((p.max_depth for p in slice_profiles.values()), default=0),
        "GlobalBranchingFactor(t)": (
            total_edges / total_nodes if total_nodes > 0 else 0.0
        ),
    }

    return ProofDag(slices=slice_profiles, metric_ledger=[ledger_entry])


def _compute_depths(snapshot) -> Dict[str, int]:
    memo: Dict[str, int] = {}

    def depth(node_hash: str) -> int:
        if node_hash in memo:
            return memo[node_hash]
        node = snapshot.nodes.get(node_hash)
        if not node or not node.premises:
            memo[node_hash] = 1
            return 1
        memo[node_hash] = 1 + max(depth(parent) for parent in node.premises)
        return memo[node_hash]

    for node_hash in snapshot.nodes:
        depth(node_hash)
    return memo


if __name__ == "__main__":
    main()
