#!/usr/bin/env python3
"""
Uplift Telemetry Evaluation Script
Computes A/B test statistics and links to DA roots.
NO_NETWORK, deterministic, ASCII-only, Proof-or-Abstain.

Generated by Manus C - Telemetry Architect
72-hour burn: Keep it blue, keep it clean, keep it sealed.
"""

import sys
import os
import json
import hashlib
import csv
from pathlib import Path
from datetime import datetime, timezone
import subprocess

# Fixed seed for determinism
RANDOM_SEED = 1700000007


def fatal(msg: str, code: int = 1) -> None:
    """Print error and exit."""
    print(f"error: {msg}", file=sys.stderr)
    sys.exit(code)


def sha256_file(path: Path) -> str:
    """Compute SHA256 hash of a file."""
    h = hashlib.sha256()
    with open(path, 'rb') as f:
        for chunk in iter(lambda: f.read(8192), b''):
            h.update(chunk)
    return h.hexdigest()


def sha256_string(s: str) -> str:
    """Compute SHA256 hash of a string."""
    return hashlib.sha256(s.encode('utf-8')).hexdigest()


def read_csv_with_validation(path: Path, expected_columns: set) -> list:
    """
    Read CSV and validate schema.
    Returns list of row dicts.
    """
    if not path.exists():
        return None
    
    rows = []
    with open(path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        actual_columns = set(reader.fieldnames or [])
        
        # Strict schema validation
        if actual_columns != expected_columns:
            missing = expected_columns - actual_columns
            extra = actual_columns - expected_columns
            msg = "Schema mismatch in " + str(path)
            if missing:
                msg += f" | missing: {missing}"
            if extra:
                msg += f" | extra: {extra}"
            fatal(f"ABSTAIN - {msg}")
        
        for row in reader:
            rows.append(row)
    
    return rows if rows else None


def aggregate_metrics(rows: list) -> dict:
    """
    Aggregate metrics from CSV rows.
    Returns dict with totals.
    """
    total_proofs = 0
    total_cpu_hours = 0.0
    total_successes = 0
    total_attempts = 0
    verify_ms_p50_values = []
    abstain_rates = []
    depth_means = []
    
    for row in rows:
        total_proofs += int(row['proofs'])
        total_cpu_hours += float(row['cpu_hours'])
        total_successes += int(row['successes'])
        total_attempts += int(row['attempts'])
        verify_ms_p50_values.append(float(row['verify_ms_p50']))
        abstain_rates.append(float(row['abstain_rate']))
        depth_means.append(float(row['depth_mean']))
    
    n = len(rows)
    return {
        'total_proofs': total_proofs,
        'total_cpu_hours': total_cpu_hours,
        'total_successes': total_successes,
        'total_attempts': total_attempts,
        'verify_ms_p50_mean': sum(verify_ms_p50_values) / n if n > 0 else 0,
        'abstain_rate_mean': sum(abstain_rates) / n if n > 0 else 0,
        'depth_mean': sum(depth_means) / n if n > 0 else 0,
        'n_samples': n
    }


def compute_uplift_stats(baseline: dict, guided: dict) -> dict:
    """
    Compute uplift statistics using Poisson rate ratio test.
    Returns dict with uplift metrics.
    """
    import numpy as np
    from scipy import stats
    
    # Set random seed for determinism
    np.random.seed(RANDOM_SEED)
    
    # Compute rates (proofs per hour)
    baseline_rate = baseline['total_proofs'] / baseline['total_cpu_hours'] if baseline['total_cpu_hours'] > 0 else 0
    guided_rate = guided['total_proofs'] / guided['total_cpu_hours'] if guided['total_cpu_hours'] > 0 else 0
    
    if baseline_rate == 0:
        fatal("ABSTAIN - baseline rate is zero, cannot compute uplift")
    
    # Uplift ratio
    uplift_x = guided_rate / baseline_rate
    uplift_pct = (uplift_x - 1.0) * 100.0
    
    # Poisson rate ratio test
    # Use two-sample Poisson test via normal approximation
    # H0: lambda_baseline = lambda_guided
    # Test statistic: Z = (rate_guided - rate_baseline) / sqrt(var_guided + var_baseline)
    
    var_baseline = baseline['total_proofs'] / (baseline['total_cpu_hours'] ** 2)
    var_guided = guided['total_proofs'] / (guided['total_cpu_hours'] ** 2)
    
    se = np.sqrt(var_baseline + var_guided)
    z_stat = (guided_rate - baseline_rate) / se if se > 0 else 0
    p_value = 2 * (1 - stats.norm.cdf(abs(z_stat)))  # Two-tailed
    
    # 95% CI for ratio using delta method (log scale)
    log_ratio = np.log(uplift_x)
    se_log_ratio = np.sqrt(1/baseline['total_proofs'] + 1/guided['total_proofs'])
    ci_95_log = [log_ratio - 1.96 * se_log_ratio, log_ratio + 1.96 * se_log_ratio]
    ci_95 = [np.exp(ci_95_log[0]), np.exp(ci_95_log[1])]
    
    # Cliff's Delta (effect size for ordinal data)
    # Approximation: use standardized mean difference
    pooled_std = np.sqrt((var_baseline + var_guided) / 2)
    cliff_delta = (guided_rate - baseline_rate) / pooled_std if pooled_std > 0 else 0
    
    # Deltas for other metrics
    verify_delta = guided['verify_ms_p50_mean'] - baseline['verify_ms_p50_mean']
    abstain_delta_pp = (guided['abstain_rate_mean'] - baseline['abstain_rate_mean']) * 100
    depth_delta = guided['depth_mean'] - baseline['depth_mean']
    
    return {
        'baseline_proofs_per_hour': round(baseline_rate, 2),
        'guided_proofs_per_hour': round(guided_rate, 2),
        'uplift_x': round(uplift_x, 2),
        'uplift_pct': round(uplift_pct, 1),
        'p_value': p_value,
        'ci_95': [round(ci_95[0], 2), round(ci_95[1], 2)],
        'effect_size': f"cliff_delta:{cliff_delta:.2f}",
        'verify_ms_p50_delta': round(verify_delta, 1),
        'abstain_delta_pp': round(abstain_delta_pp, 1),
        'depth_delta': round(depth_delta, 1)
    }


def format_p_value(p: float) -> str:
    """Format p-value with proper notation (never p=0.0)."""
    if p < 1e-10:
        return "p<1e-10"
    elif p < 1e-9:
        return "p<1e-9"
    elif p < 1e-6:
        return "p<1e-6"
    elif p < 1e-3:
        return "p<1e-3"
    elif p < 0.01:
        return f"p<0.01"
    elif p < 0.05:
        return f"p<0.05"
    else:
        return f"p={p:.3f}"


def read_da_links() -> dict:
    """
    Read DA root links from artifacts.
    Returns dict with r_t, u_t, H_t (or null if absent).
    """
    links = {
        'r_t': None,
        'u_t': None,
        'H_t': None
    }
    
    # Try reading reasoning roots
    reasoning_path = Path('artifacts/reasoning/roots.json')
    if reasoning_path.exists():
        try:
            with open(reasoning_path, 'r') as f:
                data = json.load(f)
                links['r_t'] = data.get('r_t')
        except:
            pass
    
    # Try reading UI roots
    ui_path = Path('artifacts/ui/roots.json')
    if ui_path.exists():
        try:
            with open(ui_path, 'r') as f:
                data = json.load(f)
                links['u_t'] = data.get('u_t')
        except:
            pass
    
    # Try reading composite DA
    composite_path = Path('artifacts/da/composite.json')
    if composite_path.exists():
        try:
            with open(composite_path, 'r') as f:
                data = json.load(f)
                links['H_t'] = data.get('H_t')
        except:
            pass
    
    return links


def get_git_commit() -> str:
    """Get current git commit SHA."""
    try:
        result = subprocess.run(
            ['git', 'rev-parse', 'HEAD'],
            capture_output=True,
            text=True,
            check=True
        )
        return result.stdout.strip()
    except:
        return "unknown"


def main():
    """Main execution."""
    # Expected CSV schema
    expected_columns = {
        'timestamp', 'proofs', 'cpu_hours', 'successes', 'attempts',
        'verify_ms_p50', 'abstain_rate', 'depth_mean'
    }
    
    # Input paths
    baseline_path = Path('artifacts/wpv5/baseline.csv')
    guided_path = Path('artifacts/wpv5/guided.csv')
    output_path = Path('artifacts/uplift/uplift.json')
    
    # Read and validate inputs
    baseline_rows = read_csv_with_validation(baseline_path, expected_columns)
    guided_rows = read_csv_with_validation(guided_path, expected_columns)
    
    # ABSTAIN if inputs missing
    if baseline_rows is None or guided_rows is None:
        missing = []
        if baseline_rows is None:
            missing.append(str(baseline_path))
        if guided_rows is None:
            missing.append(str(guided_path))
        fatal(f"ABSTAIN - missing or empty input: {', '.join(missing)}")
    
    # Aggregate metrics
    baseline_agg = aggregate_metrics(baseline_rows)
    guided_agg = aggregate_metrics(guided_rows)
    
    # Compute statistics
    stats = compute_uplift_stats(baseline_agg, guided_agg)
    
    # Read DA links
    da_links = read_da_links()
    
    # Compute input hashes
    baseline_hash = sha256_file(baseline_path)
    guided_hash = sha256_file(guided_path)
    
    # Build output JSON (without timestamp for deterministic hashing)
    output_for_hash = {
        'schema': 'uplift_v1',
        'source': {
            'baseline': str(baseline_path),
            'guided': str(guided_path)
        },
        'metrics': stats,
        'da_links': da_links,
        'hashes': {
            'baseline_sha256': baseline_hash,
            'guided_sha256': guided_hash,
            'output_sha256': None
        },
        'commit': get_git_commit()
    }
    
    # Compute output hash on deterministic fields only
    output_json_for_hash = json.dumps(output_for_hash, sort_keys=True, separators=(',', ':'), ensure_ascii=True)
    output_hash = sha256_string(output_json_for_hash)
    
    # Add timestamp and final hash to output
    output = output_for_hash.copy()
    output['hashes']['output_sha256'] = output_hash
    output['generated_at'] = datetime.now(timezone.utc).isoformat()
    
    # Final serialization
    output_json = json.dumps(output, sort_keys=True, separators=(',', ':'), ensure_ascii=True)
    
    # Write output
    output_path.parent.mkdir(parents=True, exist_ok=True)
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(output_json)
        f.write('\n')  # POSIX newline
    
    # Print success message (ASCII only)
    p_str = format_p_value(stats['p_value'])
    print(f"[PASS] UPLIFT: {stats['uplift_x']:.2f}x ({p_str}) baseline={stats['baseline_proofs_per_hour']:.1f}/h guided={stats['guided_proofs_per_hour']:.1f}/h")


if __name__ == '__main__':
    main()

