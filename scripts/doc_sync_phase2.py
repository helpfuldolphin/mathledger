#!/usr/bin/env python3
"""
PHASE II — NOT USED IN PHASE I

Automatic Documentation Synchronizer for Phase II Uplift Ecosystem
===================================================================

This script maintains consistency between source of truth files and documentation
by auto-generating content within marked blocks.

Sources of Truth:
- config/curriculum_uplift_phase2.yaml     - Slice definitions
- PREREG_UPLIFT_U2.yaml                    - Preregistration data
- experiments/run_uplift_u2.py             - CLI runner
- experiments/slice_success_metrics.py     - Metric type definitions

Target Documents:
- docs/PHASE2_RFL_UPLIFT_PLAN.md
- docs/fm.tex (if exists)
- paper/main.tex

Generated Content:
- Slice tables
- Metric definitions
- CLI examples
- Status tables

Marker Blocks:
<!-- BEGIN:AUTOGEN:PHASE2-SLICES -->
... autogenerated content ...
<!-- END:AUTOGEN:PHASE2-SLICES -->

For TeX files:
% BEGIN:AUTOGEN:PHASE2-SLICES
... autogenerated content ...
% END:AUTOGEN:PHASE2-SLICES

Guarantees:
- Only modifies autogen blocks
- No changes to hand-written text
- Deterministic formatting (sorted keys, stable output)
"""

import argparse
import ast
import hashlib
import inspect
import re
import sys
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import yaml


# --- Configuration ---

REPO_ROOT = Path(__file__).resolve().parent.parent

# Source of truth files
CURRICULUM_PATH = REPO_ROOT / "config" / "curriculum_uplift_phase2.yaml"
PREREG_PATH = REPO_ROOT / "PREREG_UPLIFT_U2.yaml"
RUNNER_PATH = REPO_ROOT / "experiments" / "run_uplift_u2.py"
METRICS_PATH = REPO_ROOT / "experiments" / "slice_success_metrics.py"

# Target documentation files
TARGET_MD = REPO_ROOT / "docs" / "PHASE2_RFL_UPLIFT_PLAN.md"
TARGET_FM_TEX = REPO_ROOT / "docs" / "fm.tex"
TARGET_PAPER_TEX = REPO_ROOT / "paper" / "main.tex"

# Marker patterns
MD_MARKER_BEGIN = "<!-- BEGIN:AUTOGEN:{tag} -->"
MD_MARKER_END = "<!-- END:AUTOGEN:{tag} -->"
TEX_MARKER_BEGIN = "% BEGIN:AUTOGEN:{tag}"
TEX_MARKER_END = "% END:AUTOGEN:{tag}"


# --- Source Parsing ---


def load_yaml_file(path: Path) -> Optional[Dict[str, Any]]:
    """Load a YAML file safely, returning None if not found."""
    if not path.exists():
        return None
    with open(path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


def parse_curriculum(path: Path) -> Dict[str, Any]:
    """Parse curriculum_uplift_phase2.yaml and extract slice definitions."""
    data = load_yaml_file(path)
    if not data:
        return {"version": "unknown", "slices": {}}

    return {
        "version": data.get("version", "unknown"),
        "slices": data.get("slices", {}),
    }


def parse_prereg(path: Path) -> List[Dict[str, Any]]:
    """Parse PREREG_UPLIFT_U2.yaml and extract experiment definitions."""
    data = load_yaml_file(path)
    if not data:
        return []

    # Handle both list and dict format
    if isinstance(data, list):
        return data
    return []


def parse_runner_cli(path: Path) -> Dict[str, Any]:
    """
    Extract CLI argument definitions from run_uplift_u2.py.

    Returns information about:
    - Arguments and their help text
    - Default values
    - Required flags
    """
    if not path.exists():
        return {"args": [], "description": "", "epilog": ""}

    content = path.read_text(encoding="utf-8")

    # Parse the file AST to find argparse definitions
    try:
        tree = ast.parse(content)
    except SyntaxError:
        return {"args": [], "description": "", "epilog": ""}

    args = []
    description = ""
    epilog = ""

    for node in ast.walk(tree):
        # Find ArgumentParser creation
        if isinstance(node, ast.Call):
            func = node.func
            if isinstance(func, ast.Attribute) and func.attr == "ArgumentParser":
                for kw in node.keywords:
                    if kw.arg == "description" and isinstance(kw.value, ast.Constant):
                        description = kw.value.value
                    if kw.arg == "epilog" and isinstance(kw.value, ast.Constant):
                        epilog = kw.value.value

            # Find add_argument calls
            if isinstance(func, ast.Attribute) and func.attr == "add_argument":
                arg_info = _extract_arg_info(node)
                if arg_info:
                    args.append(arg_info)

    return {"args": args, "description": description.strip(), "epilog": epilog.strip()}


def _extract_arg_info(call_node: ast.Call) -> Optional[Dict[str, Any]]:
    """Extract argument info from an add_argument call."""
    if not call_node.args:
        return None

    # Get argument name(s)
    names = []
    for arg in call_node.args:
        if isinstance(arg, ast.Constant):
            names.append(arg.value)

    if not names:
        return None

    info: Dict[str, Any] = {"names": names}

    # Extract keyword arguments
    for kw in call_node.keywords:
        if kw.arg == "help" and isinstance(kw.value, ast.Constant):
            info["help"] = kw.value.value
        elif kw.arg == "required" and isinstance(kw.value, ast.Constant):
            info["required"] = kw.value.value
        elif kw.arg == "default" and isinstance(kw.value, ast.Constant):
            info["default"] = kw.value.value
        elif kw.arg == "type" and isinstance(kw.value, ast.Name):
            info["type"] = kw.value.id
        elif kw.arg == "choices":
            if isinstance(kw.value, ast.List):
                choices = []
                for elt in kw.value.elts:
                    if isinstance(elt, ast.Constant):
                        choices.append(elt.value)
                info["choices"] = choices

    return info


def parse_metrics_types(path: Path) -> Dict[str, Dict[str, Any]]:
    """
    Extract metric function definitions from slice_success_metrics.py.

    Returns information about each metric function:
    - Name
    - Docstring
    - Parameters
    - Return type annotation
    """
    if not path.exists():
        return {}

    content = path.read_text(encoding="utf-8")

    try:
        tree = ast.parse(content)
    except SyntaxError:
        return {}

    metrics = {}

    for node in ast.walk(tree):
        if isinstance(node, ast.FunctionDef) and node.name.startswith("compute_"):
            metric_name = node.name
            docstring = ast.get_docstring(node) or ""

            # Extract parameters
            params = []
            for arg in node.args.args:
                param_info = {"name": arg.arg}
                if arg.annotation:
                    param_info["type"] = ast.unparse(arg.annotation)
                params.append(param_info)

            # Extract return type
            return_type = None
            if node.returns:
                return_type = ast.unparse(node.returns)

            metrics[metric_name] = {
                "docstring": docstring,
                "params": params,
                "return_type": return_type,
            }

    return metrics


# --- Content Generation ---


def generate_slice_table_md(curriculum: Dict[str, Any]) -> str:
    """Generate a Markdown table of slices from curriculum data."""
    lines = [
        "| Slice Name | Description | Items Count | Prereg Hash |",
        "|------------|-------------|-------------|-------------|",
    ]

    slices = curriculum.get("slices", {})
    # Sort for determinism
    for name in sorted(slices.keys()):
        config = slices[name]
        desc = config.get("description", "N/A")
        items = config.get("items", [])
        prereg_hash = config.get("prereg_hash", "N/A")

        # Truncate hash for display
        hash_display = prereg_hash[:16] + "..." if len(prereg_hash) > 16 else prereg_hash

        lines.append(f"| `{name}` | {desc} | {len(items)} | `{hash_display}` |")

    return "\n".join(lines)


def generate_prereg_table_md(prereg: List[Dict[str, Any]]) -> str:
    """Generate a Markdown table of preregistered experiments."""
    lines = [
        "| Experiment ID | Description | Slice Config | Success Metrics |",
        "|---------------|-------------|--------------|-----------------|",
    ]

    for exp in sorted(prereg, key=lambda x: x.get("experiment_id", "")):
        exp_id = exp.get("experiment_id", "N/A")
        desc = exp.get("description", "N/A")
        slice_config = exp.get("slice_config", "N/A")
        metrics = exp.get("success_metrics", [])
        metrics_str = ", ".join(metrics) if metrics else "N/A"

        lines.append(f"| `{exp_id}` | {desc} | `{slice_config}` | {metrics_str} |")

    return "\n".join(lines)


def generate_cli_examples_md(cli_info: Dict[str, Any]) -> str:
    """Generate CLI usage examples from parsed CLI info."""
    lines = []

    # Header
    lines.append("**CLI Arguments:**")
    lines.append("")
    lines.append("```bash")

    # Generate example command
    lines.append("uv run python experiments/run_uplift_u2.py \\")

    example_values = {
        "--slice": "arithmetic_simple",
        "--cycles": "100",
        "--seed": "42",
        "--mode": "baseline",
        "--out": "results/",
    }

    args = cli_info.get("args", [])
    for i, arg in enumerate(sorted(args, key=lambda x: x.get("names", [""])[0])):
        names = arg.get("names", [])
        if not names:
            continue

        name = names[0]
        if name.startswith("--"):
            value = example_values.get(name, "<value>")
            continuation = " \\" if i < len(args) - 1 else ""
            lines.append(f"  {name}={value}{continuation}")

    lines.append("```")
    lines.append("")

    # Argument descriptions
    lines.append("| Argument | Type | Required | Default | Description |")
    lines.append("|----------|------|----------|---------|-------------|")

    for arg in sorted(args, key=lambda x: x.get("names", [""])[0]):
        names = arg.get("names", [])
        if not names:
            continue

        name = names[0]
        arg_type = arg.get("type", "str")
        required = "Yes" if arg.get("required", False) else "No"
        default = arg.get("default", "—")
        if default == "—":
            default_str = "—"
        else:
            default_str = f"`{default}`"
        help_text = arg.get("help", "N/A")

        lines.append(f"| `{name}` | {arg_type} | {required} | {default_str} | {help_text} |")

    return "\n".join(lines)


def generate_metrics_table_md(metrics: Dict[str, Dict[str, Any]]) -> str:
    """Generate a Markdown table of metric function definitions."""
    lines = [
        "| Metric Function | Parameters | Return Type | Description |",
        "|-----------------|------------|-------------|-------------|",
    ]

    for name in sorted(metrics.keys()):
        info = metrics[name]
        params = info.get("params", [])
        param_names = ", ".join(p["name"] for p in params)
        return_type = info.get("return_type", "N/A")
        docstring = info.get("docstring", "N/A")

        # Extract first line of docstring
        first_line = docstring.split("\n")[0].strip() if docstring else "N/A"

        lines.append(f"| `{name}` | `{param_names}` | `{return_type}` | {first_line} |")

    return "\n".join(lines)


def generate_status_table_md(
    curriculum: Dict[str, Any], prereg: List[Dict[str, Any]]
) -> str:
    """Generate a status table showing implementation state."""
    lines = [
        "| Component | Status |",
        "|-----------|--------|",
    ]

    # Check curriculum
    num_slices = len(curriculum.get("slices", {}))
    curriculum_status = f"✅ {num_slices} slices defined" if num_slices > 0 else "❌ Not defined"
    lines.append(f"| Curriculum Slices | {curriculum_status} |")

    # Check prereg
    num_prereg = len(prereg)
    prereg_status = f"✅ {num_prereg} experiments" if num_prereg > 0 else "⏳ Not preregistered"
    lines.append(f"| Preregistration | {prereg_status} |")

    # Check runner
    runner_status = "✅ Implemented" if RUNNER_PATH.exists() else "⏳ Not implemented"
    lines.append(f"| U2 Runner | {runner_status} |")

    # Check metrics
    metrics_status = "✅ Implemented" if METRICS_PATH.exists() else "⏳ Not implemented"
    lines.append(f"| Success Metrics | {metrics_status} |")

    return "\n".join(lines)


def generate_slice_table_tex(curriculum: Dict[str, Any]) -> str:
    """Generate a LaTeX table of slices from curriculum data."""
    lines = [
        "\\begin{table}[h]",
        "\\centering",
        "\\caption{Phase II Uplift Slices (Auto-generated)}",
        "\\label{tab:phase2_slices_autogen}",
        "\\begin{tabular}{l l r l}",
        "\\toprule",
        "\\textbf{Slice} & \\textbf{Description} & \\textbf{Items} & \\textbf{Prereg Hash} \\\\",
        "\\midrule",
    ]

    slices = curriculum.get("slices", {})
    for name in sorted(slices.keys()):
        config = slices[name]
        desc = _escape_latex(config.get("description", "N/A"))
        items = config.get("items", [])
        prereg_hash = config.get("prereg_hash", "N/A")
        hash_display = prereg_hash[:12] + "..." if len(prereg_hash) > 12 else prereg_hash

        lines.append(
            f"\\texttt{{{name}}} & {desc} & {len(items)} & \\texttt{{{hash_display}}} \\\\"
        )

    lines.extend([
        "\\bottomrule",
        "\\end{tabular}",
        "\\end{table}",
    ])

    return "\n".join(lines)


def generate_metrics_table_tex(metrics: Dict[str, Dict[str, Any]]) -> str:
    """Generate a LaTeX table of metric definitions."""
    lines = [
        "\\begin{table}[h]",
        "\\centering",
        "\\caption{Phase II Success Metrics (Auto-generated)}",
        "\\label{tab:phase2_metrics_autogen}",
        "\\begin{tabular}{l p{6cm}}",
        "\\toprule",
        "\\textbf{Metric} & \\textbf{Description} \\\\",
        "\\midrule",
    ]

    for name in sorted(metrics.keys()):
        info = metrics[name]
        docstring = info.get("docstring", "N/A")
        first_line = docstring.split("\n")[0].strip() if docstring else "N/A"
        first_line = _escape_latex(first_line)

        lines.append(f"\\texttt{{{name}}} & {first_line} \\\\")

    lines.extend([
        "\\bottomrule",
        "\\end{tabular}",
        "\\end{table}",
    ])

    return "\n".join(lines)


def _escape_latex(text: str) -> str:
    """Escape special LaTeX characters."""
    replacements = [
        ("\\", "\\textbackslash{}"),
        ("_", "\\_"),
        ("#", "\\#"),
        ("$", "\\$"),
        ("%", "\\%"),
        ("&", "\\&"),
        ("{", "\\{"),
        ("}", "\\}"),
        ("~", "\\textasciitilde{}"),
        ("^", "\\textasciicircum{}"),
    ]
    for old, new in replacements:
        text = text.replace(old, new)
    return text


# --- Document Update ---


def update_autogen_block(
    content: str, tag: str, new_content: str, is_tex: bool = False
) -> Tuple[str, bool]:
    """
    Update an autogen block in the document.

    Returns: (updated_content, was_changed)
    """
    if is_tex:
        begin_marker = TEX_MARKER_BEGIN.format(tag=tag)
        end_marker = TEX_MARKER_END.format(tag=tag)
    else:
        begin_marker = MD_MARKER_BEGIN.format(tag=tag)
        end_marker = MD_MARKER_END.format(tag=tag)

    # Find the block
    begin_idx = content.find(begin_marker)
    end_idx = content.find(end_marker)

    if begin_idx == -1 or end_idx == -1:
        # Block not found - no change
        return content, False

    # Extract what's between the markers
    block_start = begin_idx + len(begin_marker)
    old_block = content[block_start:end_idx]

    # Create new block with consistent formatting
    new_block = f"\n{new_content}\n"

    if old_block == new_block:
        # No change needed
        return content, False

    # Replace the block
    updated = content[:block_start] + new_block + content[end_idx:]
    return updated, True


def process_markdown_doc(path: Path, curriculum: Dict[str, Any],
                         prereg: List[Dict[str, Any]],
                         cli_info: Dict[str, Any],
                         metrics: Dict[str, Dict[str, Any]],
                         dry_run: bool = False) -> bool:
    """
    Process a Markdown documentation file.

    Returns True if changes were made.
    """
    if not path.exists():
        return False

    content = path.read_text(encoding="utf-8")
    original_content = content
    changed = False

    # Generate and update each block type
    blocks = [
        ("PHASE2-SLICES", generate_slice_table_md(curriculum)),
        ("PHASE2-PREREG", generate_prereg_table_md(prereg)),
        ("PHASE2-CLI", generate_cli_examples_md(cli_info)),
        ("PHASE2-METRICS", generate_metrics_table_md(metrics)),
        ("PHASE2-STATUS", generate_status_table_md(curriculum, prereg)),
    ]

    for tag, generated in blocks:
        content, block_changed = update_autogen_block(content, tag, generated, is_tex=False)
        if block_changed:
            changed = True

    if changed and not dry_run:
        path.write_text(content, encoding="utf-8")

    return changed


def process_tex_doc(path: Path, curriculum: Dict[str, Any],
                    metrics: Dict[str, Dict[str, Any]],
                    dry_run: bool = False) -> bool:
    """
    Process a TeX documentation file.

    Returns True if changes were made.
    """
    if not path.exists():
        return False

    content = path.read_text(encoding="utf-8")
    original_content = content
    changed = False

    # Generate and update each block type
    blocks = [
        ("PHASE2-SLICES", generate_slice_table_tex(curriculum)),
        ("PHASE2-METRICS", generate_metrics_table_tex(metrics)),
    ]

    for tag, generated in blocks:
        content, block_changed = update_autogen_block(content, tag, generated, is_tex=True)
        if block_changed:
            changed = True

    if changed and not dry_run:
        path.write_text(content, encoding="utf-8")

    return changed


# --- Main Entry Point ---


def main():
    parser = argparse.ArgumentParser(
        description="PHASE II — Automatic Documentation Synchronizer",
        formatter_class=argparse.RawTextHelpFormatter,
        epilog="""
This script maintains documentation consistency by auto-generating
content within marked blocks. It only modifies autogen blocks and
preserves all hand-written text.

Marker blocks:
  Markdown: <!-- BEGIN:AUTOGEN:PHASE2-SLICES --> ... <!-- END:AUTOGEN:PHASE2-SLICES -->
  TeX:      % BEGIN:AUTOGEN:PHASE2-SLICES ... % END:AUTOGEN:PHASE2-SLICES

Available tags:
  PHASE2-SLICES   - Slice definition table
  PHASE2-PREREG   - Preregistration table
  PHASE2-CLI      - CLI usage examples
  PHASE2-METRICS  - Metric definitions
  PHASE2-STATUS   - Implementation status
        """,
    )

    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Show what would be changed without modifying files",
    )

    parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="Print detailed progress information",
    )

    parser.add_argument(
        "--check",
        action="store_true",
        help="Exit with error if any changes would be made (for CI)",
    )

    args = parser.parse_args()

    print("PHASE II — Documentation Synchronizer")
    print("=" * 40)
    print()

    # Load sources of truth
    if args.verbose:
        print("Loading sources of truth...")

    curriculum = parse_curriculum(CURRICULUM_PATH)
    prereg = parse_prereg(PREREG_PATH)
    cli_info = parse_runner_cli(RUNNER_PATH)
    metrics = parse_metrics_types(METRICS_PATH)

    if args.verbose:
        print(f"  Curriculum: {len(curriculum.get('slices', {}))} slices")
        print(f"  Prereg: {len(prereg)} experiments")
        print(f"  CLI: {len(cli_info.get('args', []))} arguments")
        print(f"  Metrics: {len(metrics)} functions")
        print()

    # Process documentation files
    changes_made = False
    files_processed = 0
    files_changed = 0

    # Markdown files
    md_files = [TARGET_MD]
    for path in md_files:
        if path.exists():
            files_processed += 1
            changed = process_markdown_doc(
                path, curriculum, prereg, cli_info, metrics, dry_run=args.dry_run or args.check
            )
            if changed:
                files_changed += 1
                changes_made = True
                status = "[WOULD CHANGE]" if args.dry_run or args.check else "[UPDATED]"
                print(f"{status} {path.relative_to(REPO_ROOT)}")
            elif args.verbose:
                print(f"[NO CHANGE] {path.relative_to(REPO_ROOT)}")

    # TeX files
    tex_files = [TARGET_FM_TEX, TARGET_PAPER_TEX]
    for path in tex_files:
        if path.exists():
            files_processed += 1
            changed = process_tex_doc(path, curriculum, metrics, dry_run=args.dry_run or args.check)
            if changed:
                files_changed += 1
                changes_made = True
                status = "[WOULD CHANGE]" if args.dry_run or args.check else "[UPDATED]"
                print(f"{status} {path.relative_to(REPO_ROOT)}")
            elif args.verbose:
                print(f"[NO CHANGE] {path.relative_to(REPO_ROOT)}")

    # Summary
    print()
    print("=" * 40)
    print(f"Files processed: {files_processed}")
    print(f"Files {'would be ' if args.dry_run or args.check else ''}changed: {files_changed}")

    if args.check and changes_made:
        print()
        print("ERROR: Documentation is out of sync with source files.")
        print("Run 'python scripts/doc_sync_phase2.py' to update.")
        sys.exit(1)

    if changes_made and not args.dry_run:
        print()
        print("Documentation synchronized successfully.")

    sys.exit(0)


if __name__ == "__main__":
    main()
