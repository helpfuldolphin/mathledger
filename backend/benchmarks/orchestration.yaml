# PQ Migration Benchmark Orchestration
# 5-Week Benchmark Execution Plan
#
# Author: Manus-H
# Version: 1.0

metadata:
  name: "PQ Migration Benchmark Suite"
  version: "1.0"
  author: "Manus-H"
  description: "5-week benchmark orchestration for post-quantum migration"

# Global configuration
config:
  output_dir: "./artifacts/benchmarks"
  report_format: "markdown"
  export_csv: true
  export_json: true
  acceptance_criteria_file: "./backend/benchmarks/acceptance_criteria.yaml"

# Week 1: Micro-Benchmarks
week1:
  name: "Micro-Benchmarks"
  description: "Establish baseline performance for hash algorithms"
  duration_days: 7
  
  tasks:
    - name: "micro_sha256"
      command: "python3 backend/benchmarks/micro_benchmarks.py --algorithm sha256"
      iterations: 10
      output: "week1_micro_sha256.csv"
      
    - name: "micro_sha3"
      command: "python3 backend/benchmarks/micro_benchmarks.py --algorithm sha3-256"
      iterations: 10
      output: "week1_micro_sha3.csv"
      
    - name: "micro_blake3"
      command: "python3 backend/benchmarks/micro_benchmarks.py --algorithm blake3"
      iterations: 10
      output: "week1_micro_blake3.csv"
      optional: true  # BLAKE3 may not be available
      
    - name: "micro_comparison"
      command: "python3 backend/benchmarks/micro_benchmarks.py --compare-all"
      depends_on: ["micro_sha256", "micro_sha3"]
      output: "week1_micro_comparison.csv"
  
  reports:
    - name: "Week 1 Summary"
      template: "templates/week1_report.md"
      output: "week1_micro_benchmark_report.md"
      
  acceptance_criteria:
    - metric: "sha256_throughput_mbps"
      threshold: ">= 100"
      severity: "high"
    - metric: "sha3_throughput_mbps"
      threshold: ">= 50"
      severity: "medium"

# Week 2: Component Benchmarks
week2:
  name: "Component Benchmarks"
  description: "Measure Merkle tree and block operation overhead"
  duration_days: 7
  
  tasks:
    - name: "component_merkle"
      command: "python3 backend/benchmarks/component_benchmarks.py --operation merkle"
      iterations: 100
      output: "week2_component_merkle.csv"
      
    - name: "component_dual_merkle"
      command: "python3 backend/benchmarks/component_benchmarks.py --operation dual-merkle"
      iterations: 100
      output: "week2_component_dual_merkle.csv"
      
    - name: "component_block_seal"
      command: "python3 backend/benchmarks/component_benchmarks.py --operation block-seal"
      iterations: 100
      output: "week2_component_block_seal.csv"
      
    - name: "component_validation"
      command: "python3 backend/benchmarks/component_benchmarks.py --operation validation"
      iterations: 100
      output: "week2_component_validation.csv"
      
    - name: "component_overhead_analysis"
      command: "python3 backend/benchmarks/analyze_overhead.py"
      depends_on: ["component_merkle", "component_dual_merkle", "component_block_seal"]
      output: "week2_overhead_analysis.json"
  
  reports:
    - name: "Week 2 Summary"
      template: "templates/week2_report.md"
      output: "week2_component_benchmark_report.md"
      
  acceptance_criteria:
    - metric: "dual_merkle_overhead_percent"
      threshold: "<= 300"
      severity: "critical"
    - metric: "block_seal_overhead_percent"
      threshold: "<= 300"
      severity: "critical"
    - metric: "validation_latency_ms"
      threshold: "<= 100"
      severity: "high"

# Week 3: Integration Benchmarks
week3:
  name: "Integration Benchmarks"
  description: "Validate end-to-end performance"
  duration_days: 7
  
  tasks:
    - name: "integration_full_validation"
      command: "python3 backend/benchmarks/integration_benchmarks.py --scenario full-validation"
      iterations: 10
      output: "week3_integration_full_validation.csv"
      
    - name: "integration_chain_validation"
      command: "python3 backend/benchmarks/integration_benchmarks.py --scenario chain-validation"
      iterations: 5
      output: "week3_integration_chain_validation.csv"
      
    - name: "integration_historical_verification"
      command: "python3 backend/benchmarks/integration_benchmarks.py --scenario historical"
      iterations: 3
      output: "week3_integration_historical.csv"
      
    - name: "integration_epoch_transition"
      command: "python3 backend/benchmarks/integration_benchmarks.py --scenario epoch-transition"
      iterations: 10
      output: "week3_integration_epoch_transition.csv"
      
    - name: "integration_acceptance_check"
      command: "python3 backend/benchmarks/check_acceptance.py --week 3"
      depends_on: ["integration_full_validation", "integration_chain_validation"]
      output: "week3_acceptance_report.json"
  
  reports:
    - name: "Week 3 Summary"
      template: "templates/week3_report.md"
      output: "week3_integration_benchmark_report.md"
      
  acceptance_criteria:
    - metric: "block_validation_throughput_blocks_per_second"
      threshold: ">= 10"
      severity: "high"
    - metric: "chain_validation_throughput_blocks_per_second"
      threshold: ">= 50"
      severity: "medium"

# Week 4: Network Benchmarks
week4:
  name: "Network Benchmarks"
  description: "Assess network impact on testnet"
  duration_days: 7
  
  tasks:
    - name: "network_propagation"
      command: "python3 backend/benchmarks/network_benchmarks.py --scenario propagation"
      iterations: 10
      output: "week4_network_propagation.csv"
      
    - name: "network_consensus"
      command: "python3 backend/benchmarks/network_benchmarks.py --scenario consensus"
      iterations: 5
      output: "week4_network_consensus.csv"
      
    - name: "network_bandwidth"
      command: "python3 backend/benchmarks/network_benchmarks.py --scenario bandwidth"
      iterations: 3
      output: "week4_network_bandwidth.csv"
      
    - name: "network_impact_analysis"
      command: "python3 backend/benchmarks/analyze_network_impact.py"
      depends_on: ["network_propagation", "network_consensus", "network_bandwidth"]
      output: "week4_network_impact.json"
  
  reports:
    - name: "Week 4 Summary"
      template: "templates/week4_report.md"
      output: "week4_network_benchmark_report.md"
      
  acceptance_criteria:
    - metric: "propagation_time_multiplier"
      threshold: "<= 2.0"
      severity: "critical"
    - metric: "consensus_latency_ms"
      threshold: "<= 5000"
      severity: "high"
    - metric: "orphan_rate_percent"
      threshold: "<= 5"
      severity: "medium"

# Week 5: Stress Benchmarks
week5:
  name: "Stress Benchmarks"
  description: "Identify breaking points and bottlenecks"
  duration_days: 7
  
  tasks:
    - name: "stress_high_statement_count"
      command: "python3 backend/benchmarks/stress_benchmarks.py --scenario high-statements"
      iterations: 5
      output: "week5_stress_high_statements.csv"
      
    - name: "stress_high_block_rate"
      command: "python3 backend/benchmarks/stress_benchmarks.py --scenario high-block-rate"
      iterations: 5
      output: "week5_stress_high_block_rate.csv"
      
    - name: "stress_long_chain"
      command: "python3 backend/benchmarks/stress_benchmarks.py --scenario long-chain"
      iterations: 3
      output: "week5_stress_long_chain.csv"
      
    - name: "stress_concurrent_validation"
      command: "python3 backend/benchmarks/stress_benchmarks.py --scenario concurrent"
      iterations: 5
      output: "week5_stress_concurrent.csv"
      
    - name: "stress_bottleneck_analysis"
      command: "python3 backend/benchmarks/analyze_bottlenecks.py"
      depends_on: ["stress_high_statement_count", "stress_high_block_rate", "stress_long_chain"]
      output: "week5_bottleneck_analysis.json"
  
  reports:
    - name: "Week 5 Summary"
      template: "templates/week5_report.md"
      output: "week5_stress_benchmark_report.md"
      
  acceptance_criteria:
    - metric: "max_statements_per_block"
      threshold: ">= 1000"
      severity: "medium"
    - metric: "max_block_rate_per_second"
      threshold: ">= 5"
      severity: "medium"

# Final Report
final_report:
  name: "5-Week Benchmark Summary"
  description: "Comprehensive analysis of all benchmark results"
  
  tasks:
    - name: "aggregate_results"
      command: "python3 backend/benchmarks/aggregate_results.py"
      depends_on: ["week1", "week2", "week3", "week4", "week5"]
      output: "final_aggregate_results.json"
      
    - name: "generate_final_report"
      command: "python3 backend/benchmarks/generate_final_report.py"
      depends_on: ["aggregate_results"]
      output: "final_benchmark_report.md"
      
    - name: "check_all_acceptance_criteria"
      command: "python3 backend/benchmarks/check_all_acceptance.py"
      depends_on: ["aggregate_results"]
      output: "final_acceptance_report.json"
  
  reports:
    - name: "Executive Summary"
      template: "templates/executive_summary.md"
      output: "executive_summary.md"
    - name: "Technical Deep Dive"
      template: "templates/technical_deep_dive.md"
      output: "technical_deep_dive.md"

# Notification configuration
notifications:
  email:
    enabled: true
    recipients:
      - "ops@mathledger.network"
    events:
      - "week_complete"
      - "acceptance_failure"
      - "final_report_ready"
  
  slack:
    enabled: true
    webhook_url: "${SLACK_WEBHOOK_URL}"
    channel: "#pq-migration-benchmarks"
    events:
      - "week_complete"
      - "acceptance_failure"

# CI/CD integration
ci_cd:
  github_actions:
    enabled: true
    workflow_file: ".github/workflows/pq_benchmarks.yml"
    trigger: "schedule"  # Run on schedule
    schedule_cron: "0 0 * * 0"  # Every Sunday at midnight
  
  artifacts:
    retention_days: 90
    upload_to_s3: true
    s3_bucket: "mathledger-benchmark-artifacts"
