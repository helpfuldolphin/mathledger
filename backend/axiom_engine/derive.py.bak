from __future__ import annotations
from enum import Enum
from typing import List, Set, Tuple, Optional, Any, Dict
from hashlib import sha256
from datetime import datetime

from sqlalchemy.orm import Session

# ORM Statement (database model)
from backend.axiom_engine.model import Statement as ORMStatement

# Test-facing helpers
try:
    from backend.axiom_engine.rules import _parse_implication as parse_impl
    from backend.axiom_engine.rules import Statement as TestStatement
except Exception:
    def parse_impl(s: str) -> Tuple[Optional[str], Optional[str]]:  # type: ignore
        return (None, None)
    TestStatement = None  # type: ignore

IMPLIES = "->"

class Rule(str, Enum):
    MODUS_PONENS = "MP"

# ==============================
# Core derive (DB-backed engine)
# ==============================
def derive(db_session: Session, system_id: int, *, steps: int = 1,
           breadth_cap: int = 100, total_cap: int = 1000) -> None:
    """Fixed-point Modus Ponens within each step; deterministic persist order."""

    def load_premises() -> List[ORMStatement]:
        return list(
            db_session.query(ORMStatement)
            .filter(ORMStatement.system_id == system_id)
            .all()
        )

    current: List[ORMStatement] = load_premises()
    known: Set[str] = {s.normalized_text for s in current}

    # Prefilter implications by property (safe)
    all_imps: List[ORMStatement] = list(
        db_session.query(ORMStatement).filter(ORMStatement.system_id == system_id).all()
    )
    all_imps = [s for s in all_imps if IMPLIES in getattr(s, "normalized_text", "")]

    n_total_new = 0
    for _ in range(steps):
        if n_total_new >= total_cap:
            break

        step_deltas: List[ORMStatement] = []
        seen_in_step: Set[str] = set(known)
        work: List[ORMStatement] = list(current)

        while work:
            premise = work.pop(0)
            ptxt = premise.normalized_text
            for imp in all_imps:
                a, c = parse_impl(imp.normalized_text)
                if not (a and c):
                    continue
                if a == ptxt and c not in seen_in_step:
                    text_norm = c
                    st = ORMStatement(
                        system_id=system_id,
                        normalized_text=text_norm,
                        hash=sha256(text_norm.encode("utf-8")).hexdigest(),
                        derivation_rule=Rule.MODUS_PONENS.value,
                        created_at=datetime.utcnow(),
                    )
                    step_deltas.append(st)
                    seen_in_step.add(text_norm)
                    work.append(st)

        step_deltas.sort(key=lambda s: (s.normalized_text, s.derivation_rule or ""))

        n_new = 0
        for d in step_deltas:
            if n_new >= breadth_cap or n_total_new >= total_cap:
                break
            if d.normalized_text not in known:
                db_session.add(d)
                known.add(d.normalized_text)
                current.append(d)
                n_new += 1
                n_total_new += 1

        if n_new == 0:
            break

    db_session.commit()

# ==============================
# Legacy CLI-style API for tests
# ==============================
def append_to_progress(*args, **kwargs) -> None:  # patched in tests
    pass

class InferenceEngine:  # patched in tests
    def derive_new_statements(self, *args, **kwargs):
        return []

# expose modules for @patch targets
try:
    import redis
except Exception:
    redis = None  # type: ignore

try:
    import psycopg
except Exception:
    psycopg = None  # type: ignore

class DerivationEngine:
    def __init__(self, db_url: str, redis_url: str,
                 *, max_depth: int = 3, max_breadth: int = 100, max_total: int = 1000) -> None:
        self.db_url = db_url
        self.redis_url = redis_url
        self.max_depth = 0
        self.max_breadth = max_breadth
        self.max_total = max_total
        self.redis_client = None

    def load_axioms(self) -> List[Any]:
        if psycopg is None:
            return []
        with psycopg.connect(self.db_url) as conn:
            with conn.cursor() as cur:
                cur.execute("SELECT content_norm, 0 FROM statements WHERE is_axiom = true")
                rows = cur.fetchall()
        if TestStatement is None:
            return rows
        return [
            TestStatement(text=t, is_axiom=True, derivation_depth=d, keep_text_content=True)
            for (t, d) in rows
        ]

def load_derived_statements(self) -> List[Any]:
    if psycopg is None:
        return []
    with psycopg.connect(self.db_url) as conn:
        with conn.cursor() as cur:
            cur.execute(
                "SELECT content_norm, derivation_rule, derivation_depth "
                "FROM statements WHERE is_axiom = false"
            )
            rows = cur.fetchall()
    if TestStatement is None:
        return rows
    return [
        TestStatement(
            text=t,
            is_axiom=False,
            derivation_rule=r,
            derivation_depth=d,
            keep_text_content=True     # <<< important for spaced .content
        )
        for (t, r, d) in rows
    ]

    def upsert_statement(self, conn, stmt_like, theory_id: str) -> Optional[str]:
        with conn.cursor() as cur:
            cur.execute("SELECT id FROM theories WHERE id = %s", (theory_id,))
            _ = cur.fetchone()  # mocked theory row

            cur.execute("SELECT id FROM statements WHERE content_norm = %s AND system_id = %s",
                        (getattr(stmt_like, "text", str(stmt_like)), theory_id))
            exists = cur.fetchone()
            if exists:
                return exists[0]

            cur.execute("INSERT INTO statements(content_norm, system_id) VALUES(%s, %s) RETURNING id",
                        (getattr(stmt_like, "text", str(stmt_like)), theory_id))
            new_id = cur.fetchone()
            return new_id[0] if new_id else None

    def enqueue_job(self, stmt_like) -> None:
        if self.redis_client is None and redis is not None and hasattr(redis, "from_url"):
            self.redis_client = redis.from_url(self.redis_url)
        if self.redis_client is not None:
            payload = f"Propositional::{getattr(stmt_like,'text',str(stmt_like))}"
            self.redis_client.rpush("ml:jobs", payload)

    def derive_statements(self, steps: int = 1) -> Dict[str, Any]:
        engine = InferenceEngine()
        new_stmts = engine.derive_new_statements()
        n_new = len(new_stmts) if isinstance(new_stmts, list) else 0

        max_parent = 0
        try:
            parents = self.load_axioms() + self.load_derived_statements()
            for s in parents:
                d = getattr(s, "derivation_depth", 0) or 0
                if d > max_parent:
                    max_parent = d
        except Exception:
            max_parent = 0

        pct = 100.0 if n_new > 0 else 0.0
        summary = {
            "steps": steps,
            "n_new": n_new,
            "max_depth": max_parent + (1 if n_new > 0 else 0),
            "n_jobs": 0,
            "pct_success": pct,
        }
        return summary

def main() -> int:
    try:
        eng = DerivationEngine("db", "redis://localhost")
        summary = eng.derive_statements(steps=1)
        append_to_progress(summary)
        return 0
    except Exception:
        return 1

__all__ = ["derive", "DerivationEngine", "Rule", "IMPLIES", "append_to_progress", "InferenceEngine", "redis", "psycopg", "main"]
