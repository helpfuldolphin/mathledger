--- a/experiments/u2/policy.py
+++ b/experiments/u2/policy.py
@@ -5,6 +5,11 @@ Core execution engine for U2 experiments with:
 - Policy-driven search
 """
 from typing import Dict, Any, Optional
+
+# Phase III: Noise integration imports
+from backend.verification.rfl_integration import (
+    compute_expected_value,
+    apply_bias_correction,
+)
+from backend.verification.error_codes import VerifierErrorCode
 from rfl.prng import DeterministicPRNG
 
 
@@ -25,18 +30,18 @@ class SearchPolicy:
     def update(
         self,
         item: str,
-        success: bool,
+        value: float,
         cycle: int,
     ) -> None:
         """Update policy based on feedback.
         
         Args:
             item: Item that was verified
-            success: Whether verification succeeded
+            value: Feedback value (continuous in [-1, 1])
             cycle: Cycle number
         """
         raise NotImplementedError
@@ -60,18 +65,15 @@ class RFLPolicy(SearchPolicy):
     def update(
         self,
         item: str,
-        success: bool,
+        value: float,
         cycle: int,
     ) -> None:
         """Update policy weights using policy gradient."""
         
-        # Convert success to value
-        value = 1.0 if success else -1.0
-        
         # Compute gradient
         grad = self._compute_gradient(item, value)
         
         # Update weights
         self.weights[item] = self.weights.get(item, 0.0) + self.learning_rate * grad
     
     def _compute_gradient(self, item: str, value: float) -> float:
