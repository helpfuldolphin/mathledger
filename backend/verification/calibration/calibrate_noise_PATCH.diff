# REAL-READY UNIFIED DIFF
# File: backend/verification/calibration/calibrate_noise.py
# Purpose: Fix imports and add missing functions

--- a/backend/verification/calibration/calibrate_noise.py
+++ b/backend/verification/calibration/calibrate_noise.py
@@ -23,9 +23,9 @@
 from multiprocessing import Pool, cpu_count
 from collections import defaultdict
 
-from backend.verification.telemetry import run_lean_with_monitoring
-from backend.verification.error_codes import VerifierTier, VerifierErrorCode
+from backend.verification.telemetry_runtime import run_lean_with_monitoring, LeanVerificationTelemetry
+from backend.verification.lean_executor import construct_lean_command, get_lean_version
 from backend.verification.calibration.statistical_fitting import (
     fit_bernoulli_rate,
     fit_timeout_distribution,
     wilson_confidence_interval,
 )


@@ +100,0 @@
+
+def calibrate_single_module(args):
+    """
+    Worker function to calibrate a single module.
+    
+    Args:
+        args: Tuple of (module_name, tier, timeout_s, verification_id)
+    
+    Returns:
+        LeanVerificationTelemetry or None if failed
+    """
+    module_name, tier, timeout_s, verification_id = args
+    
+    try:
+        # Construct Lean command
+        # Note: This assumes module is in current project
+        # Adjust path construction as needed for your project structure
+        module_path = Path(f"{module_name.replace('.', '/')}.lean")
+        
+        lean_command = construct_lean_command(
+            module_path=module_path,
+            timeout_s=timeout_s,
+            use_lake=True,  # Use Lake for project builds
+            trace_tactics=True,
+        )
+        
+        # Run with monitoring
+        telemetry = run_lean_with_monitoring(
+            module_name=module_name,
+            lean_command=lean_command,
+            timeout_s=timeout_s,
+            verification_id=verification_id,
+            context=f"calibration_{tier.lower()}",
+            tier=tier.lower(),
+        )
+        
+        return telemetry
+    
+    except Exception as e:
+        print(f"Error calibrating {module_name}: {e}", file=sys.stderr)
+        return None
+
+
+def calibrate_tier(tier: str, n_samples: int, modules: List[str], workers: int, seed: int) -> List[Dict[str, Any]]:
+    """
+    Calibrate a single tier with n_samples.
+    
+    Args:
+        tier: Tier name (FAST, BALANCED, SLOW)
+        n_samples: Number of samples to collect
+        modules: List of module names to sample from
+        workers: Number of worker processes
+        seed: Random seed for reproducibility
+    
+    Returns:
+        List of telemetry dictionaries
+    """
+    
+    import random
+    random.seed(seed)
+    
+    # Tier-specific timeouts
+    tier_timeouts = {
+        "FAST": 30.0,
+        "BALANCED": 60.0,
+        "SLOW": 120.0,
+    }
+    
+    timeout_s = tier_timeouts[tier]
+    
+    # Sample modules (with replacement)
+    sampled_modules = random.choices(modules, k=n_samples)
+    
+    # Create worker arguments
+    worker_args = [
+        (module, tier, timeout_s, f"{tier}_{seed}_{i}")
+        for i, module in enumerate(sampled_modules)
+    ]
+    
+    # Run in parallel
+    print(f"Calibrating {tier} with {n_samples} samples using {workers} workers...")
+    
+    results = []
+    with Pool(processes=workers) as pool:
+        for i, telemetry in enumerate(pool.imap_unordered(calibrate_single_module, worker_args)):
+            if telemetry is not None:
+                results.append(telemetry.to_dict())
+            
+            # Progress update every 10 samples
+            if (i + 1) % 10 == 0:
+                print(f"  Progress: {i + 1}/{n_samples} samples completed")
+    
+    print(f"Calibration complete: {len(results)}/{n_samples} successful")
+    
+    return results
+
+
+def main():
+    """Main entry point."""
+    args = parse_args()
+    
+    # Load modules
+    if args.modules:
+        modules_path = Path(args.modules)
+        if not modules_path.exists():
+            print(f"Error: Module list file not found: {modules_path}", file=sys.stderr)
+            sys.exit(1)
+        
+        with open(modules_path) as f:
+            modules = [line.strip() for line in f if line.strip() and not line.startswith("#")]
+    else:
+        # Default modules
+        modules = [
+            "Mathlib.Algebra.Ring.Basic",
+            "Mathlib.Data.Nat.Basic",
+            "Mathlib.Init.Data.Nat.Lemmas",
+        ]
+    
+    print(f"Loaded {len(modules)} modules for calibration")
+    
+    # Calibrate each tier
+    all_results = {}
+    for tier in args.tiers:
+        results = calibrate_tier(
+            tier=tier,
+            n_samples=args.n,
+            modules=modules,
+            workers=args.workers,
+            seed=args.seed,
+        )
+        all_results[tier] = results
+    
+    # Fit statistical models
+    calibrated_models = {}
+    for tier, results in all_results.items():
+        print(f"\nFitting statistical models for {tier}...")
+        
+        # Count outcomes
+        n_total = len(results)
+        n_timeout = sum(1 for r in results if r["outcome"] == "verifier_timeout")
+        n_fail = sum(1 for r in results if r["outcome"] in ["proof_invalid", "proof_incomplete"])
+        n_success = sum(1 for r in results if r["outcome"] == "verified")
+        
+        # Fit Bernoulli rates
+        timeout_rate, timeout_ci = fit_bernoulli_rate(n_timeout, n_total)
+        fail_rate, fail_ci = fit_bernoulli_rate(n_fail, n_total)
+        success_rate = n_success / n_total if n_total > 0 else 0.0
+        
+        # Fit timeout distribution
+        timeout_durations = [r["duration_ms"] for r in results if r["outcome"] == "verifier_timeout"]
+        if timeout_durations:
+            timeout_dist = fit_timeout_distribution(timeout_durations)
+        else:
+            timeout_dist = {"distribution": "none", "parameters": {}}
+        
+        calibrated_models[tier] = {
+            "timeout_rate": timeout_rate,
+            "timeout_ci": timeout_ci,
+            "fail_rate": fail_rate,
+            "fail_ci": fail_ci,
+            "success_rate": success_rate,
+            "timeout_distribution": timeout_dist,
+            "n_samples": n_total,
+        }
+        
+        print(f"  Timeout rate: {timeout_rate:.3f} (95% CI: [{timeout_ci[0]:.3f}, {timeout_ci[1]:.3f}])")
+        print(f"  Fail rate: {fail_rate:.3f} (95% CI: [{fail_ci[0]:.3f}, {fail_ci[1]:.3f}])")
+        print(f"  Success rate: {success_rate:.3f}")
+    
+    # Export to YAML
+    if args.export:
+        export_data = {
+            "calibration_metadata": {
+                "timestamp": time.time(),
+                "n_samples_per_tier": args.n,
+                "tiers": args.tiers,
+                "seed": args.seed,
+                "lean_version": get_lean_version(),
+            },
+            "calibrated_models": calibrated_models,
+        }
+        
+        export_path = Path(args.export)
+        export_path.parent.mkdir(parents=True, exist_ok=True)
+        
+        with open(export_path, 'w') as f:
+            yaml.dump(export_data, f, default_flow_style=False)
+        
+        print(f"\nCalibration exported to: {export_path}")
+
+
+if __name__ == "__main__":
+    main()
