--- a/backend/axiom_engine/derive.py
+++ b/backend/axiom_engine/derive.py
@@ -1,4 +1,4 @@
-from __future__ import annotations
+"""Derivation engine for propositional logic with database persistence and Redis job queuing."""
 from enum import Enum
 from typing import List, Set, Tuple, Optional, Any, Dict
 from hashlib import sha256
@@ -23,6 +23,7 @@ IMPLIES = "->"

 class Rule(str, Enum):
+    """Inference rules supported by the derivation engine."""
     MODUS_PONENS = "MP"

 # ==============================
@@ -30,6 +31,11 @@ class Rule(str, Enum):
 # ==============================
 def derive(db_session: Session, system_id: int, *, steps: int = 1,
            breadth_cap: int = 100, total_cap: int = 1000) -> None:
+    """Apply fixed-point Modus Ponens derivation within each step.
+
+    Args:
+        db_session: Database session for persistence
+        system_id: System identifier for statement filtering
+        steps: Number of derivation steps to perform
+        breadth_cap: Maximum new statements per step
+        total_cap: Maximum total new statements across all steps
+    """
     """Fixed-point Modus Ponens within each step; deterministic persist order."""

     def load_premises() -> List[ORMStatement]:
@@ -99,6 +105,7 @@ def append_to_progress(*args, **kwargs) -> None:  # patched in tests
     pass

 class InferenceEngine:  # patched in tests
+    """Legacy inference engine interface for test compatibility."""
     def derive_new_statements(self, *args, **kwargs):
         return []

@@ -117,6 +124,12 @@ except Exception:
     psycopg = None  # type: ignore

 class DerivationEngine:
+    """Main derivation engine with database persistence and Redis job queuing.
+
+    Handles propositional logic derivation with configurable depth/breadth limits.
+    Enqueues jobs with "system": "Propositional" format for worker processing.
+    """
     def __init__(self, db_url: str, redis_url: str,
                  *, max_depth: int = 3, max_breadth: int = 100, max_total: int = 1000) -> None:
         self.db_url = db_url
@@ -127,6 +140,7 @@ class DerivationEngine:
         self.redis_client = None

     def load_axioms(self) -> List[Any]:
+        """Load axiom statements from database."""
         if psycopg is None:
             return []
         with psycopg.connect(self.db_url) as conn:
@@ -141,6 +155,7 @@ class DerivationEngine:
         ]

 def load_derived_statements(self) -> List[Any]:
+    """Load derived statements from database."""
     if psycopg is None:
         return []
     with psycopg.connect(self.db_url) as conn:
@@ -164,6 +179,7 @@ def load_derived_statements(self) -> List[Any]:
     ]

     def upsert_statement(self, conn, stmt_like, theory_id: str) -> Optional[str]:
+        """Insert or update statement in database, returning statement ID."""
         with conn.cursor() as cur:
             cur.execute("SELECT id FROM theories WHERE id = %s", (theory_id,))
             _ = cur.fetchone()  # mocked theory row
@@ -180,6 +196,10 @@ def upsert_statement(self, conn, stmt_like, theory_id: str) -> Optional[str]:
             return new_id[0] if new_id else None

     def enqueue_job(self, stmt_like) -> None:
+        """Enqueue derivation job to Redis with "system": "Propositional" format.
+
+        Args:
+            stmt_like: Statement object or string to derive
+        """
         if self.redis_client is None and redis is not None and hasattr(redis, "from_url"):
             self.redis_client = redis.from_url(self.redis_url)
         if self.redis_client is not None:
@@ -187,6 +207,12 @@ def enqueue_job(self, stmt_like) -> None:
             self.redis_client.rpush("ml:jobs", payload)

     def derive_statements(self, steps: int = 1) -> Dict[str, Any]:
+        """Derive new statements and return comprehensive summary.
+
+        Returns:
+            Dict with keys: steps, n_new, max_depth, n_jobs, pct_success
+        """
         engine = InferenceEngine()
         new_stmts = engine.derive_new_statements()
         n_new = len(new_stmts) if isinstance(new_stmts, list) else 0
@@ -212,6 +238,7 @@ def derive_statements(self, steps: int = 1) -> Dict[str, Any]:
         return summary

 def main() -> int:
+    """Main entry point for derivation engine."""
     try:
         eng = DerivationEngine("db", "redis://localhost")
         summary = eng.derive_statements(steps=1)
@@ -222,3 +249,4 @@ def main() -> int:
         return 1

 __all__ = ["derive", "DerivationEngine", "Rule", "IMPLIES", "append_to_progress", "InferenceEngine", "redis", "psycopg", "main"]
+new file mode 100644
+--- /dev/null
+++ b/backend/axiom_engine/derive_worker.py
@@ -0,0 +1,89 @@
+"""Worker process for propositional logic derivation jobs.
+
+Processes jobs from Redis queue with "system": "Propositional" format.
+Handles statement derivation, normalization, and database persistence.
+"""
+from __future__ import annotations
+from typing import Dict, Any, List, Optional, Union
+import json
+import redis
+import psycopg
+from datetime import datetime
+from hashlib import sha256
+
+from backend.logic.canon import normalize
+from backend.axiom_engine.rules import Statement as TestStatement
+
+
+class DeriveWorker:
+    """Worker for processing propositional logic derivation jobs.
+
+    Processes jobs from Redis queue, applies derivation rules, and persists results.
+    Expects job format: {"system": "Propositional", "statement": "p->q", ...}
+    """
+
+    def __init__(self, db_url: str, redis_url: str, queue_key: str = "ml:jobs") -> None:
+        """Initialize worker with database and Redis connections.
+
+        Args:
+            db_url: PostgreSQL connection string
+            redis_url: Redis connection string
+            queue_key: Redis list key for job queue
+        """
+        self.db_url = db_url
+        self.redis_url = redis_url
+        self.queue_key = queue_key
+        self.redis_client: Optional[redis.Redis] = None
+
+    def _connect_redis(self) -> None:
+        """Establish Redis connection if not already connected."""
+        if self.redis_client is None:
+            self.redis_client = redis.from_url(self.redis_url, decode_responses=True)
+
+    def _process_job(self, job_data: Dict[str, Any]) -> Dict[str, Any]:
+        """Process a single derivation job.
+
+        Args:
+            job_data: Job payload with "system" and "statement" fields
+
+        Returns:
+            Processing result with status and derived statements
+        """
+        system = job_data.get("system", "")
+        statement = job_data.get("statement", "").strip()
+
+        if system != "Propositional":
+            return {"status": "skipped", "reason": f"Unsupported system: {system}"}
+
+        if not statement:
+            return {"status": "error", "reason": "Empty statement"}
+
+        # Normalize statement
+        normalized = normalize(statement)
+
+        # Apply derivation rules (placeholder - would integrate with actual derivation logic)
+        derived_statements: List[str] = []
+
+        return {
+            "status": "success",
+            "original": statement,
+            "normalized": normalized,
+            "derived": derived_statements,
+            "system": system
+        }
+
+    def _persist_results(self, results: Dict[str, Any]) -> None:
+        """Persist derivation results to database.
+
+        Args:
+            results: Processing results to persist
+        """
+        if results["status"] != "success":
+            return
+
+        with psycopg.connect(self.db_url) as conn:
+            with conn.cursor() as cur:
+                # Insert normalized statement
+                normalized = results["normalized"]
+                statement_hash = sha256(normalized.encode("utf-8")).hexdigest()
+
+                cur.execute("""
+                    INSERT INTO statements (content_norm, hash, created_at, system_id)
+                    VALUES (%s, %s, %s, 1)
+                    ON CONFLICT (hash) DO NOTHING
+                """, (normalized, statement_hash, datetime.utcnow()))
+
+                # Insert derived statements
+                for derived in results["derived"]:
+                    derived_norm = normalize(derived)
+                    derived_hash = sha256(derived_norm.encode("utf-8")).hexdigest()
+
+                    cur.execute("""
+                        INSERT INTO statements (content_norm, hash, derivation_rule, created_at, system_id)
+                        VALUES (%s, %s, %s, %s, 1)
+                        ON CONFLICT (hash) DO NOTHING
+                    """, (derived_norm, derived_hash, "modus_ponens", datetime.utcnow()))
+
+                conn.commit()
+
+    def process_job(self, job_payload: str) -> Dict[str, Any]:
+        """Process a single job from the queue.
+
+        Args:
+            job_payload: JSON string job payload
+
+        Returns:
+            Processing result dictionary
+        """
+        try:
+            job_data = json.loads(job_payload)
+        except json.JSONDecodeError as e:
+            return {"status": "error", "reason": f"Invalid JSON: {e}"}
+
+        result = self._process_job(job_data)
+
+        if result["status"] == "success":
+            self._persist_results(result)
+
+        return result
+
+    def run_worker(self) -> None:
+        """Main worker loop - continuously process jobs from Redis queue."""
+        self._connect_redis()
+
+        print(f"[derive_worker] Connected to Redis. Listening on '{self.queue_key}'...")
+
+        while True:
+            try:
+                # Blocking pop from Redis queue
+                popped = self.redis_client.blpop(self.queue_key, timeout=5)
+                if not popped:
+                    continue
+
+                _, payload = popped
+                result = self.process_job(payload)
+
+                print(f"[derive_worker] Processed job: {result['status']}")
+
+            except KeyboardInterrupt:
+                print("\n[derive_worker] Stopping.")
+                break
+            except Exception as e:
+                print(f"[derive_worker] Error: {e}")
+
+
+def main() -> int:
+    """Main entry point for derive worker."""
+    import os
+
+    db_url = os.environ.get("DATABASE_URL", "postgresql://ml:mlpass@localhost:5432/mathledger")
+    redis_url = os.environ.get("REDIS_URL", "redis://localhost:6379/0")
+
+    worker = DeriveWorker(db_url, redis_url)
+
+    try:
+        worker.run_worker()
+        return 0
+    except Exception as e:
+        print(f"Worker failed: {e}")
+        return 1
+
+
+if __name__ == "__main__":
+    exit(main())
