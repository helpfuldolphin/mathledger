"""
Lean Proof-Term Canonicalization
================================

This module provides canonicalization helpers for Lean proof sources
generated by the MathLedger worker.  The goals are:

* strip nondeterministic metadata (job IDs, trailing whitespace)
* normalize whitespace and line endings
* produce canonical UTF-8/ASCII byte payloads for hashing

These helpers are intentionally conservative â€” they do not attempt to parse
Lean syntax.  Instead they focus on deterministic text normalization so that
proof hashes remain stable across reruns and Lean versions.
"""

from __future__ import annotations

import re
from typing import Final


_JOB_UNDERSCORE_RE: Final[re.Pattern[str]] = re.compile(r"(job_)[0-9a-f]{6,}", re.IGNORECASE)
_JOB_COMMENT_RE: Final[re.Pattern[str]] = re.compile(r"(job\s+)[0-9a-f]{6,}", re.IGNORECASE)
_NEWLINE_RE: Final[re.Pattern[str]] = re.compile(r"\r\n?")  # Collapse CRLF/CR to LF
_BLANK_BLOCK_RE: Final[re.Pattern[str]] = re.compile(r"\n{3,}")  # Limit consecutive blank lines
_CANONICAL_JOB_TOKEN: Final[str] = "CANONICAL"


def _scrub_job_ids(text: str) -> str:
    """Replace job-specific identifiers with canonical token."""

    def _replace(match: re.Match[str]) -> str:
        prefix = match.group(1)
        return f"{prefix}{_CANONICAL_JOB_TOKEN}"

    text = _JOB_UNDERSCORE_RE.sub(_replace, text)
    text = _JOB_COMMENT_RE.sub(_replace, text)
    return text


def canonicalize_proof_text(source: str | None) -> str:
    """
    Produce a canonical textual representation of a Lean proof file.

    Args:
        source: Lean source text as captured from worker output.

    Returns:
        Canonicalized string with normalized whitespace and scrubbed metadata.
    """
    if not source:
        return ""

    text = _NEWLINE_RE.sub("\n", source)
    text = text.replace("\t", "    ")

    lines = [line.rstrip() for line in text.split("\n")]
    text = "\n".join(lines).strip()
    text = _scrub_job_ids(text)
    text = _BLANK_BLOCK_RE.sub("\n\n", text)
    return text + "\n"


def canonicalize_proof_bytes(source: str | None) -> bytes:
    """
    Canonical bytes suitable for hashing.

    Args:
        source: Lean source text.

    Returns:
        UTF-8 encoded canonical byte payload.
    """
    return canonicalize_proof_text(source).encode("utf-8")


def canonicalize_module_name(name: str | None) -> str:
    """Canonicalize Lean module names that embed job identifiers."""
    if not name:
        return ""
    return _scrub_job_ids(name.strip())


__all__ = [
    "canonicalize_module_name",
    "canonicalize_proof_bytes",
    "canonicalize_proof_text",
]
