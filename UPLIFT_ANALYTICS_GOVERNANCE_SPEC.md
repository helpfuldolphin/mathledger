# UPLIFT ANALYTICS GOVERNANCE SPECIFICATION

**STATUS: PHASE II**

## 1. Overview

This document establishes the governance protocol for all Phase II uplift analytics. Adherence to this specification is mandatory for any experiment intended to produce evidence for governance decisions. The primary goal is to ensure that all analytic evidence is **reproducible, statistically sound, and resistant to manipulation**.

The procedures herein apply to the output of the `u2_analysis.py` module, specifically the `summary.json` artifact it generates.

---

## 2. Rules for Analytics Reproducibility

Determinism is a non-negotiable requirement for all evidence packs. The same analysis script run on the same input data with the same parameters MUST produce bit-for-bit identical output.

| Rule ID | Requirement | Implementation | Verification |
|---|---|---|---|
| **REP-1** | **Deterministic Seed** | The `compute_uplift_metrics` function MUST be called with an explicit, integer `bootstrap_seed`. This seed is the sole source of randomness for all bootstrap resampling. | The `summary.json` artifact MUST contain the `bootstrap_seed` value used. Reviewers must be able to re-run the analysis with this seed and get an identical result. |
| **REP-2** | **Immutable Inputs** | The input log files (`baseline.jsonl`, `rfl.jsonl`) are considered immutable once an experiment is complete. They must be archived in a read-only state. | The analysis runner script should checksum the input files and record the hashes in the experiment's metadata. The governance review should verify that these checksums match the archived artifacts. |
| **REP-3** | **Versioned Code** | The exact version of the `mathledger` codebase used to run the analysis must be recorded. | The Git commit hash of the `mathledger` repository at the time of execution must be captured and included in the final report. |
| **REP-4** | **Versioned Dependencies** | All Python package dependencies are strictly managed by `pyproject.toml` and `uv.lock`. | The `uv.lock` file must be committed alongside the analysis code. The execution environment must be synced from this lockfile to ensure the exact same package versions are used. |

---

## 3. Forbidden Manipulations of Metrics

To ensure the integrity of our findings, the following practices are strictly forbidden. Evidence of these practices will invalidate an experiment's results.

| Rule ID | Forbidden Practice | Description |
|---|---|---|
| **MAN-1** | **Cherry-Picking Results** | Selectively reporting on slices or metrics that show favorable results while ignoring others. The `summary.json` MUST report on all four asymmetric uplift environments. |
| **MAN-2** | **P-Hacking** | Running multiple statistical tests or variations of tests and only reporting the ones that yield a significant p-value (or a CI that excludes zero). The statistical methods are fixed: Wilson CI for proportions and Percentile Bootstrap for continuous metrics. |
| **MAN-3**
| **Post-Hoc Hypothesis Change** | Altering the success criteria or hypothesis after observing the results. All success criteria are fixed in the `SLICE_SUCCESS_CRITERIA` dictionary before the experiment begins. |
| **MAN-4** | **Selective Data Filtering** | Excluding or removing "unfavorable" data points from the input logs without a clear, preregistered justification (e.g., removing known corrupted cycles). Any and all data filtering must be automatically performed by the `load_u2_experiment` function and be universally applied. |
| **MAN-5** | **Misrepresenting CIs** | Stating that a result is "significant" just because the point estimate (the `delta`) is positive. Significance is only achieved if the **entire confidence interval** for the delta excludes zero. |

---

## 4. Requirements for `summary.json` Schema

The `summary.json` file is the canonical artifact for an experiment's results. It is the single source of truth for governance review. It MUST conform to the structure generated by the `u2_analysis.py` module.

```json
{
  "slice_id": "string",
  "sample_size": {
    "baseline": "integer",
    "rfl": "integer"
  },
  "metrics": {
    "success_rate": {
      "baseline": "number",
      "rfl": "number",
      "delta": "number",
      "ci": ["number", "number"]
    },
    "abstention_rate": {
      "baseline": "number",
      "rfl": "number",
      "delta": "number",
      "ci": ["number", "number"]
    },
    "throughput": {
      "baseline_stat": "number",
      "treatment_stat": "number",
      "delta": "number",
      "delta_ci_low": "number",
      "delta_ci_high": "number",
      "delta_pct": "number",
      "significant": "boolean"
    }
  },
  "governance": {
    "passed": "boolean",
    "details": {
      "sample_size_passed": "boolean",
      "success_rate_passed": "boolean",
      "abstention_rate_passed": "boolean",
      "throughput_uplift_passed": "boolean"
    }
  },
  "reproducibility": {
    "bootstrap_seed": "integer",
    "n_bootstrap": "integer"
  }
}
```

---

## 5. Confidence Readout Ladder

This ladder provides a standardized framework for interpreting the statistical evidence from an experiment. It maps the primary uplift metric (throughput uplift percentage) and its 95% bootstrap confidence interval (CI) to a clear governance label.

Let `uplift` be `metrics.throughput.delta_pct` and `ci` be `[metrics.throughput.delta_ci_low, metrics.throughput.delta_ci_high]`.
Let `T` be the `min_throughput_uplift_pct` for the slice from `SLICE_SUCCESS_CRITERIA`.

| Scenario | CI relative to 0 | CI relative to Threshold `T` | Governance Label | Interpretation | Action |
|---|---|---|---|---|---|
| **1** | Entirely above 0 | Entirely above `T` | **Strong Positive Effect** | Statistically significant uplift that meets or exceeds the success criterion. | **Proceed.** High confidence. |
| **2** | Entirely above 0 | Straddles `T` | **Positive Effect** | Statistically significant uplift, but uncertainty exists about whether it meets the success criterion. | **Proceed with caution.** The effect is real, but may be smaller than desired. |
| **3** | Entirely above 0 | Entirely below `T` | **Minor Positive Effect** | Statistically significant uplift, but it is confidently below the required threshold. | **Hold.** The effect is real but insufficient. Re-evaluate policy or theory. |
| **4** | Straddles 0 | - | **Inconclusive** | The data is compatible with both a positive and negative effect. No conclusion can be drawn. | **Hold.** Insufficient evidence. Requires more data or a more powerful experiment. |
| **5** | Entirely below 0 | - | **Negative Effect** | The RFL policy shows a statistically significant *decrease* in performance. A regression. | **Rollback.** The change is harmful. Investigate immediately. |

This ladder ensures that decisions are based not just on the point estimate of the uplift, but on the full range of plausible values indicated by the confidence interval.
